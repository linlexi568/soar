"""Standalone training/search entry for SOAR (MCTS over DSL rules).

ËøÅÁßªËá™ 01_pi_light/train_pi_light.pyÔºåÂπ∂Êîπ‰∏∫ÈªòËÆ§‰øùÂ≠òÂà∞ 01_soar/results„ÄÇ

üöÄ ÊîØÊåÅ Isaac Gym GPU Âä†ÈÄü‰ªøÁúüÔºà100-500√ó Âä†ÈÄüÔºâ
"""
from __future__ import annotations
import argparse, os, time, json, io, contextlib, warnings, sys, pathlib
import numpy as np

# === Isaac Gym ÊîØÊåÅÊ£ÄÊµã ===
ISAAC_GYM_AVAILABLE = False
try:
    from isaacgym import gymapi
    ISAAC_GYM_AVAILABLE = True
    print("[Isaac Gym] ‚úÖ Ê£ÄÊµãÂà∞ Isaac GymÔºåGPU Âä†ÈÄüÂ∑≤ÂêØÁî®")
except ImportError:
    print("[Isaac Gym] ‚ö†Ô∏è  Êú™Ê£ÄÊµãÂà∞ Isaac Gym„ÄÇËØ∑ÂÆâË£Ö https://developer.nvidia.com/isaac-gym ÂêéÂÜçËøêË°åËÆ≠ÁªÉ„ÄÇ")

from typing import List, Dict, Any, Tuple

# Restore deprecated numpy aliases required by older dependencies like cma
if not hasattr(np, 'Inf'):
    np.Inf = np.inf  # type: ignore[attr-defined]
if not hasattr(np, 'NINF'):
    np.NINF = -np.inf  # type: ignore[attr-defined]
from collections import OrderedDict
# Limit BLAS threads early to reduce per-process memory/threads pressure
try:
    os.environ.setdefault('OMP_NUM_THREADS', '1')
    os.environ.setdefault('MKL_NUM_THREADS', '1')
    os.environ.setdefault('OPENBLAS_NUM_THREADS', '1')
    os.environ.setdefault('NUMEXPR_MAX_THREADS', '1')
except Exception:
    pass
# Â∑≤ÁßªÈô§ÂéÜÂè≤Â§öÂêéÁ´Ø‰æùËµñ‰∏éÂàÜÊîØÈÄªËæëÔºåÁªü‰∏Ä‰ΩøÁî® Isaac Gym

# ÂÖºÂÆπ‰∏§ÁßçÁî®Ê≥ïÔºöÂåÖÂÜÖÁõ∏ÂØπÂØºÂÖ• Êàñ Áõ¥Êé•ËÑöÊú¨ÊâßË°å
try:
    if __package__:
        from . import MCTS_Agent, BinaryOpNode, UnaryOpNode, TerminalNode
        from .serialization import save_program_json, save_search_history, deserialize_program
    else:
        raise ImportError
except Exception:
    import importlib.util as _ilu, importlib as _il, pathlib as _pl, sys as _sys
    _CUR = _pl.Path(__file__).resolve()
    _PKG_DIR = _CUR.parent
    _PKG_NAME = 'soar_local'
    _spec = _ilu.spec_from_file_location(_PKG_NAME, str(_PKG_DIR / '__init__.py'), submodule_search_locations=[str(_PKG_DIR)])
    if _spec is None or _spec.loader is None:
        raise ImportError('Failed to locate 01_soar package files')
    _mod = _ilu.module_from_spec(_spec)  # type: ignore
    _sys.modules[_PKG_NAME] = _mod
    _spec.loader.exec_module(_mod)       # type: ignore
    # ‰ΩøÁî®Âª∂ËøüÂä†ËΩΩÂô®‰ª•ÂÖºÂÆπ Python 3.8 ÁöÑÁ±ªÂûãÊ≥®Ëß£ÈôêÂà∂ÔºõËã•‰ªÖËØ∑Ê±Ç --helpÔºåÂàôË∑≥ËøáÂä†ËΩΩ
    import sys as __sys
    if ('--help' in __sys.argv) or ('-h' in __sys.argv):
        MCTS_Agent = None  # type: ignore
    else:
        _m_loader = getattr(_mod, '_load_mcts_agent', None)
        if _m_loader is None:
            raise ImportError('soar_local missing _load_mcts_agent')
        MCTS_Agent = _m_loader()
    # ÊéßÂà∂Âô®Êîπ‰∏∫Êï∞Â≠¶ÂéüËØ≠ÊâßË°åÂô®
    try:
        MathProgramController = getattr(_mod, 'MathProgramController')
    except Exception:
        # Âä®ÊÄÅÂØºÂÖ• program_executor
        import importlib as _ilx
        MathProgramController = _ilx.import_module(f'{_PKG_NAME}.program_executor').MathProgramController  # type: ignore[name-defined]
    BinaryOpNode = getattr(_mod, 'BinaryOpNode')
    UnaryOpNode = getattr(_mod, 'UnaryOpNode')
    TerminalNode = getattr(_mod, 'TerminalNode')
    _ser_mod = _il.import_module(f'{_PKG_NAME}.serialization')
    save_program_json = getattr(_ser_mod, 'save_program_json')
    save_search_history = getattr(_ser_mod, 'save_search_history')
    deserialize_program = getattr(_ser_mod, 'deserialize_program')

# ‰ΩøÁî® Isaac Gym ÊµãËØïÂô®ÔºõËã•‰ªÖËØ∑Ê±Ç --help ÂàôÂª∂ËøüÂØºÂÖ•
import sys as _sys, pathlib as _pl
_CURR = _pl.Path(__file__).resolve(); _ROOTP = _CURR.parent.parent
if str(_ROOTP) not in _sys.path:
    _sys.path.insert(0, str(_ROOTP))
if ('--help' in _sys.argv) or ('-h' in _sys.argv):
    SimulationTester = None  # type: ignore
else:
    try:
        from utilities.isaac_tester import SimulationTester  # type: ignore
    except Exception as _te:
        raise ImportError(f"Failed to import SimulationTester (Isaac): {_te}")

from utilities.reward_profiles import get_reward_profile, describe_profile

def build_trajectory(name: str):
    if name == 'figure8':
        return { 'type': 'figure_8','initial_xyz': [0, 0, 1.0], 'params': {'A': 0.8,'B': 0.5,'period': 12}}
    elif name == 'helix':
        return { 'type': 'helix','initial_xyz': [0, 0, 0.5], 'params': {'R': 0.7,'period': 10,'v_z': 0.15}}
    elif name == 'circle':
        return { 'type': 'circle','initial_xyz': [0, 0, 0.8], 'params': {'R': 0.9,'period': 10}}
    elif name == 'square':
        return { 'type': 'square','initial_xyz': [0, 0, 0.8], 'params': {'side_len': 1.2,'period': 12,'corner_hold': 0.5}}
    elif name == 'step_hover':
        return { 'type': 'step_hover','initial_xyz': [0, 0, 0.6], 'params': {'z2': 1.2,'switch_time': 6.0}}
    elif name == 'spiral_out':
        return { 'type': 'spiral_out','initial_xyz': [0, 0, 0.6], 'params': {'R0': 0.2,'k': 0.05,'period': 9,'v_z':0.02}}
    elif name == 'zigzag3d':
        return { 'type': 'zigzag3d','initial_xyz': [0, 0, 0.7], 'params': {'amplitude': 0.8,'segments': 6,'z_inc': 0.08,'period': 14.0}}
    elif name == 'lemniscate3d':
        return { 'type': 'lemniscate3d','initial_xyz': [0, 0, 0.7], 'params': {'a': 0.9,'period': 16.0,'z_amp': 0.25}}
    elif name in ('random_wp','random_waypoints'):
        return { 'type': 'random_waypoints','initial_xyz': [0, 0, 0.8], 'params': {'hold_time': 1.2, 'transition': 'linear'}}
    elif name == 'spiral_in_out':
        return { 'type': 'spiral_in_out','initial_xyz': [0, 0, 0.7], 'params': {'R_in': 0.9,'R_out': 0.2,'period': 14.0,'z_wave': 0.15}}
    elif name == 'stairs':
        return { 'type': 'stairs','initial_xyz': [0, 0, 0.6], 'params': {'levels': [0.6, 0.9, 1.2], 'segment_time': 3.0}}
    elif name == 'coupled_surface':
        return { 'type': 'coupled_surface','initial_xyz': [0, 0, 0.8], 'params': {'ax': 0.9,'ay': 0.7,'f1': 1.0,'f2': 2.0,'phase': 1.0472,'z_base': 0.8,'z_amp': 0.25,'surf_amp': 0.15}}
    # ÊµãËØïÈõÜÊûÅÁ´ØÁâàÊú¨ (ËÆ≠ÁªÉ/ÊµãËØïÂàÜÁ¶ª)
    elif name == 'coupled_surface_extreme':
        return { 'type': 'coupled_surface','initial_xyz': [0, 0, 0.9], 'params': {'ax': 1.1,'ay': 0.9,'f1': 1.5,'f2': 3.0,'phase': 0.7,'z_base': 0.9,'z_amp': 0.35,'surf_amp': 0.22}}
    elif name == 'zigzag3d_aggressive':
        return { 'type': 'zigzag3d','initial_xyz': [0, 0, 0.6], 'params': {'amplitude': 1.1,'segments': 8,'z_inc': 0.12,'period': 10.0}}
    elif name == 'lemniscate3d_wild':
        return { 'type': 'lemniscate3d','initial_xyz': [0, 0, 0.6], 'params': {'a': 1.2,'period': 12.0,'z_amp': 0.40}}
    elif name == 'spiral_chaotic':
        return { 'type': 'spiral_in_out','initial_xyz': [0, 0, 0.65], 'params': {'R_in': 1.1,'R_out': 0.15,'period': 10.0,'z_wave': 0.25}}
    elif name == 'stairs_harsh':
        return { 'type': 'stairs','initial_xyz': [0, 0, 0.5], 'params': {'levels': [0.5, 0.8, 1.1, 1.4],'segment_time': 2.2}}
    else:
        raise ValueError(f"Unknown trajectory: {name}")

def build_disturbances(preset: str | None):
    if not preset:
        return []
    if preset == 'mild_wind':
        return [
            {'type': 'SUSTAINED_WIND','info':'mild','start_time':3.0,'end_time':6.0,'force':[0.004,0.0,0.0]},
            {'type': 'PULSE','time':8.0,'force':[0.008,-0.004,0.0],'info':'pulse'}
        ]
    if preset == 'stress':
        return [
            {'type': 'SUSTAINED_WIND','info':'stress:steady_wind','start_time':2.5,'end_time':6.5,'force':[0.007,0.0,0.0]},
            {'type': 'GUSTY_WIND','info':'stress:gusty_wind','start_time':7.5,'end_time':11.5,'base_force':[0.0,-0.004,0.0],'gust_frequency':6.0,'gust_amplitude':0.006},
            {'type': 'MASS_CHANGE','info':'stress:mass_up','time':12.0,'mass_multiplier':1.08},
            {'type': 'PULSE','info':'stress:pulse','time':14.0,'force':[-0.008,0.008,0.0]}
        ]
    raise ValueError(f"Unknown disturbance preset: {preset}")

def build_preset_list(preset: str) -> list[str]:
    """Return trajectory name list for a given preset (used by standardized test verification)."""
    if preset == 'train_core':
        return ['figure8','helix','circle','square','step_hover','spiral_out']
    if preset in ('test_challenge',):
        return ['zigzag3d','lemniscate3d','random_wp','spiral_in_out','stairs','coupled_surface']
    if preset == 'test_extreme':
        return ['coupled_surface_extreme','zigzag3d_aggressive','lemniscate3d_wild','spiral_chaotic','stairs_harsh']
    if preset == 'full_eval':
        return ['figure8','helix','circle','square','step_hover','spiral_out',
                'zigzag3d','lemniscate3d','random_wp','spiral_in_out','stairs','coupled_surface']
    if preset == 'pi_strong_train':
        return ['zigzag3d','lemniscate3d','random_wp','spiral_in_out','stairs','coupled_surface']
    raise ValueError(f"Unknown preset: {preset}")

def parse_args():
    p = argparse.ArgumentParser(description='Train/Search SOAR rules using MCTS')
    p.add_argument('--iters', type=int, default=100)
    p.add_argument('--traj', type=str, default='figure8',
                   choices=['figure8','helix','circle','square','step_hover','spiral_out',
                            'zigzag3d','lemniscate3d','random_wp','random_waypoints','spiral_in_out','stairs','coupled_surface'])
    p.add_argument('--traj_list', type=str, nargs='*', default=None)
    p.add_argument('--traj_preset', type=str, default=None,
                   choices=['pi_strong_train'])
    p.add_argument('--aggregate', type=str, default='harmonic', choices=['mean','min','harmonic'])
    p.add_argument('--duration', type=int, default=20)
    p.add_argument('--disturbance', type=str, default='mild_wind', choices=[None,'mild_wind','stress'])
    p.add_argument('--save-every', type=int, default=0)
    p.add_argument('--save-program', type=str, default='01_soar/results/best_program.json')
    p.add_argument('--save-history', type=str, default='01_soar/results/search_history.json')
    p.add_argument('--seed', type=int, default=None)
    p.add_argument('--report', type=int, default=20)
    # MCTS config
    p.add_argument('--mcts_max_depth', type=int, default=20)
    p.add_argument('--rollout_depth', type=int, default=4)
    # Â∑≤ÈªòËÆ§ÂÖ≥Èó≠Â§çÊùÇÂ∫¶ÊÉ©ÁΩö
    p.add_argument('--complexity_penalty', type=float, default=0.0)
    p.add_argument('--pw_alpha', type=float, default=0.6)
    p.add_argument('--pw_c', type=float, default=1.5)
    # Â§çÊùÇÂ∫¶Ë∞ÉÂ∫¶ÔºàÈªòËÆ§ÂÖ≥Èó≠Ôºâ
    p.add_argument('--complexity-min-scale', type=float, default=0.0)
    p.add_argument('--complexity-max-scale', type=float, default=0.0)
    p.add_argument('--complexity-ramp-start', type=float, default=0.0)
    p.add_argument('--complexity-ramp-end', type=float, default=0.0)
    p.add_argument('--no-complexity-penalty', action='store_true')
    p.add_argument('--reward_profile', type=str, default='pilight_boost', choices=['default','pilight_boost','pilight_freq_boost','control_law_discovery'])
    p.add_argument('--tqdm', action='store_true')
    p.add_argument('--banner-every', type=int, default=0)
    p.add_argument('--quiet-eval', action='store_true')
    p.add_argument('--warm_start_cmaes', action='store_true')
    p.add_argument('--warm_start_path', type=str, default='03_CMA-ES/results/best_program.json')
    p.add_argument('--warm-start-program', type=str, default=None)
    # ÈªòËÆ§Âú® warm start ÂêéË°•ÈΩêÂà∞ min-rules-guardÔºåÊé®Âä®Êó©ÊúüÂàÜË£Ç
    p.add_argument('--pad-after-warm-start', action='store_true', default=True)
    # ÊÄßËÉΩ
    p.add_argument('--log-skip', type=int, default=2)
    p.add_argument('--in-memory-log', action='store_true')
    p.add_argument('--short-duration', type=int, default=6)
    p.add_argument('--full-duration', type=int, default=20)
    p.add_argument('--short-frac', type=float, default=0.4)
    # ÊäóËøáÊãüÂêàÔºöÂë®ÊúüÊÄßÂÖ®Êó∂ÈïøÊé¢ÈíàÊ∑∑ÂêàÔºàÈªòËÆ§ÂÖ≥Èó≠Ôºâ
    p.add_argument('--fullmix-every', type=int, default=0, help='ÊØèÈöîNÊ¨°Ëø≠‰ª£ÔºåÊääÂÖ®Êó∂ÈïøËØÑ‰º∞ÊåâÊØî‰æãÊ∑∑ÂÖ•ËÆ≠ÁªÉÁõÆÊ†áÔºõ0=Á¶ÅÁî®')
    p.add_argument('--fullmix-frac', type=float, default=0.0, help='Ê∑∑ÂêàÊØî‰æãÔºövalue = (1-frac)*short + frac*fullÔºàÈªòËÆ§0Á¶ÅÁî®Ôºâ')
    p.add_argument('--fullmix-ramp-start', type=float, default=0.0, help='ÊåâËøõÂ∫¶‰ªéÊ≠§Â§ÑÂºÄÂßãÁ∫øÊÄßÁà¨ÂçáÂà∞ fullmix-fracÔºà0-1ÔºåÈªòËÆ§0Ôºâ')
    p.add_argument('--fullmix-ramp-end', type=float, default=0.0, help='Âú®Ê≠§ËøõÂ∫¶ËææÂà∞ fullmix-fracÔºà0-1ÔºåÈªòËÆ§0=ÊÅíÂÆöÊØî‰æãÔºâ')
    # ÁºìÂ≠ò/Â§çÁî®
    p.add_argument('--cache-size', type=int, default=2048)
    p.add_argument('--reuse-env', action='store_true')
    p.add_argument('--quiet-sim', action='store_true')
    # Âπ∂Ë°å‰∏éÊâπÊ¨°
    p.add_argument('--parallel-traj', action='store_true')
    p.add_argument('--num-workers', type=int, default=0, help='Âπ∂Ë°åËØÑ‰º∞ÁöÑËøõÁ®ãÊï∞Ôºö0=Á¶ÅÁî®ËøõÁ®ãÊ±†ÔºàÂçïËøõÁ®ãÔºâÔºå>1=ÂêØÁî®Âõ∫ÂÆöËøõÁ®ãÊï∞Ôºå-1=Ëá™Âä®(Á∫¶Á≠â‰∫éCPU-1)')
    p.add_argument('--traj-batch-size', type=int, default=0)
    
    # === Isaac Gym GPU Âä†ÈÄüÈÄâÈ°π ===
    p.add_argument('--use-isaac-gym', action='store_true', 
                   help='ÂêØÁî® Isaac Gym GPU Âä†ÈÄü‰ªøÁúüÔºàÈúÄË¶Å NVIDIA GPU + Isaac GymÔºâ')
    p.add_argument('--isaac-num-envs', type=int, default=512,
                   help='Isaac Gym Âπ∂Ë°åÁéØÂ¢ÉÊï∞ÔºàÊé®Ëçê 256-1024Ôºâ')
    
    # Ëø≠‰ª£ÁÆÄÊä•
    p.add_argument('--iter-log-file', type=str, default=None)
    # ÊµãËØïÈõÜÂÆûÊó∂È™åËØÅ
    p.add_argument('--test-verify-every', type=int, default=0, help='ÊØèÈöîNÊ¨°Ëø≠‰ª£Âú®ÊµãËØïÈõÜ‰∏äÈ™åËØÅÔºõ0=Á¶ÅÁî®')
    p.add_argument('--test-traj-preset', type=str, default='test_challenge', help='ÊµãËØïÈõÜËΩ®ËøπÈ¢ÑËÆæ')
    p.add_argument('--test-aggregate', type=str, default='harmonic', choices=['mean','min','harmonic'])
    p.add_argument('--test-disturbance', type=str, default='mild_wind', choices=[None,'mild_wind','stress'])
    p.add_argument('--test-duration', type=int, default=20, help='ÊµãËØïÈõÜËØÑ‰º∞Êó∂Èïø')
    p.add_argument('--test-clip-D', type=float, default=1.2, help='ÊµãËØïÈõÜDË£ÅÂâ™')
    # CMA-ES Ê∑∑ÂêàËÆ≠ÁªÉÔºàMCTSË¥üË¥£ÁªìÊûÑÔºåCMAË¥üË¥£ÂèÇÊï∞ÂæÆË∞ÉÔºâ
    p.add_argument('--cma-refine-every', type=int, default=0, help='ÊØèÈöîNÊ¨°MCTSËø≠‰ª£ÔºåÂØπÂΩìÂâçÊúÄ‰ºòÁ®ãÂ∫èÁöÑPIDÂèÇÊï∞ÂÅö‰∏ÄËΩÆCMA-ESÂæÆË∞ÉÔºõ0=Á¶ÅÁî®')
    p.add_argument('--cma-popsize', type=int, default=8, help='CMA-ESÁßçÁæ§Â§ßÂ∞èÔºàË∂äÂ∞èË∂äÂø´Ôºâ')
    p.add_argument('--cma-maxiter', type=int, default=20, help='CMA-ESÊØèÊ¨°ÂæÆË∞ÉÁöÑÊúÄÂ§ßËø≠‰ª£Êï∞')
    p.add_argument('--cma-sigma', type=float, default=0.15, help='CMA-ESÂàùÂßãÊ≠•ÈïøÔºàÁõ∏ÂØπ‰∫éÂΩìÂâçÂèÇÊï∞Ôºâ')
    p.add_argument('--cma-parallel', action='store_true', help='CMA-ESËØÑ‰º∞ÊòØÂê¶Âπ∂Ë°åÔºàÂÆûÈ™åÊÄßÔºâ')
    # Â§çÊ†∏/Èó®Êéß‰∏éÊ†áÂáÜÂåñÊµãËØïÈ™åËØÅÂ∑≤ÁßªÈô§ÔºåÊé•Âè£Á≤æÁÆÄ‰∏∫ËÆ≠ÁªÉÈõÜÂ∫¶Èáè
    # ÂàÜÊÆµÂ¢ûÈïø/Êé¢Á¥¢
    p.add_argument('--min-rules-guard', type=int, default=2)
    p.add_argument('--max-rules', type=int, default=8)
    p.add_argument('--add-rule-bias-base', type=int, default=2)
    p.add_argument('--min-rules-final', type=int, default=None)
    p.add_argument('--min-rules-ramp-start', type=float, default=0.30)
    p.add_argument('--min-rules-ramp-end', type=float, default=0.70)
    p.add_argument('--epsilon-max', type=float, default=0.25)
    p.add_argument('--epsilon-end-progress', type=float, default=0.30)
    p.add_argument('--swap-span', type=int, default=4)
    p.add_argument('--stagnation-window', type=int, default=0)
    p.add_argument('--epsilon-rebound', type=float, default=0.18)
    p.add_argument('--rebound-iters', type=int, default=80)
    p.add_argument('--rebound-decay-iters', type=int, default=0)
    p.add_argument('--rebound-target-eps', type=float, default=0.12)
    p.add_argument('--stagnation-seconds', type=int, default=0)
    p.add_argument('--epsilon-rebound-target', type=float, default=0.0)
    p.add_argument('--time-rebound-iters', type=int, default=0)
    p.add_argument('--diversity-bonus-max', type=float, default=0.0)
    p.add_argument('--diversity-end-progress', type=float, default=0.30)
    p.add_argument('--strict-bonus-scale', type=float, default=0.0)
    p.add_argument('--prefer-more-rules-tie-delta', type=float, default=0.0)
    p.add_argument('--prefer-fewer-rules-tie-delta', type=float, default=0.0)
    p.add_argument('--full-action-prob', type=float, default=0.0)
    p.add_argument('--allowed-cond-unaries', type=str, default='identity,abs')
    p.add_argument('--trig-as-phase-window', action='store_true')
    p.add_argument('--trig-lt-max', type=float, default=0.25)
    p.add_argument('--compose-by-gain', action='store_true')
    p.add_argument('--semantics', type=str, default=None, choices=[None,'first_match','compose_by_gain','blend_topk'])
    p.add_argument('--require-k', type=int, default=0)
    p.add_argument('--blend-topk-k', type=int, default=2)
    p.add_argument('--gain-slew-limit', type=str, default=None)
    p.add_argument('--min-hold-steps', type=int, default=0)
    p.add_argument('--clip-P', type=float, default=None)
    p.add_argument('--clip-I', type=float, default=None)
    p.add_argument('--clip-D', type=float, default=1.2)
    p.add_argument('--overlap-penalty', type=float, default=0.0)
    p.add_argument('--conflict-penalty', type=float, default=0.0)
    p.add_argument('--auto-unfreeze-patience', type=int, default=0)
    p.add_argument('--auto-unfreeze-steps', type=int, default=0)
    p.add_argument('--auto-unfreeze-penalty-scale', type=float, default=0.6)
    p.add_argument('--auto-unfreeze-eps-boost', type=float, default=0.15)
    # CMA-ES ËÅîÂêàË∞ÉÂèÇ„ÄÅTR„ÄÅÂÖàÈ™å‰∏éÂÄôË°•‰ºòÂåñÁ≠âÊâ©Â±ïÂ∑≤ÁßªÈô§ÔºåÊé•Âè£Á≤æÁÆÄ
    # ML-driven dynamic tuning (OFF by default)
    p.add_argument('--ml-scheduler', type=str, default='none', choices=['none','heuristic','nn'], help='ÂêØÁî®Âü∫‰∫éMLÁöÑMCTSÂä®ÊÄÅË∞ÉÂèÇÔºàÈªòËÆ§ÂÖ≥Èó≠Ôºâ')
    p.add_argument('--ml-interval', type=int, default=5, help='ÊØèÈöîNÊ¨°Ëø≠‰ª£ÊâßË°å‰∏ÄÊ¨°Ë∞ÉÂèÇÔºàÈªòËÆ§5Ôºâ')
    p.add_argument('--ml-warmup-iters', type=int, default=10, help='ÂâçNÊ¨°Ëø≠‰ª£‰∏çÂÅöË∞ÉÂèÇÔºå‰ªÖÊî∂ÈõÜ‰∏ä‰∏ãÊñáÔºàÈªòËÆ§10Ôºâ')
    p.add_argument('--ml-path', type=str, default='01_soar/results/nn_trained/ml_sched.pt', help='ÂΩì --ml-scheduler=nn Êó∂ÁöÑÊ®°ÂûãË∑ØÂæÑÔºàTorchScriptÔºâ')
    p.add_argument('--ml-strategy', type=str, default='absolute', choices=['absolute','delta'], help='Êõ¥Êñ∞ÊñπÂºèÔºöÁªùÂØπË¶ÜÁõñÊàñÂ¢ûÈáè')
    p.add_argument('--ml-allowed', type=str, default='pw_alpha,pw_c,_puct_enable,_puct_c,_edit_prior_c,_dirichlet_eps,_value_mix_lambda,_full_action_prob,_prefer_more_rules_tie_delta,_prefer_fewer_rules_tie_delta,_add_rule_bias_base,_epsilon_max', help='ÂÖÅËÆ∏Ë¢´MLÊõ¥Êñ∞ÁöÑÂèÇÊï∞ÁôΩÂêçÂçïÔºåÈÄóÂè∑ÂàÜÈöî')
    p.add_argument('--ml-safe-bounds', type=str, default='pw_alpha:0.4,1.0;pw_c:0.8,2.0;_puct_c:0.5,2.5;_dirichlet_eps:0.0,0.5;_edit_prior_c:0.0,1.0;_value_mix_lambda:0.0,0.3;_full_action_prob:0.0,0.9;_prefer_more_rules_tie_delta:0.0,0.1;_prefer_fewer_rules_tie_delta:0.0,0.1;_epsilon_max:0.05,0.6;_add_rule_bias_base:1,16', help='ÂÆâÂÖ®ËæπÁïåÔºåÊ†ºÂºè name:lo,hi;name2:lo,hi')
    p.add_argument('--ml-log', action='store_true', help='ÊâìÂç∞MLË∞ÉÂèÇÂèòÊõ¥Êó•Âøó')
    p.add_argument('--ml-dump-csv', type=str, default='', help='Â∞Ü ML Ë∞ÉÂèÇËÆ≠ÁªÉÊ†∑Êú¨ËøΩÂä†ÂÜôÂÖ• CSV Ë∑ØÂæÑÔºàÂàóÂõ∫ÂÆöÔºå‰æø‰∫éÁ¶ªÁ∫øÁõëÁù£ËÆ≠ÁªÉÔºâ')
    # Online policy training ‰∏éÊâãÂä® AlphaZero-lite ÊóãÈíÆÂ∑≤ÁßªÈô§ÔºåÁªü‰∏Ä‰∫§Áî± ML Â±Ç
    # ML Áã¨Âç†Ê®°ÂºèÔºöÈöêËóèÊâãÂä® MCTS ÊóãÈíÆÔºå‰∫§Áî± ML Ë∞ÉÂ∫¶
    p.add_argument('--ml-exclusive', action='store_true', help='Áã¨Âç†Ê®°ÂºèÔºöÂøΩÁï• MCTS ÊâãÂä®Ë∂ÖÂèÇÔºåÁªü‰∏ÄÁî± ML Ë∞ÉÂ∫¶ÔºàAlphaZero-lite ÈªòËÆ§Ôºâ')
    args = p.parse_args()
    # Ê†áÂáÜÂåñÈªòËÆ§Ôºö‰∏çÊåáÂÆö --semantics ÂàôÈªòËÆ§ compose_by_gain
    try:
        sem = getattr(args, 'semantics', None)
        if sem in (None, 'None'):
            setattr(args, 'compose_by_gain', True)
        else:
            s = str(sem).strip().lower()
            setattr(args, 'compose_by_gain', (s == 'compose_by_gain'))
    except Exception:
        pass
    return args

# ====================================================================
# Isaac Gym ÊâπÈáèËØÑ‰º∞ÂáΩÊï∞ÔºàGPU Âä†ÈÄüÔºâ
# ====================================================================

# ÂÖ®Â±Ä Isaac Gym ÁéØÂ¢ÉÊ±†ÔºàÈÅøÂÖçÈáçÂ§çÂàùÂßãÂåñÔºâ
_isaac_env_pool = None
_isaac_env_lock = None

def _get_isaac_env_pool(num_envs=512, duration=20.0):
    """Ëé∑ÂèñÊàñÂàõÂª∫ Isaac Gym ÁéØÂ¢ÉÊ±†ÔºàÂçï‰æãÊ®°ÂºèÔºâ"""
    global _isaac_env_pool, _isaac_env_lock
    
    if not ISAAC_GYM_AVAILABLE:
        return None
    
    if _isaac_env_pool is None:
        try:
            # Âª∂ËøüÂØºÂÖ•
            import sys, pathlib
            cur = pathlib.Path(__file__).resolve()
            env_path = cur.parent / 'envs'
            if str(env_path) not in sys.path:
                sys.path.insert(0, str(env_path))
            
            # Êó¢ÊîØÊåÅÂåÖÂÜÖÁõ∏ÂØπÂØºÂÖ•Ôºå‰πüÊîØÊåÅÈÄöËøásys.pathÊ≥®ÂÖ•ÁöÑÊú¨Âú∞ÂØºÂÖ•
            try:
                from .envs.isaac_gym_drone_env import IsaacGymDroneEnv  # type: ignore
            except Exception:
                from isaac_gym_drone_env import IsaacGymDroneEnv  # type: ignore
            
            print(f"[Isaac Gym] üöÄ ÂàùÂßãÂåñ GPU ÁéØÂ¢ÉÊ±†Ôºö{num_envs} Âπ∂Ë°åÁéØÂ¢É")
            _isaac_env_pool = IsaacGymDroneEnv(
                num_envs=num_envs,
                duration_sec=duration,
                headless=True,
                use_gpu=True
            )
            print("[Isaac Gym] ‚úÖ ÁéØÂ¢ÉÊ±†ÂàùÂßãÂåñÂÆåÊàê")
        except Exception as e:
            print(f"[Isaac Gym] ‚ùå ÂàùÂßãÂåñÂ§±Ë¥•Ôºö{e}")
            _isaac_env_pool = None
    
    return _isaac_env_pool


def _evaluate_batch_isaac_gym(
    programs: List,
    trajectories: List,
    duration: float,
    reward_weights: Dict,
    aggregate: str = 'harmonic',
    **kwargs
):
    """
    ‰ΩøÁî® Isaac Gym ÊâπÈáèËØÑ‰º∞Â§ö‰∏™Á®ãÂ∫è
    
    Args:
        programs: Á®ãÂ∫èÂàóË°®
        trajectories: ËΩ®ËøπÂàóË°®
        duration: ËØÑ‰º∞Êó∂Èïø
        reward_weights: Â•ñÂä±ÊùÉÈáç
        aggregate: ËÅöÂêàÊñπÂºè
        **kwargs: ÂÖ∂‰ªñÂèÇÊï∞
    
    Returns:
        scores: ÊØè‰∏™Á®ãÂ∫èÁöÑÂæóÂàÜ
    """
    env = _get_isaac_env_pool(num_envs=len(programs) * len(trajectories), duration=duration)
    
    if env is None:
        raise RuntimeError('Isaac Gym ÁéØÂ¢ÉÂàùÂßãÂåñÂ§±Ë¥•ÔºåÊó†Ê≥ïÁªßÁª≠ÊâπÈáèËØÑ‰º∞„ÄÇ')
    
    import torch
    import numpy as np
    
    try:
        # ÈáçÁΩÆÊâÄÊúâÁéØÂ¢É
        obs = env.reset()
        
        # ‰∏∫ÊØè‰∏™ÁéØÂ¢ÉÂàÜÈÖç (Á®ãÂ∫è, ËΩ®Ëøπ) ÂØπ
        num_programs = len(programs)
        num_trajs = len(trajectories)
        total_envs = num_programs * num_trajs
        
        # ÂàõÂª∫ÊéßÂà∂Âô®ÂÆû‰æãÔºàÊâπÈáèÔºâ
        controllers = []
        env_assignments = []  # (prog_idx, traj_idx)
        
        for prog_idx, prog in enumerate(programs):
            for traj_idx, traj in enumerate(trajectories):
                controller = MathProgramController(program=prog, suppress_init_print=True)
                controllers.append(controller)
                env_assignments.append((prog_idx, traj_idx))
        
        # ËøêË°å‰ªøÁúü
        num_steps = int(duration * env.control_freq)
        episode_rewards = np.zeros(total_envs)
        
        for step in range(num_steps):
            actions = []
            
            # ‰∏∫ÊØè‰∏™ÁéØÂ¢ÉËÆ°ÁÆóÊéßÂà∂
            for env_idx, (controller, (prog_idx, traj_idx)) in enumerate(zip(controllers, env_assignments)):
                # ‰ªéËßÇÊµãÊèêÂèñÁä∂ÊÄÅ
                state = {
                    'pos': obs['position'][env_idx],
                    'vel': obs['velocity'][env_idx],
                    'quat': obs['orientation'][env_idx],
                    'ang_vel': obs['angular_velocity'][env_idx]
                }
                
                # Ë∞ÉÁî®ÊéßÂà∂Âô®
                # TODO: ÈúÄË¶ÅÊ†πÊçÆËΩ®ËøπËÆ°ÁÆóÁõÆÊ†á‰ΩçÁΩÆ
                target_pos = np.array([0, 0, 1.0])  # ÁÆÄÂåñÁ§∫‰æã
                
                # ËÆ°ÁÆó RPM
                rpm = controller.computeControl(
                    control_timestep=1.0 / env.control_freq,
                    cur_pos=state['pos'],
                    cur_quat=state['quat'],
                    cur_vel=state['vel'],
                    cur_ang_vel=state['ang_vel'],
                    target_pos=target_pos,
                    target_rpy=np.array([0, 0, 0]),
                    target_vel=np.array([0, 0, 0])
                )
                
                actions.append(rpm)
            
            # ÊâπÈáèÊâßË°å
            actions_tensor = torch.tensor(np.array(actions), dtype=torch.float32, device=env.device)
            obs, rewards, dones, _ = env.step(actions_tensor)
            
            # Á¥ØÁßØÂ•ñÂä±
            episode_rewards += rewards.cpu().numpy()
        
        # ËÅöÂêàÊØè‰∏™Á®ãÂ∫èÁöÑÂæóÂàÜ
        program_scores = np.zeros(num_programs)
        
        for prog_idx in range(num_programs):
            # ÊèêÂèñËØ•Á®ãÂ∫èÂú®ÊâÄÊúâËΩ®Ëøπ‰∏äÁöÑÂ•ñÂä±
            traj_rewards = []
            for traj_idx in range(num_trajs):
                env_idx = prog_idx * num_trajs + traj_idx
                traj_rewards.append(episode_rewards[env_idx])
            
            # ËÅöÂêà
            if aggregate == 'harmonic':
                # Ë∞ÉÂíåÂπ≥Âùá
                program_scores[prog_idx] = len(traj_rewards) / np.sum(1.0 / (np.array(traj_rewards) + 1e-9))
            elif aggregate == 'min':
                program_scores[prog_idx] = np.min(traj_rewards)
            else:  # mean
                program_scores[prog_idx] = np.mean(traj_rewards)
        
        return program_scores.tolist()
    
    except Exception as e:
        print(f"[Isaac Gym] ‚ö†Ô∏è  ÊâπÈáèËØÑ‰º∞Â§±Ë¥•: {e}")
        import traceback
        traceback.print_exc()
        raise


def _worker_evaluate_single(packed):
    (traj, program, dur, suppress, _deep_quiet_ignored, disturbances, reward_weights, log_skip, in_memory, compose_by_gain, clip_P, clip_I, clip_D, pen_overlap, pen_conflict, semantics, require_k, blend_topk_k, gain_slew_limit, min_hold_steps) = packed
    cur = pathlib.Path(__file__).resolve(); root = cur.parent.parent
    if str(root) not in sys.path:
        sys.path.insert(0, str(root))
    def _core():
        # Fallback chain for controller import (prefer soar)
        try:
            from soar import MathProgramController as _PLC  # type: ignore
        except Exception:
            try:
                from program_executor import MathProgramController as _PLC  # type: ignore
            except Exception:
                import importlib
                _PLC = importlib.import_module('01_soar.program_executor').MathProgramController  # type: ignore
        from utilities.isaac_tester import SimulationTester as _ST  # type: ignore
        controller = _PLC(program=program, suppress_init_print=suppress)
        tester = _ST(
            controller=controller,
            test_scenarios=disturbances,
            output_folder='01_soar/results/mcts_eval',
            gui=False,
            weights=reward_weights,
            trajectory=traj,
            duration_sec=dur,
            log_skip=log_skip,
            in_memory=in_memory,
            quiet=suppress
        )
        reward = tester.run()
        # Êñ∞ÊéßÂà∂Âô®Êó†ÈáçÂè†ÁªüËÆ°ÔºåÂøΩÁï•Áõ∏ÂÖ≥ÊÉ©ÁΩö
        return reward
    if suppress:
        buf = io.StringIO()
        with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):
            return _core()
    return _core()

def main():
    args = parse_args()
    # Ëø≠‰ª£Êó•ÂøóÔºöÈªòËÆ§ÂÜôÂà∞ 01_soar
    from pathlib import Path as _Path
    iter_log_path = getattr(args, 'iter_log_file', None) or str(_Path('01_soar')/ 'results' / 'iter_log.csv')
    _Path(iter_log_path).parent.mkdir(parents=True, exist_ok=True)
    with open(iter_log_path, 'w', encoding='utf-8') as f:
        f.write('iter,short_best,rule_count,elapsed_s,it_per_s,epsilon,rebound_active\n')

    if args.seed is not None:
        import random, numpy as np
        random.seed(args.seed); np.random.seed(args.seed)
    # Â∑≤ÁßªÈô§ÂéÜÂè≤Ê∑±Â∫¶ÈùôÈü≥‰∏éÂ§ñÈÉ®‰æùËµñÊäëÂà∂ÈÄªËæë

    DSL_VARIABLES = [
        'err_p_roll', 'err_p_pitch', 'err_d_roll', 'err_d_pitch',
        'ang_vel_x', 'ang_vel_y', 'err_i_roll', 'err_i_pitch',
        'pos_err_x', 'pos_err_y', 'pos_err_z',
        'err_i_x', 'err_i_y', 'err_i_z',
        'pos_err_xy', 'rpy_err_mag', 'ang_vel_mag', 'pos_err_z_abs'
    ]
    # Â¢ûÂä† PID ÂèÇÊï∞ÁöÑÂÖ≥ÈîÆÂå∫ÂüüÂØÜÂ∫¶ (0.5-2.5)
    DSL_CONSTANTS = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.2, 2.5, 3.0, 5.0]
    DSL_OPERATORS = ['+', '-', '*', '/', 'abs', '>', '<', 'max', 'min', 'sin', 'cos', 'tan', 'log1p', 'sqrt']

    if args.traj_list:
        traj_names = args.traj_list
    else:
        if getattr(args, 'traj_preset', None) == 'pi_strong_train':
            traj_names = ['zigzag3d','lemniscate3d','random_wp','spiral_in_out','stairs','coupled_surface']
        else:
            traj_names = [args.traj]
    trajectories = [build_trajectory(n) for n in traj_names]
    disturbances = build_disturbances(args.disturbance)
    reward_weights, reward_ks = get_reward_profile(args.reward_profile)
    print(describe_profile(args.reward_profile))

    # Â∞ÅÈó≠ MCTS È¢ÑËÆæÔºöÁî± ML Â±ÇËá™ÈÄÇÂ∫îÔºåÊó†ÈúÄÈ¢ÑËÆæ

    short_iters = int(args.iters * max(0.0, min(1.0, args.short_frac)))

    def _ast_to_str_local(node):
        if isinstance(node, BinaryOpNode):
            return f"({_ast_to_str_local(node.left)} {node.op} {_ast_to_str_local(node.right)})"
        if isinstance(node, UnaryOpNode):
            return f"{node.op}({_ast_to_str_local(node.child)})"
        if isinstance(node, TerminalNode):
            return str(node.value)
        return str(node)
    def hash_program(program: list) -> str:
        import hashlib as _hl
        parts=[]
        for rule in program:
            cond=_ast_to_str_local(rule['condition'])
            acts=[]
            for a in rule['action']:
                if isinstance(a,BinaryOpNode) and a.op=='set' and isinstance(a.left,TerminalNode) and isinstance(a.right,TerminalNode):
                    acts.append(f"{a.left.value}:{a.right.value}")
            parts.append(cond+"|"+",".join(sorted(acts)))
        raw="||".join(parts)
        return _hl.sha1(raw.encode('utf-8')).hexdigest()
    class LRUCache:
        def __init__(self, capacity:int):
            self.capacity=capacity
            self.store: OrderedDict[str, float] = OrderedDict()
        def get(self, k:str):
            if k not in self.store:
                return None
            v=self.store.pop(k)
            self.store[k]=v
            return v
        def put(self, k:str, v:float):
            if self.capacity<=0:
                return
            if k in self.store:
                self.store.pop(k)
            elif len(self.store)>=self.capacity:
                self.store.popitem(last=False)
            self.store[k]=v
        def __len__(self):
            return len(self.store)
    cache = LRUCache(args.cache_size)
    true_holder: Dict[str, Any] = {'map': {}, 'version': 0}
    def _record_true(program: list, val: float):
        try:
            h = hash_program(program)
            true_holder['map'][h] = float(val)
            true_holder['version'] = int(true_holder.get('version', 0)) + 1
        except Exception:
            pass

    env_pool: Dict[Tuple[int,int], Any] = {}
    agent_holder: Dict[str, Any] = {'agent': None}
    pool_holder: Dict[str, Any] = {'pool': None, 'enabled': False, 'worker_n': 0}
    if args.parallel_traj:
        try:
            import multiprocessing as mp
            # Interpret num-workers semantics:
            #   0 -> disable pool (single-process)
            #  >1 -> exactly that many workers
            #  -1 -> auto (cpu_count-1, at least 2)
            # prefer multiprocessing.cpu_count() to avoid static analyzers complaining about os
            try:
                _cpu_n = int(mp.cpu_count())  # type: ignore[attr-defined]
            except Exception:
                _cpu_n = 2
            auto_n = max(2, (_cpu_n or 2) - 1)
            if args.num_workers == 0:
                pool_holder['worker_n'] = 0
                pool_holder['enabled'] = False
                print('[Parallel] num-workers=0 -> Á¶ÅÁî®ËøõÁ®ãÊ±†ÔºàÂçïËøõÁ®ãËØÑ‰º∞Ôºâ')
            elif args.num_workers < 0:
                pool_holder['worker_n'] = auto_n
                pool_holder['enabled'] = (pool_holder['worker_n'] > 1 and len(trajectories) > 1)
            else:
                pool_holder['worker_n'] = int(args.num_workers)
                pool_holder['enabled'] = (pool_holder['worker_n'] > 1 and len(trajectories) > 1)
            if pool_holder['enabled']:
                ctx = mp.get_context('spawn')
                pool_holder['pool'] = ctx.Pool(processes=pool_holder['worker_n'])
        except Exception as _pe:
            print(f"[Parallel][WARN] ÂàõÂª∫ËøõÁ®ãÊ±†Â§±Ë¥•ÔºåÂõûÈÄÄÂçïËøõÁ®ã: {_pe}")
            pool_holder['enabled'] = False

    T = len(trajectories)
    traj_batch_size = args.traj_batch_size if args.traj_batch_size and args.traj_batch_size>0 else T
    traj_batch_size = min(max(1, traj_batch_size), T)
    def get_traj_batch_for_iter(iter_idx: int) -> Tuple[list, str]:
        if traj_batch_size >= T:
            return list(range(T)), f"allT{T}"
        start = (iter_idx * traj_batch_size) % T
        idxs = [(start + k) % T for k in range(traj_batch_size)]
        batch_id = "b" + "-".join(str(i) for i in idxs)
        return idxs, batch_id

    def dynamic_eval(program: list):
        ag = agent_holder['agent']
        cur_it = getattr(ag, 'total_iterations_done', 0) if ag is not None else 0
        dur = args.full_duration if cur_it >= short_iters else args.short_duration
        if dur == args.short_duration:
            idxs = list(range(T))
            batch_id = f"allT{T}_short"
            selected_trajs = [trajectories[i] for i in idxs]
        else:
            idxs, batch_id = get_traj_batch_for_iter(cur_it)
            selected_trajs = [trajectories[i] for i in idxs]
        if ag is not None and hasattr(ag, '_tt_salt'):
            try:
                ag._tt_salt = f"{dur}|{batch_id}"
            except Exception:
                pass
        prog_hash = hash_program(program)
        tv = int(true_holder.get('version', 0))
        ver_suffix = f"|TV{tv}" if float(getattr(args, 'true_mixin_alpha', 0.0) or 0.0) > 0.0 else ""
        cache_key = f"{dur}|{batch_id}|{prog_hash}{ver_suffix}"
        cached = cache.get(cache_key)
        if cached is not None:
            return cached
        scores = []
        if pool_holder['enabled']:
            packs = [(
                traj, program, dur, args.quiet_sim, False,
                disturbances, reward_weights, args.log_skip, args.in_memory_log,
                bool(getattr(args, 'compose_by_gain', False)),
                getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2),
                getattr(args,'gain_slew_limit', None), int(getattr(args,'min_hold_steps',0) or 0)
            ) for traj in selected_trajs]
            try:
                scores = pool_holder['pool'].map(_worker_evaluate_single, packs)  # type: ignore
            except Exception as _pm:
                print(f"[Parallel][WARN] ËøõÁ®ãÊ±†ÊâßË°åÂ§±Ë¥•ÔºåÂõûÈÄÄÂçïËøõÁ®ã: {_pm}")
                pool_holder['enabled'] = False
                for traj in selected_trajs:
                    scores.append(_worker_evaluate_single((
                        traj, program, dur, args.quiet_sim, False,
                        disturbances, reward_weights, args.log_skip, args.in_memory_log,
                        bool(getattr(args, 'compose_by_gain', False)),
                        getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                        float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                        getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2),
                        getattr(args,'gain_slew_limit', None), int(getattr(args,'min_hold_steps',0) or 0)
                    )))
        else:
            for bi, ti in enumerate(idxs):
                traj = trajectories[ti]
                controller = MathProgramController(program=program, suppress_init_print=args.quiet_sim)
                tester = SimulationTester(controller=controller,
                                          test_scenarios=disturbances,
                                          output_folder='01_soar/results/mcts_eval',
                                          gui=False,
                                          weights=reward_weights,
                                          trajectory=traj,
                                          duration_sec=dur,
                                          log_skip=args.log_skip,
                                          in_memory=args.in_memory_log,
                                          quiet=args.quiet_sim)
                rew = tester.run()
                # ÂøΩÁï• overlap/conflict ÊÉ©ÁΩöÔºà‰∏çÈÄÇÁî®‰∫éÊï∞Â≠¶ÂéüËØ≠ÊéßÂà∂Ôºâ
                scores.append(rew)
        if args.aggregate == 'mean':
            value = float(sum(scores)/len(scores))
        elif args.aggregate == 'min':
            value = float(min(scores))
        elif args.aggregate == 'harmonic':
            import math
            value = len(scores)/sum(1/(s+1e-9) for s in scores)
        else:
            value = float(sum(scores)/len(scores))

        # ÊäóËøáÊãüÂêàÔºöÂë®ÊúüÊÄßÂÖ®Êó∂ÈïøÊé¢ÈíàÊ∑∑Âêà
        _mix_used = False
        try:
            _mix_every = int(getattr(args, 'fullmix_every', 0) or 0)
            _mix_frac_cfg = float(getattr(args, 'fullmix_frac', 0.0) or 0.0)
            if _mix_every > 0 and _mix_frac_cfg > 0.0:
                _progress = float(cur_it / max(1, total))
                _rs = float(getattr(args, 'fullmix_ramp_start', 0.0) or 0.0)
                _re = float(getattr(args, 'fullmix_ramp_end', 0.0) or 0.0)
                if _re > _rs and _re > 0.0:
                    _k = (_progress - _rs) / max(1e-9, (_re - _rs))
                    _k = min(1.0, max(0.0, _k))
                    _mix_frac = _mix_frac_cfg * _k
                else:
                    _mix_frac = _mix_frac_cfg
                if _mix_frac > 0.0 and (cur_it % _mix_every == 0):
                    _v_full = float(full_eval(program))
                    value = float((1.0 - _mix_frac) * float(value) + _mix_frac * _v_full)
                    _mix_used = True
        except Exception:
            _mix_used = False
        mix_alpha = float(getattr(args, 'true_mixin_alpha', 0.0) or 0.0)
        if mix_alpha > 0.0:
            try:
                tmap = true_holder.get('map', {})
                if prog_hash in tmap:
                    tval = float(tmap[prog_hash])
                    value = float((1.0 - mix_alpha) * float(value) + mix_alpha * tval)
            except Exception:
                pass
        # Ëã•ÂêØÁî®Ê∑∑ÂêàÔºåÂàô‰∏çÁºìÂ≠òÊ∑∑ÂêàÂÄºÔºåÈÅøÂÖçÊ±°ÊüìÁü≠ËØÑ‰º∞ÁºìÂ≠ò
        if not _mix_used:
            cache.put(cache_key, value)
        return value

    def short_eval_all(program: list):
        dur = args.short_duration
        prog_hash = hash_program(program)
        cache_key = f"{dur}|allT{len(trajectories)}|{prog_hash}|SHORT"
        cached = cache.get(cache_key)
        if cached is not None:
            return cached
        ag = agent_holder.get('agent')
        if ag is not None and hasattr(ag, '_tt_salt'):
            try:
                ag._tt_salt = f"{dur}|allT{len(trajectories)}|SHORT"
            except Exception:
                pass
        scores = []
        if pool_holder['enabled']:
            packs = [(
                traj, program, dur, args.quiet_sim, False,
                disturbances, reward_weights, args.log_skip, args.in_memory_log,
                bool(getattr(args, 'compose_by_gain', False)),
                getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2),
                getattr(args,'gain_slew_limit', None), int(getattr(args,'min_hold_steps',0) or 0)
            ) for traj in trajectories]
            try:
                scores = pool_holder['pool'].map(_worker_evaluate_single, packs)  # type: ignore
            except Exception as _pm:
                print(f"[Parallel][WARN] ËøõÁ®ãÊ±†ÊâßË°åÂ§±Ë¥•(short_eval_all)ÔºåÂõûÈÄÄÂçïËøõÁ®ã: {_pm}")
                for traj in trajectories:
                    scores.append(_worker_evaluate_single((
                        traj, program, dur, args.quiet_sim, False,
                        disturbances, reward_weights, args.log_skip, args.in_memory_log,
                        bool(getattr(args, 'compose_by_gain', False)),
                        getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                        float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                        getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2)
                    )))
        else:
            for ti, traj in enumerate(trajectories):
                controller = MathProgramController(program=program, suppress_init_print=args.quiet_sim)
                tester = SimulationTester(controller=controller,
                                          test_scenarios=disturbances,
                                          output_folder='01_soar/results/mcts_eval',
                                          gui=False,
                                          weights=reward_weights,
                                          trajectory=traj,
                                          duration_sec=dur,
                                          log_skip=args.log_skip,
                                          in_memory=args.in_memory_log,
                                          quiet=args.quiet_sim)
                rew = tester.run()
                # ÂøΩÁï• overlap/conflict ÊÉ©ÁΩö
                scores.append(rew)
        if args.aggregate == 'mean':
            value = float(sum(scores)/len(scores))
        elif args.aggregate == 'min':
            value = float(min(scores))
        elif args.aggregate == 'harmonic':
            import math
            value = len(scores)/sum(1/(s+1e-9) for s in scores)
        else:
            value = float(sum(scores)/len(scores))
        cache.put(cache_key, value)
        return value

    def full_eval(program: list):
        dur = args.full_duration
        prog_hash = hash_program(program)
        tv = int(true_holder.get('version', 0))
        ver_suffix = f"|TV{tv}" if float(getattr(args, 'true_mixin_alpha', 0.0) or 0.0) > 0.0 else ""
        cache_key = f"{dur}|allT{len(trajectories)}|{prog_hash}{ver_suffix}"
        cached = cache.get(cache_key)
        if cached is not None:
            return cached
        ag = agent_holder.get('agent')
        if ag is not None and hasattr(ag, '_tt_salt'):
            try:
                ag._tt_salt = f"{dur}|allT{len(trajectories)}"
            except Exception:
                pass
        scores = []
        if pool_holder['enabled']:
            packs = [(
                traj, program, dur, args.quiet_sim, False,
                disturbances, reward_weights, args.log_skip, args.in_memory_log,
                bool(getattr(args, 'compose_by_gain', False)),
                getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2),
                getattr(args,'gain_slew_limit', None), int(getattr(args,'min_hold_steps',0) or 0)
            ) for traj in trajectories]
            try:
                scores = pool_holder['pool'].map(_worker_evaluate_single, packs)  # type: ignore
            except Exception as _pm:
                print(f"[Parallel][WARN] ËøõÁ®ãÊ±†ÊâßË°åÂ§±Ë¥•(full_eval)ÔºåÂõûÈÄÄÂçïËøõÁ®ã: {_pm}")
                for traj in trajectories:
                    scores.append(_worker_evaluate_single((
                        traj, program, dur, args.quiet_sim, False,
                        disturbances, reward_weights, args.log_skip, args.in_memory_log,
                        bool(getattr(args, 'compose_by_gain', False)),
                        getattr(args,'clip_P',None), getattr(args,'clip_I',None), getattr(args,'clip_D',None),
                        float(getattr(args,'overlap_penalty',0.0) or 0.0), float(getattr(args,'conflict_penalty',0.0) or 0.0),
                        getattr(args,'semantics', None), int(getattr(args,'require_k',0) or 0), int(getattr(args,'blend_topk_k',2) or 2)
                    )))
        else:
            for ti, traj in enumerate(trajectories):
                # Áªü‰∏Ä‰ΩøÁî®Êï∞Â≠¶ÂéüËØ≠ÊéßÂà∂Âô®
                controller = MathProgramController(program=program, suppress_init_print=args.quiet_sim)
                tester = SimulationTester(controller=controller,
                                          test_scenarios=disturbances,
                                          output_folder='01_soar/results/mcts_eval',
                                          gui=False,
                                          weights=reward_weights,
                                          trajectory=traj,
                                          duration_sec=dur,
                                          log_skip=args.log_skip,
                                          in_memory=args.in_memory_log,
                                          quiet=args.quiet_sim)
                rew = tester.run()
                try:
                    if (getattr(args,'overlap_penalty',0.0) or 0.0)>0 or (getattr(args,'conflict_penalty',0.0) or 0.0)>0:
                        metrics = controller.get_overlap_metrics()
                        mean_overlap = float(metrics.get('mean_overlap', 1.0))
                        mean_action_diff = float(metrics.get('mean_action_diff', 0.0))
                        rew = float(rew) - float(mean_overlap) * float(getattr(args,'overlap_penalty',0.0) or 0.0) - float(mean_action_diff) * float(getattr(args,'conflict_penalty',0.0) or 0.0)
                except Exception:
                    pass
                scores.append(rew)
        if args.aggregate == 'mean':
            value = float(sum(scores)/len(scores))
        elif args.aggregate == 'min':
            value = float(min(scores))
        elif args.aggregate == 'harmonic':
            import math
            value = len(scores)/sum(1/(s+1e-9) for s in scores)
        else:
            value = float(sum(scores)/len(scores))
        mix_alpha = float(getattr(args, 'true_mixin_alpha', 0.0) or 0.0)
        if mix_alpha > 0.0:
            try:
                tmap = true_holder.get('map', {})
                if prog_hash in tmap:
                    tval = float(tmap[prog_hash])
                    value = float((1.0 - mix_alpha) * float(value) + mix_alpha * tval)
            except Exception:
                pass
        cache.put(cache_key, value)
        return value

    def full_eval_true(program: list):
        dur = args.full_duration
        prog_hash = hash_program(program)
        cache_key = f"{dur}|allT{len(trajectories)}|{prog_hash}|TRUE"
        cached = cache.get(cache_key)
        if cached is not None:
            return cached
        ag = agent_holder.get('agent')
        if ag is not None and hasattr(ag, '_tt_salt'):
            try:
                ag._tt_salt = f"{dur}|allT{len(trajectories)}|TRUE"
            except Exception:
                pass
        scores = []
        for ti, traj in enumerate(trajectories):
            controller = MathProgramController(program=program, suppress_init_print=args.quiet_sim)
            tester = SimulationTester(controller=controller,
                                      test_scenarios=disturbances,
                                      output_folder='01_soar/results/mcts_eval',
                                      gui=False,
                                      weights=reward_weights,
                                      trajectory=traj,
                                      duration_sec=dur,
                                      log_skip=args.log_skip,
                                      in_memory=args.in_memory_log,
                                      quiet=args.quiet_sim)
            rew = tester.run()
            scores.append(float(rew))
        if args.aggregate == 'mean':
            value = float(sum(scores)/len(scores))
        elif args.aggregate == 'min':
            value = float(min(scores))
        elif args.aggregate == 'harmonic':
            import math
            value = len(scores)/sum(1/(s+1e-9) for s in scores)
        else:
            value = float(sum(scores)/len(scores))
        cache.put(cache_key, value)
        return value

    # mini_full_eval ‰∏é TR Áõ∏ÂÖ≥ÈÄªËæëÂ∑≤ÁßªÈô§ÔºåÁÆÄÂåñËÆ≠ÁªÉÊµÅÁ®ã

    evaluation_func = dynamic_eval

    warm_prog = None
    if getattr(args, 'warm_start_cmaes', False):
        try:
            with open(args.warm_start_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, dict) and 'rules' in data:
                try:
                    prog = deserialize_program(data)
                    if isinstance(prog, list) and len(prog) > 0:
                        warm_prog = prog
                        kp = ki = kd = None
                        try:
                            for a in prog[0].get('action', []):
                                if isinstance(a, BinaryOpNode) and a.op == 'set' and isinstance(a.left, TerminalNode) and isinstance(a.right, TerminalNode):
                                    if a.left.value == 'P': kp = float(a.right.value)
                                    elif a.left.value == 'I': ki = float(a.right.value)
                                    elif a.left.value == 'D': kd = float(a.right.value)
                        except Exception:
                            pass
                        if all(v is not None for v in (kp,ki,kd)):
                            print(f"[WarmStart] ËΩΩÂÖ• best_program (CMA-ES) -> P={kp:.4f} I={ki:.4f} D={kd:.4f}")
                        else:
                            print("[WarmStart] ËΩΩÂÖ• best_program (CMA-ES) -> ÂçïËßÑÂàôÂ∑≤Ê≥®ÂÖ•")
                    else:
                        print('[WarmStart][WARN] best_program JSON ‰∏≠ rules ‰∏∫Á©∫ÔºåË∑≥Ëøá warm start')
                except Exception as _dp_e:
                    print(f"[WarmStart][WARN] Ëß£Êûê best_program JSON Â§±Ë¥•ÔºåÂ∞ÜÂ∞ùËØï legacy Ê†ºÂºè: {_dp_e}")
            if warm_prog is None:
                params = data.get('best_params') or data.get('best_params'.upper())
                if isinstance(params, list) and len(params) >= 3:
                    kp, ki, kd = params[:3]
                    condition = BinaryOpNode('>', TerminalNode('pos_err_x'), TerminalNode(-999.0))
                    action = [
                        BinaryOpNode('set', TerminalNode('P'), TerminalNode(round(float(kp),4))),
                        BinaryOpNode('set', TerminalNode('I'), TerminalNode(round(float(ki),4))),
                        BinaryOpNode('set', TerminalNode('D'), TerminalNode(round(float(kd),4)))
                    ]
                    warm_prog = [{'condition': condition, 'action': action}]
                    print(f"[WarmStart] Loaded CMA-ES gains P={kp:.4f} I={ki:.4f} D={kd:.4f} -> Ê≥®ÂÖ•ÂàùÂßãÁ®ãÂ∫è")
                else:
                    print('[WarmStart][WARN] Êú™ÊâæÂà∞ best_paramsÔºåË∑≥Ëøá warm start')
        except Exception as e:
            print(f'[WarmStart][ERROR] ËØªÂèñ {args.warm_start_path} Â§±Ë¥•: {e}')
    if getattr(args, 'warm_start_program', None):
        try:
            pj = str(getattr(args, 'warm_start_program'))
            with open(pj, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, dict) and 'rules' in data:
                warm_prog = deserialize_program(data)
                print(f"[WarmStart] ‰ªé SOAR Á®ãÂ∫èËΩΩÂÖ•: {pj} -> ËßÑÂàôÊï∞={len(warm_prog) if isinstance(warm_prog, list) else 'N/A'}")
            else:
                print(f"[WarmStart][WARN] {pj} ‰∏çÊòØÂåÖÂê´ 'rules' ÁöÑÁ®ãÂ∫è JSONÔºåÂøΩÁï•")
        except Exception as e:
            print(f"[WarmStart][ERROR] Âä†ËΩΩ --warm-start-program Â§±Ë¥•: {e}")

    def _temp_eval(prog:list):
        scores=[]
        for traj in trajectories:
            controller = MathProgramController(program=prog, suppress_init_print=args.quiet_sim)
            tester = SimulationTester(controller=controller,
                                      test_scenarios=disturbances,
                                      output_folder='01_soar/results/mcts_eval',
                                      gui=False,
                                      weights=reward_weights,
                                      trajectory=traj,
                                      duration_sec=args.full_duration,
                                      log_skip=args.log_skip,
                                      in_memory=args.in_memory_log,
                                      quiet=args.quiet_sim)
            scores.append(tester.run())
        if args.aggregate == 'mean':
            return float(sum(scores)/len(scores))
        elif args.aggregate == 'min':
            return float(min(scores))
        elif args.aggregate == 'harmonic':
            import math
            return len(scores)/sum(1/(s+1e-9) for s in scores)
        return float(sum(scores)/len(scores))

    # CMA-Joint Áõ∏ÂÖ≥Â∑•ÂÖ∑‰∏éÈÄªËæëÂ∑≤Âà†Èô§ÔºåÁªü‰∏ÄÁî± MCTS ÊêúÁ¥¢‰∏é ML Ë∞ÉÂ∫¶Ë¥üË¥£

    base_complexity = 0.0
    print('[Config] Â§çÊùÇÂ∫¶ÊÉ©ÁΩöÔºöÂ∑≤ÂÖ®Â±ÄÂÖ≥Èó≠ (complexity_penalty=0)')

    agent = MCTS_Agent(
        _temp_eval,
        DSL_VARIABLES,
        DSL_CONSTANTS,
        DSL_OPERATORS,
        max_depth=args.mcts_max_depth,
        rollout_depth=args.rollout_depth,
        complexity_penalty=base_complexity,
        pw_alpha=args.pw_alpha,
        pw_c=args.pw_c,
        warm_start_program=warm_prog
    )
    agent._complexity_min_scale = 0.0
    agent._complexity_max_scale = 0.0
    agent._complexity_ramp_start = 0.0
    agent._complexity_ramp_end = 0.0
    if hasattr(agent, '_dynamic_complexity'):
        agent._dynamic_complexity = 0.0
    if hasattr(agent, '_min_rules_guard'):
        agent._min_rules_guard = max(1, int(args.min_rules_guard))
    if hasattr(agent, '_max_rules'):
        agent._max_rules = max(2, int(args.max_rules))
    if hasattr(agent, '_add_rule_bias_base'):
        agent._add_rule_bias_base = max(1, int(args.add_rule_bias_base))

    try:
        if hasattr(agent, '_epsilon_max'):
            agent._epsilon_max = float(args.epsilon_max)
        if hasattr(agent, '_epsilon_end_progress'):
            agent._epsilon_end_progress = float(args.epsilon_end_progress)
        if hasattr(agent, '_swap_span'):
            agent._swap_span = int(args.swap_span)
        if hasattr(agent, '_stagnation_window'):
            agent._stagnation_window = int(args.stagnation_window)
        if hasattr(agent, '_epsilon_rebound'):
            agent._epsilon_rebound = float(args.epsilon_rebound)
        if hasattr(agent, '_rebound_iters'):
            agent._rebound_iters = int(args.rebound_iters)
        if hasattr(agent, '_diversity_bonus_max'):
            agent._diversity_bonus_max = float(args.diversity_bonus_max)
        if hasattr(agent, '_diversity_end_progress'):
            agent._diversity_end_progress = float(args.diversity_end_progress)
        if hasattr(agent, '_strict_bonus_scale'):
            agent._strict_bonus_scale = float(args.strict_bonus_scale)
        if hasattr(agent, '_prefer_more_rules_tie_delta'):
            agent._prefer_more_rules_tie_delta = float(getattr(args, 'prefer_more_rules_tie_delta', 0.0))
        if hasattr(agent, '_prefer_fewer_rules_tie_delta'):
            agent._prefer_fewer_rules_tie_delta = float(getattr(args, 'prefer_fewer_rules_tie_delta', 0.0))
        if hasattr(agent, '_full_action_prob'):
            agent._full_action_prob = float(getattr(args, 'full_action_prob', 0.0))
        if hasattr(agent, '_allowed_cond_unaries'):
            try:
                allow_raw = getattr(args, 'allowed_cond_unaries', 'identity,abs') or 'identity,abs'
                allow_set = set([s.strip() for s in allow_raw.split(',') if s.strip()])
                if 'identity' not in allow_set:
                    allow_set.add('identity')
                agent._allowed_cond_unaries = allow_set
                print(f"[DSL] allowed cond unaries: {sorted(list(agent._allowed_cond_unaries))}")
            except Exception as _au_e:
                print(f"[DSL][WARN] Ëß£Êûê --allowed-cond-unaries Â§±Ë¥•Ôºå‰ΩøÁî®ÈªòËÆ§: {_au_e}")
        if hasattr(agent, '_trig_as_phase_window'):
            agent._trig_as_phase_window = bool(getattr(args, 'trig_as_phase_window', False))
        if hasattr(agent, '_trig_lt_max'):
            agent._trig_lt_max = float(getattr(args, 'trig_lt_max', 0.25))
        if hasattr(agent, '_enable_macros'):
            agent._enable_macros = bool(getattr(args, 'enable_macros', False))
        if hasattr(agent, '_edit_credit_mode'):
            agent._edit_credit_mode = str(getattr(args, 'edit_credit', 'off') or 'off')
        if hasattr(agent, '_edit_credit_c'):
            agent._edit_credit_c = float(getattr(args, 'edit_credit_c', 0.8) or 0.8)
        # MCTS ÂÖàÈ™åÔºàNN/ÂêØÂèëÂºèÔºâ‰∏éÊâãÂä®Ê≥®ÂÖ•Â∑≤ÁßªÈô§ÔºåÁªü‰∏ÄÁî± ML Â±ÇË∞ÉÂèÇÈ©±Âä®
    except Exception as _exp_e:
        print(f"[Explore][WARN] Ê≥®ÂÖ• epsilon/swap ÂèÇÊï∞Â§±Ë¥•: {_exp_e}")

    try:
        if hasattr(agent, '_min_rules_guard_initial'):
            agent._min_rules_guard_initial = int(agent._min_rules_guard)
        if hasattr(agent, '_min_rules_guard_final'):
            if args.min_rules_final is not None:
                agent._min_rules_guard_final = max(1, int(args.min_rules_final))
            else:
                agent._min_rules_guard_final = max(1, int(agent._min_rules_guard))
        if hasattr(agent, '_min_rules_ramp_start'):
            agent._min_rules_ramp_start = float(args.min_rules_ramp_start)
        if hasattr(agent, '_min_rules_ramp_end'):
            agent._min_rules_ramp_end = float(args.min_rules_ramp_end)
        if hasattr(agent, '_min_rules_guard_effective'):
            agent._min_rules_guard_effective = int(agent._min_rules_guard)
    except Exception as _dyn_min_e:
        print(f"[MinRules][WARN] Ê≥®ÂÖ•Âä®ÊÄÅ‰∏ãÈôêÂèÇÊï∞Â§±Ë¥•: {_dyn_min_e}")

    try:
        if hasattr(agent, 'root') and hasattr(agent, '_min_rules_guard'):
            pad_ok = True
            if warm_prog is not None and not getattr(args, 'pad_after_warm_start', False):
                pad_ok = False
            if hasattr(agent, '_pad_after_warm_start'):
                agent._pad_after_warm_start = bool(getattr(args, 'pad_after_warm_start', False))
            if pad_ok:
                cur_rules = len(agent.root.program) if agent.root and hasattr(agent.root, 'program') else 0
                while cur_rules < agent._min_rules_guard and cur_rules < getattr(agent, '_max_rules', 8):
                    agent.root.program.append(agent._generate_random_rule())
                    cur_rules += 1
            else:
                print('[WarmStart] ‰øùÊåÅÂçïÂàÜÊÆµÔºå‰∏çÂú® warm start ÂêéÂº∫Ë°åË°•ÈΩêÂà∞ min-rules-guard')
    except Exception as _init_seg_e:
        print(f"[InitSeg][WARN] Ë°•ÈΩêÊúÄÂ∞èÂàÜÊÆµÂ§±Ë¥•: {_init_seg_e}")

    agent_holder['agent'] = agent
    agent.evaluation_function = dynamic_eval

    # --- ML Áã¨Âç†Ê®°ÂºèÔºöÈîÅÂÆö MCTS Ë∂ÖÂèÇ‰∏∫Á®≥ÂÅ•ÈªòËÆ§ÔºåÁî± ML ÊåÅÊúâ ---
    if bool(getattr(args, 'ml_exclusive', False)):
        try:
            if hasattr(agent, '_puct_enable'):
                agent._puct_enable = True
            if hasattr(agent, '_puct_c'):
                agent._puct_c = 1.0
            if hasattr(agent, '_dirichlet_eps'):
                agent._dirichlet_eps = 0.20
            if hasattr(agent, '_dirichlet_alpha'):
                agent._dirichlet_alpha = 0.3
            if hasattr(agent, '_value_mix_lambda'):
                agent._value_mix_lambda = 0.10
            agent.pw_alpha = 0.82
            agent.pw_c = 1.10
            if hasattr(agent, '_add_rule_bias_base'):
                agent._add_rule_bias_base = max(6, int(getattr(agent, '_add_rule_bias_base', 2)))
            if hasattr(agent, '_full_action_prob'):
                agent._full_action_prob = max(0.55, float(getattr(agent, '_full_action_prob', 0.0)))
            if hasattr(agent, '_prefer_more_rules_tie_delta'):
                agent._prefer_more_rules_tie_delta = max(0.02, float(getattr(agent, '_prefer_more_rules_tie_delta', 0.0)))
            if hasattr(agent, '_prefer_fewer_rules_tie_delta'):
                agent._prefer_fewer_rules_tie_delta = 0.0
            if hasattr(agent, '_epsilon_max'):
                agent._epsilon_max = min(0.25, float(getattr(agent, '_epsilon_max', 0.25)))
            print('[ML-Exclusive] AlphaZero-lite defaults applied. Manual MCTS knobs are ignored; ML layer owns them.')
        except Exception as _mx_e:
            print(f"[ML-Exclusive][WARN] ÂàùÂßãÂåñÈªòËÆ§Â§±Ë¥•Ôºö{_mx_e}")

    # --- ML scheduler wiring (optional) ---
    ml_sched = None
    ml_allowed = set([s.strip() for s in str(getattr(args,'ml_allowed','')).split(',') if s.strip()])
    try:
        from .ml_param_scheduler import HeuristicScheduler, NNScheduler, MCTSContext, parse_bounds_spec, apply_mcts_param_updates  # type: ignore
    except Exception:
        # Fallback 1: direct import from current package directory (script mode)
        try:
            import ml_param_scheduler as _mls  # type: ignore
            HeuristicScheduler = getattr(_mls, 'HeuristicScheduler')
            NNScheduler = getattr(_mls, 'NNScheduler')
            MCTSContext = getattr(_mls, 'MCTSContext')
            parse_bounds_spec = getattr(_mls, 'parse_bounds_spec')
            apply_mcts_param_updates = getattr(_mls, 'apply_mcts_param_updates')
        except Exception:
            # Fallback 2: dynamic package alias created earlier (_PKG_NAME)
            import importlib as _il
            _mod = _il.import_module(f"{_PKG_NAME}.ml_param_scheduler")  # type: ignore[name-defined]
            HeuristicScheduler = getattr(_mod, 'HeuristicScheduler')
            NNScheduler = getattr(_mod, 'NNScheduler')
            MCTSContext = getattr(_mod, 'MCTSContext')
            parse_bounds_spec = getattr(_mod, 'parse_bounds_spec')
            apply_mcts_param_updates = getattr(_mod, 'apply_mcts_param_updates')
    safe_bounds = parse_bounds_spec(getattr(args,'ml_safe_bounds',''))
    if str(getattr(args,'ml_scheduler','none')) != 'none':
        if str(getattr(args,'ml_scheduler')) == 'heuristic':
            ml_sched = HeuristicScheduler(strategy=str(getattr(args,'ml_strategy','absolute')), allowed=ml_allowed, safe_bounds=safe_bounds, log=bool(getattr(args,'ml_log', False)))
        elif str(getattr(args,'ml_scheduler')) == 'nn':
            ml_sched = NNScheduler(model_path=str(getattr(args,'ml_path','') or ''), strategy=str(getattr(args,'ml_strategy','absolute')), allowed=ml_allowed, safe_bounds=safe_bounds, log=bool(getattr(args,'ml_log', False)))

    # AlphaZero-lite ÊâãÂä®Ê≥®ÂÖ•ÈÄâÈ°πÂ∑≤ÁßªÈô§ÔºåÈªòËÆ§‰∫§Áî± ML Â±Ç/Áã¨Âç†Ê®°ÂºèÁÆ°ÁêÜ

    # Âú®Á∫øÁ≠ñÁï•ÂÖàÈ™åËÆ≠ÁªÉ‰∏éÈááÊ†∑Èí©Â≠êÂ∑≤ÁßªÈô§ÔºåÈÅøÂÖçÈ¢ùÂ§ñ‰æùËµñ

    # Â§çÊ†∏/Èó®Êéß‰∏é TestVerify Â∑≤ÁßªÈô§ÔºåÁªü‰∏Ä‰ª•ËÆ≠ÁªÉÈõÜ best ‰∏∫ÂáÜ

    print(f"[Perf] short={args.short_duration}s full={args.full_duration}s short_frac={args.short_frac:.2f} log_skip={args.log_skip} in_memory={args.in_memory_log}")
    print(f"[MCTS] Config: max_depth={args.mcts_max_depth} rollout_depth={args.rollout_depth} complexity_penalty={args.complexity_penalty} pw_alpha={args.pw_alpha} pw_c={args.pw_c}")
    if pool_holder['enabled']:
        print(f"[Parallel] Using persistent Pool: workers={pool_holder['worker_n']} | traj_batch_size={traj_batch_size}/{T}")
    else:
        print(f"[Parallel] Single-process eval | traj_batch_size={traj_batch_size}/{T}")
    total = args.iters
    report_interval = max(1, total // args.report) if args.report>0 else max(1,total//20)
    t0 = time.time()
    last_improve_time = t0
    last_best_reward_seen = -1e18
    ckpt_last_saved_train = -1e18
    print(f"[INFO] Multi-trajectory search: {traj_names} | disturbances={bool(disturbances)} aggregate={args.aggregate}")
    print(f"[INFO] For long run (e.g., 30000 iters) consider: lower --duration or subset trajectories per iter for speed.")

    use_tqdm = False
    if args.tqdm:
        try:
            from tqdm import trange  # type: ignore
            tqdm_range = trange(1, total+1, desc='MCTS', ncols=100, leave=True)  # type: ignore
            iter_range = tqdm_range  # type: ignore
            use_tqdm = True
        except Exception:
            print('[WARN] Êú™ËÉΩÂØºÂÖ• tqdmÔºåÊîπÁî®ÊôÆÈÄöÊó•Âøó„ÄÇ')
            iter_range = range(1, total+1)
    else:
        iter_range = range(1, total+1)

    import contextlib, io
    suppress = args.quiet_eval
    last_verified_hash = None
    log_best_train_hash = None
    log_best_train_score = float('nan')
    saved_penalty = {
        'overlap': float(getattr(args, 'overlap_penalty', 0.0) or 0.0),
        'conflict': float(getattr(args, 'conflict_penalty', 0.0) or 0.0),
        'complexity': float(base_complexity),
    }
    no_improve_count = 0
    unfreeze_until_iter = 0
    burst_since_size = 0
    last_prog_size_seen = 0
    burst_until_iter = 0
    burst_saved: Dict[str, Any] = {
        'min_rules_guard_effective': None,
        'add_rule_bias_base': None,
        'prefer_more_rules_tie_delta': None,
    }

    # ÂàùÂßãÈ™åËØÅ„ÄÅÂÖ®Â±ÄÊúÄ‰ºòÊ≥®ÂÖ•‰∏éÊµãËØïÈõÜÈ™åËØÅÂ∑≤ÁßªÈô§

    # ML Ë∞ÉÂ∫¶ÁöÑÊîπËøõË∑üË∏™ÂàùÂÄºÔºàÈÅøÂÖçÊú™ÁªëÂÆöÔºâ
    last_best_reward_for_ml = -1e18
    last_improve_time_for_ml = t0
    last_improve_iter_for_ml = 0

    # È¢ÑËÆæÂæ™ÁéØÂ§ñÂèòÈáèÔºåÈÅøÂÖçÂú®ÊûÅÁ´ØÊÉÖÂÜµ‰∏ãÊú™ÁªëÂÆöÔºàÂ¶Ç 0 Ê¨°Ëø≠‰ª£Ôºâ
    it = 0
    best_prog = None
    best_reward_now = float('nan')
    prog_size = 0

    for it in iter_range:
        if getattr(args, 'banner_every', 0) and args.banner_every > 0 and ((it-1) % args.banner_every == 0):
            banner = f"{'-'*18}Á¨¨{it}/{total}Ê¨°Ëø≠‰ª£{'-'*12}"
            if use_tqdm:
                try:
                    from tqdm import tqdm  # type: ignore
                    tqdm.write(banner)
                except Exception:
                    print(banner)
            else:
                print(banner)
        if suppress:
            buf = io.StringIO()
            with contextlib.redirect_stdout(buf):
                agent.search(iterations=1, total_target=total)
        else:
            agent.search(iterations=1, total_target=total)

        if use_tqdm:
            if it % report_interval == 0 or it == 1:
                elapsed = time.time() - t0
                speed = it/elapsed if elapsed>0 else 0.0
                remaining = total - it
                eta = remaining/speed if speed>0 else float('inf')
                _, bs = agent.get_best_program()
                try:
                    tqdm_range.set_postfix(best=f"{bs:.4f}", it_s=f"{speed:.2f}", ETA=f"{eta/60:.1f}m")  # type: ignore
                except Exception:
                    pass
        else:
            if it % report_interval == 0 or it == 1:
                elapsed = time.time() - t0
                speed = it/elapsed if elapsed>0 else 0.0
                remaining = total - it
                eta = remaining/speed if speed>0 else float('inf')
                _, bs = agent.get_best_program()
                print(f"[MCTS] {it}/{total} best={bs:.4f} speed={speed:.2f} it/s ETA={eta:.1f}s")

        # ML scheduler hook (post-search for this iter)
        try:
            # Respect explicit 0: don't coerce 0 to default via `or`.
            _warm_raw = getattr(args, 'ml_warmup_iters', 10)
            try:
                warmup_iters = int(_warm_raw)
            except Exception:
                warmup_iters = 10
            _intv_raw = getattr(args, 'ml_interval', 5)
            try:
                interval = int(_intv_raw)
            except Exception:
                interval = 5
            interval = max(1, interval)
            if ml_sched is not None and it >= warmup_iters:
                if (it % interval) == 0:
                    # Build context
                    try:
                        best_prog, best_val = agent.get_best_program()
                    except Exception:
                        best_prog = None
                        best_val = float('nan')
                    # derive improvement signals
                    delta = 0.0
                    if isinstance(best_val, (int, float)) and best_val > last_best_reward_for_ml + 1e-12:
                        delta = float(best_val - last_best_reward_for_ml)
                        last_best_reward_for_ml = float(best_val)
                        last_improve_time_for_ml = time.time()
                        last_improve_iter_for_ml = int(it)
                    sec_since = time.time() - (last_improve_time_for_ml if 'last_improve_time_for_ml' in locals() else t0)
                    it_since = it - (last_improve_iter_for_ml if 'last_improve_iter_for_ml' in locals() else 0)
                    rule_count = len(best_prog) if best_prog else 0
                    ctx_obj = MCTSContext(
                        iter_idx=int(it),
                        total_target=int(total),
                        progress=float(it/max(1,total)),
                        best_reward=float(best_val if isinstance(best_val,(int,float)) else float('nan')),
                        best_reward_delta=float(delta),
                        seconds_since_improve=float(sec_since if isinstance(sec_since,(int,float)) else -1.0),
                        iters_since_improve=int(it_since),
                        rule_count=int(rule_count),
                        epsilon=float(getattr(agent,'epsilon',0.0) or 0.0),
                        stagnation_window=int(getattr(agent,'_stagnation_window',0) or 0)
                    )
                    updates = ml_sched.step(ctx_obj)
                    # Optionally dump feature->target pair for NN training
                    dump_path = str(getattr(args, 'ml_dump_csv', '') or '').strip()
                    try:
                        if dump_path:
                            import csv
                            import os
                            # Import shared KEY_ORDER for stable column names (package + script modes)
                            try:
                                from .ml_param_scheduler import KEY_ORDER  # type: ignore
                            except Exception:
                                try:
                                    from ml_param_scheduler import KEY_ORDER  # type: ignore
                                except Exception:
                                    try:
                                        import importlib as _il_k
                                        KEY_ORDER = getattr(_il_k.import_module(f"{_PKG_NAME}.ml_param_scheduler"), 'KEY_ORDER')  # type: ignore[name-defined]
                                    except Exception:
                                        # Last-resort fallback to a local constant to avoid breaking CSV dump
                                        KEY_ORDER = [
                                            'pw_alpha','pw_c','_puct_c','_edit_prior_c','_dirichlet_eps','_full_action_prob',
                                            '_prefer_more_rules_tie_delta','_prefer_fewer_rules_tie_delta','_add_rule_bias_base',
                                            '_value_mix_lambda','_epsilon_max'
                                        ]
                            # Keep features aligned with NNScheduler input (7 dims)
                            cols_feat = [
                                'progress','best_reward','best_reward_delta',
                                'seconds_since_improve','iters_since_improve','rule_count','epsilon'
                            ]
                            header = cols_feat + KEY_ORDER
                            os.makedirs(os.path.dirname(dump_path) or '.', exist_ok=True)
                            new_file = not os.path.exists(dump_path)
                            row_feat = [
                                float(ctx_obj.progress),
                                float(ctx_obj.best_reward), float(ctx_obj.best_reward_delta),
                                float(ctx_obj.seconds_since_improve), int(ctx_obj.iters_since_improve),
                                int(ctx_obj.rule_count), float(ctx_obj.epsilon)
                            ]
                            # targets: fall back to current agent values if scheduler returned empty or partial
                            target_vals = []
                            for k in KEY_ORDER:
                                if updates and (k in updates):
                                    target_vals.append(float(updates[k]))
                                else:
                                    try:
                                        # Fallback to agent's current value, with a safe default if missing
                                        target_vals.append(float(getattr(agent, k, float('nan'))))
                                    except Exception:
                                        target_vals.append(float('nan'))
                            row = row_feat + target_vals
                            with open(dump_path, 'a', newline='', encoding='utf-8') as fcsv:
                                w = csv.writer(fcsv)
                                if new_file:
                                    w.writerow(header)
                                w.writerow(row)
                                try:
                                    fcsv.flush()
                                    import os as _osfs
                                    try:
                                        _osfs.fsync(fcsv.fileno())
                                    except Exception:
                                        pass
                                except Exception:
                                    pass
                            try:
                                if it % interval == 0:
                                    print(f"[ML-Sched] Dumped CSV row -> {dump_path} @ iter {it}")
                            except Exception:
                                pass
                    except Exception as _dump_e:
                        try:
                            _dp = dump_path if 'dump_path' in locals() and dump_path else '(unset)'
                            print(f"[ML-Sched][WARN] Ê†∑Êú¨ÂØºÂá∫Â§±Ë¥•Ôºàiter={it} path={_dp}Ôºâ: {_dump_e}")
                        except Exception:
                            pass
                    if updates:
                        apply_mcts_param_updates(agent, updates,
                                                 strategy=str(getattr(args,'ml_strategy','absolute')),
                                                 bounds=safe_bounds,
                                                 int_keys={'_add_rule_bias_base'},
                                                 log=bool(getattr(args,'ml_log', False)))
        except Exception as _ml_e:
            if it == 1:
                print(f"[ML-Sched][WARN] Ë∞ÉÂ∫¶Âô®ÂºÇÂ∏∏ÔºàÂ∑≤ÂøΩÁï•ÔºâÔºö{_ml_e}")

        # ËΩªÈáèÂÄôË°•‰ºòÂåñÂ∑≤ÁßªÈô§ÔºåÈÅøÂÖçÂàÜÊîØËÜ®ËÉÄ

        try:
            cur_prog, cur_best = agent.get_best_program()
            if cur_best is None:
                cur_best = -1e18
            if cur_best > last_best_reward_seen + 1e-9:
                last_best_reward_seen = cur_best
                last_improve_time = time.time()
            else:
                stg_sec = int(getattr(args, 'stagnation_seconds', 0))
                eps_target = float(getattr(args, 'epsilon_rebound_target', 0.0) or 0.0)
                if stg_sec > 0 and eps_target > 0.0:
                    if (time.time() - last_improve_time) >= stg_sec:
                        setattr(agent, '_epsilon_rebound', max(float(getattr(agent, '_epsilon_rebound', 0.0)), eps_target))
                        until_iter = int(getattr(agent, 'total_iterations_done', it)) + (int(getattr(args, 'time_rebound_iters', 0)) or int(getattr(args, 'rebound_iters', 80)))
                        setattr(agent, '_rebound_until_iter', until_iter)
                        msg = f"[Rebound-Time] no-improve >= {stg_sec}s -> epsilon>= {eps_target:.2f} for {until_iter - int(getattr(agent, 'total_iterations_done', it))} iters"
                        if use_tqdm:
                            try:
                                from tqdm import tqdm  # type: ignore
                                tqdm.write(msg)
                            except Exception:
                                print(msg)
                        else:
                            print(msg)
                        last_improve_time = time.time()
        except Exception as _stg_e:
            if it == 1:
                print(f"[Stagnation][WARN] Êó∂Èó¥ÂÅúÊªûÊ£ÄÊµãÂ§±Ë¥•: {_stg_e}")

        # mini-full Êé¢ÈíàËØÑ‰º∞Â∑≤ÁßªÈô§

        # È™åËØÅ‰∏éËá™Âä®Ëß£ÂÜªÈÄªËæëÂ∑≤ÁßªÈô§

        # Âú®Á∫øÁ≠ñÁï•ÂÖàÈ™åÊõ¥Êñ∞Â∑≤ÁßªÈô§

        # CMA-ES ÂèÇÊï∞ÂæÆË∞ÉÈÄªËæëÔºàMCTSË¥üË¥£ÁªìÊûÑÔºåCMAË¥üË¥£ÂèÇÊï∞Ôºâ
        if args.cma_refine_every > 0 and it % args.cma_refine_every == 0:
            try:
                cur_prog_cma, cur_best_cma = agent.get_best_program()
                if cur_prog_cma and len(cur_prog_cma) > 0:
                    msg_start = f"[CMA-Refine] @ iter {it}: ÂºÄÂßãÂæÆË∞ÉÂΩìÂâçÊúÄ‰ºòÁ®ãÂ∫èÔºà{len(cur_prog_cma)}ËßÑÂàôÔºâÁöÑPIDÂèÇÊï∞..."
                    if use_tqdm:
                        try:
                            from tqdm import tqdm
                            tqdm.write(msg_start)
                        except Exception:
                            print(msg_start)
                    else:
                        print(msg_start)
                    
                    # ÊèêÂèñÂΩìÂâçÊâÄÊúâËßÑÂàôÁöÑPIDÂèÇÊï∞
                    rule_params = []  # [(rule_idx, P, I, D), ...]
                    for rule_idx, rule in enumerate(cur_prog_cma):
                        kp = ki = kd = None
                        try:
                            for act in rule.get('action', []):
                                if isinstance(act, BinaryOpNode) and act.op == 'set' and isinstance(act.left, TerminalNode) and isinstance(act.right, TerminalNode):
                                    if act.left.value == 'P': kp = float(act.right.value)
                                    elif act.left.value == 'I': ki = float(act.right.value)
                                    elif act.left.value == 'D': kd = float(act.right.value)
                        except Exception:
                            pass
                        if all(v is not None for v in (kp, ki, kd)):
                            rule_params.append((rule_idx, kp, ki, kd))
                    
                    if rule_params:
                        # ÂØºÂÖ•CMA-ES
                        try:
                            import cma  # type: ignore
                        except ImportError:
                            print("[CMA-Refine][WARN] Êú™ÂÆâË£ÖcmaÂ∫ìÔºåË∑≥ËøáCMAÂæÆË∞É")
                            rule_params = []
                        
                        if rule_params:
                            # ÂáÜÂ§á‰ºòÂåñÁõÆÊ†áÂáΩÊï∞
                            def cma_objective(flat_params):
                                # flat_params: [P1, I1, D1, P2, I2, D2, ...]
                                # ÈáçÂª∫Á®ãÂ∫è
                                prog_copy = [dict(r) for r in cur_prog_cma]
                                param_idx = 0
                                for rule_idx, _, _, _ in rule_params:
                                    new_p = flat_params[param_idx]
                                    new_i = flat_params[param_idx + 1]
                                    new_d = flat_params[param_idx + 2]
                                    param_idx += 3
                                    
                                    # Êõ¥Êñ∞action‰∏≠ÁöÑPID
                                    new_actions = []
                                    for act in prog_copy[rule_idx].get('action', []):
                                        if isinstance(act, BinaryOpNode) and act.op == 'set' and isinstance(act.left, TerminalNode):
                                            if act.left.value == 'P':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('P'), TerminalNode(float(new_p))))
                                            elif act.left.value == 'I':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('I'), TerminalNode(float(new_i))))
                                            elif act.left.value == 'D':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('D'), TerminalNode(float(new_d))))
                                            else:
                                                new_actions.append(act)
                                        else:
                                            new_actions.append(act)
                                    prog_copy[rule_idx]['action'] = new_actions
                                
                                # ËØÑ‰º∞Ôºà‰ΩøÁî®Áü≠Êó∂Èïø‰ª•Âä†ÈÄüÔºâ
                                try:
                                    score = float(short_eval_all(prog_copy))
                                    return -score  # CMAÊúÄÂ∞èÂåñÔºåÊâÄ‰ª•ÂèñË¥ü
                                except Exception:
                                    return 1e18
                            
                            # ÊûÑÂª∫ÂàùÂßãÂèÇÊï∞ÂêëÈáè
                            x0 = []
                            for _, kp, ki, kd in rule_params:
                                x0.extend([kp, ki, kd])
                            
                            # ËøêË°åCMA-ES
                            sigma0 = args.cma_sigma
                            opts = {
                                'popsize': args.cma_popsize,
                                'maxiter': args.cma_maxiter,
                                'verb_disp': 0,
                                'verb_log': 0,
                                'verbose': -9,
                            }
                            
                            try:
                                es = cma.CMAEvolutionStrategy(x0, sigma0, opts)
                                cma_iter = 0
                                while not es.stop() and cma_iter < args.cma_maxiter:
                                    solutions = es.ask()
                                    fitness = [cma_objective(sol) for sol in solutions]
                                    es.tell(solutions, fitness)
                                    cma_iter += 1
                                
                                # Ëé∑ÂèñÊúÄ‰ºòËß£
                                best_sol = es.result.xbest
                                best_fit = -es.result.fbest  # ËΩ¨ÂõûÊ≠£ÂàÜÊï∞
                                
                                # Êõ¥Êñ∞Á®ãÂ∫è
                                param_idx = 0
                                for rule_idx, _, _, _ in rule_params:
                                    new_p = best_sol[param_idx]
                                    new_i = best_sol[param_idx + 1]
                                    new_d = best_sol[param_idx + 2]
                                    param_idx += 3
                                    
                                    # Êõ¥Êñ∞ÂΩìÂâçÁ®ãÂ∫èÁöÑaction
                                    new_actions = []
                                    for act in cur_prog_cma[rule_idx].get('action', []):
                                        if isinstance(act, BinaryOpNode) and act.op == 'set' and isinstance(act.left, TerminalNode):
                                            if act.left.value == 'P':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('P'), TerminalNode(round(float(new_p), 4))))
                                            elif act.left.value == 'I':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('I'), TerminalNode(round(float(new_i), 4))))
                                            elif act.left.value == 'D':
                                                new_actions.append(BinaryOpNode('set', TerminalNode('D'), TerminalNode(round(float(new_d), 4))))
                                            else:
                                                new_actions.append(act)
                                        else:
                                            new_actions.append(act)
                                    cur_prog_cma[rule_idx]['action'] = new_actions
                                
                                # È™åËØÅÊîπËøõÂπ∂Ê≥®ÂÖ•ÂõûMCTS
                                refined_score = float(short_eval_all(cur_prog_cma))
                                if refined_score > cur_best_cma + 1e-6:
                                    # Ê≥®ÂÖ•ÂõûMCTSÁöÑbest
                                    try:
                                        agent.best_program = cur_prog_cma
                                        agent.best_value = refined_score
                                        msg_success = f"[CMA-Refine] ÂÆåÊàê! ÂæóÂàÜÊèêÂçá: {cur_best_cma:.4f} -> {refined_score:.4f} (+{refined_score - cur_best_cma:.4f})"
                                    except Exception:
                                        msg_success = f"[CMA-Refine] ÂÆåÊàê! ÂæóÂàÜ: {refined_score:.4f} (Êó†Ê≥ïÊ≥®ÂÖ•MCTSÔºå‰ªÖËÆ∞ÂΩï)"
                                else:
                                    msg_success = f"[CMA-Refine] ÂÆåÊàê! ÂæóÂàÜÊú™ÊèêÂçá: {cur_best_cma:.4f} -> {refined_score:.4f}"
                                
                                if use_tqdm:
                                    try:
                                        from tqdm import tqdm
                                        tqdm.write(msg_success)
                                    except Exception:
                                        print(msg_success)
                                else:
                                    print(msg_success)
                            
                            except Exception as _cma_run_e:
                                print(f"[CMA-Refine][WARN] CMAËøêË°åÂ§±Ë¥•: {_cma_run_e}")
            except Exception as _cma_e:
                if it == args.cma_refine_every:
                    print(f"[CMA-Refine][WARN] CMAÂæÆË∞ÉÂ§±Ë¥•: {_cma_e}")
        
        # ÊµãËØïÈõÜÂÆûÊó∂È™åËØÅÈÄªËæë
        if args.test_verify_every > 0 and it % args.test_verify_every == 0:
            try:
                cur_prog_test, cur_best_test = agent.get_best_program()
                if cur_prog_test:
                    # ÂáÜÂ§áÊµãËØïÈõÜËΩ®Ëøπ
                    test_traj_names = []
                    if args.test_traj_preset:
                        preset = args.test_traj_preset
                        if preset == 'test_challenge':
                            test_traj_names = ['zigzag3d','lemniscate3d','random_waypoints','spiral_in_out','stairs','coupled_surface']
                    
                    if test_traj_names:
                        # ÊûÑÂª∫ÊµãËØïÈõÜÁöÑÂπ≤Êâ∞ÂíåÂ•ñÂä±ÈÖçÁΩÆ
                        test_disturbances = build_disturbances(args.test_disturbance)
                        test_reward_weights, test_reward_ks = get_reward_profile(args.reward_profile)
                        
                        # Âú®ÊµãËØïÈõÜ‰∏äËØÑ‰º∞
                        test_rewards = []
                        for tj_name in test_traj_names:
                            _tj_actual = tj_name if tj_name != 'random_wp' else 'random_waypoints'
                            _traj = build_trajectory(_tj_actual)
                            
                            # ÂàõÂª∫ÊéßÂà∂Âô®
                            _ctrl_test = MathProgramController(program=cur_prog_test, suppress_init_print=True)
                            
                            # ÂàõÂª∫ SimulationTester
                            _tst = SimulationTester(
                                controller=_ctrl_test,
                                test_scenarios=test_disturbances,
                                weights=test_reward_weights,
                                trajectory=_traj,
                                duration_sec=args.test_duration,
                                log_skip=args.log_skip,
                                output_folder='01_soar/results/test_verify',
                                gui=False,
                                in_memory=True,
                                quiet=True
                            )
                            
                            try:
                                _rwd_test = _tst.run()
                                test_rewards.append(float(_rwd_test))
                            except Exception:
                                test_rewards.append(-1e18)
                        
                        # ËÅöÂêàÊµãËØïÈõÜÂæóÂàÜ
                        if test_rewards:
                            if args.test_aggregate == 'harmonic':
                                valid_test_rws = [r for r in test_rewards if r > 0]
                                if valid_test_rws:
                                    test_score = len(valid_test_rws) / sum(1.0/r for r in valid_test_rws)
                                else:
                                    test_score = -1e18
                            elif args.test_aggregate == 'min':
                                test_score = min(test_rewards)
                            else:
                                test_score = sum(test_rewards) / len(test_rewards)
                            
                            # Ê£ÄÊü•ÊòØÂê¶Ë∂ÖËøáÂéÜÂè≤ÊúÄ‰Ω≥
                            if not hasattr(agent, '_best_test_score'):
                                agent._best_test_score = -1e18  # type: ignore
                                agent._best_test_program = None  # type: ignore
                            
                            if test_score > agent._best_test_score + 1e-9:  # type: ignore
                                agent._best_test_score = test_score  # type: ignore
                                agent._best_test_program = cur_prog_test  # type: ignore
                                
                                # Á´ãÂç≥‰øùÂ≠òÂà∞‰∏ªÊñá‰ª∂
                                test_meta = {
                                    'best_score': float(cur_best_test),
                                    'test_score': float(test_score),
                                    'test_verified_at_iter': int(it),
                                    'iters': int(it),
                                    'trajectories': traj_names,
                                    'test_trajectories': test_traj_names,
                                    'aggregate': args.aggregate,
                                    'test_aggregate': args.test_aggregate,
                                    'disturbance': args.disturbance,
                                    'test_disturbance': args.test_disturbance,
                                }
                                save_program_json(cur_prog_test, args.save_program, meta=test_meta)
                                
                                msg = f"[TestVerify] @ iter {it}: ÊµãËØïÈõÜÂæóÂàÜ {test_score:.4f} > ÂéÜÂè≤ÊúÄ‰Ω≥ {agent._best_test_score - test_score + test_score:.4f}ÔºåÂ∑≤‰øùÂ≠ò"  # type: ignore
                                if use_tqdm:
                                    try:
                                        from tqdm import tqdm
                                        tqdm.write(msg)
                                    except Exception:
                                        print(msg)
                                else:
                                    print(msg)
                            else:
                                msg = f"[TestVerify] @ iter {it}: ÊµãËØïÈõÜÂæóÂàÜ {test_score:.4f} <= ÂéÜÂè≤ÊúÄ‰Ω≥ {agent._best_test_score:.4f}ÔºåË∑≥Ëøá‰øùÂ≠ò"  # type: ignore
                                if use_tqdm:
                                    try:
                                        from tqdm import tqdm
                                        tqdm.write(msg)
                                    except Exception:
                                        print(msg)
                                else:
                                    print(msg)
            except Exception as _test_e:
                if it == args.test_verify_every:
                    print(f"[TestVerify][WARN] ÊµãËØïÈõÜÈ™åËØÅÂ§±Ë¥•: {_test_e}")

        if args.save_every and it % args.save_every == 0:
            best_prog_mid, best_score_mid = agent.get_best_program()
            try:
                cur_train_for_ckpt = float(short_eval_all(best_prog_mid)) if best_prog_mid else float('nan')
            except Exception:
                cur_train_for_ckpt = float('nan')
            if not isinstance(cur_train_for_ckpt, float) or (cur_train_for_ckpt != cur_train_for_ckpt):
                cur_train_for_ckpt = float(best_score_mid)
            if cur_train_for_ckpt <= ckpt_last_saved_train + 1e-9:
                skip_msg = f"[‰øùÂ≠ò] Ë∑≥Ëøá checkpoint @ {it}ÔºàËÆ≠ÁªÉÂàÜÊú™ÊèêÂçáÔºö{cur_train_for_ckpt:.6f} <= {ckpt_last_saved_train:.6f}Ôºâ"
                if use_tqdm:
                    try:
                        from tqdm import tqdm
                        tqdm.write(skip_msg)
                    except Exception:
                        print(skip_msg)
                else:
                    print(skip_msg)
            else:
                ckpt_last_saved_train = cur_train_for_ckpt
            meta_mid = {
                'best_score': float(best_score_mid),
                'iters': it,
                'trajectories': traj_names,
                'aggregate': args.aggregate,
                'disturbance': args.disturbance,
                'partial': True,
                'best_score_short': float(cur_train_for_ckpt)
            }
            try:
                import os as _os
                _ck_dir = _os.path.join(_os.path.dirname(args.save_program) or '.', 'checkpoints')
                _os.makedirs(_ck_dir, exist_ok=True)
                _ck_path = _os.path.join(_ck_dir, f"best_program_iter_{it:06d}.json")
            except Exception:
                _ck_path = args.save_program + f".iter_{it:06d}.json"
            save_program_json(best_prog_mid, _ck_path, meta=meta_mid)
            save_search_history(agent.best_history, args.save_history)
            msg = f"[‰øùÂ≠ò] ‰∏≠Èó¥Ê£ÄÊü•ÁÇπ @ {it} -> {_ck_path} (short={cur_train_for_ckpt:.6f})"
            if use_tqdm:
                try:
                    from tqdm import tqdm
                    tqdm.write(msg)
                except Exception:
                    print(msg)
            else:
                print(msg)

        best_prog, best_reward_now = agent.get_best_program()
        prog_size = len(best_prog) if best_prog else 0
    try:
        cur_hash_for_log = hash_program(best_prog) if best_prog else None
        if best_prog and cur_hash_for_log != log_best_train_hash:
            log_best_train_score = float(short_eval_all(best_prog))
            log_best_train_hash = cur_hash_for_log
    except Exception:
        log_best_train_score = float(best_reward_now)

    # ÁªìÊûÑÁàÜÂèëÈÄªËæëÂ∑≤ÁßªÈô§Ôºå‰øùÊåÅ‰∏ªÊµÅÁ®ãÁÆÄÊ¥Å

    elapsed_total = time.time() - t0
    it_per_sec = it/elapsed_total if elapsed_total > 0 else 0.0
    try:
        with open(iter_log_path, 'a', encoding='utf-8') as f:
            try:
                rdec = int(getattr(args, 'rebound_decay_iters', 0) or 0)
                rtarget = float(getattr(args, 'rebound_target_eps', 0.12) or 0.12)
                if rdec > 0 and getattr(agent, '_rebound_until_iter', 0) and getattr(agent, 'total_iterations_done', 0) < getattr(agent, '_rebound_until_iter', 0):
                    remaining = int(getattr(agent, '_rebound_until_iter', 0)) - int(getattr(agent, 'total_iterations_done', 0))
                    ratio = min(1.0, max(0.0, 1.0 - (remaining / max(1, rdec))))
                    cur_min = float(getattr(agent, '_epsilon_rebound', 0.0))
                    decayed = max(rtarget, cur_min * (1.0 - ratio) + rtarget * ratio)
                    if hasattr(agent, 'epsilon'):
                        agent.epsilon = max(rtarget, min(1.0, float(getattr(agent, 'epsilon', 0.0))))
                    setattr(agent, '_epsilon_rebound', decayed)
            except Exception:
                pass
            eps = getattr(agent, 'epsilon', 0.0)
            rebound_active = 1 if getattr(agent, 'total_iterations_done', 0) < getattr(agent, '_rebound_until_iter', 0) else 0
            bcur_logged = float(log_best_train_score) if best_prog else float('nan')
            f.write(f"{it},{bcur_logged:.6f},{prog_size},{elapsed_total:.2f},{it_per_sec:.3f},{eps:.4f},{rebound_active}\n")
    except Exception as _log_e:
        if it == 1:
            print(f"[IterLog][WARN] ÂÜôÂÖ•Â§±Ë¥•: {_log_e}")

    try:
        if pool_holder.get('pool') is not None:
            pool_holder['pool'].close()  # type: ignore
            pool_holder['pool'].join()   # type: ignore
    except Exception as _pc:
        print(f"[Parallel][WARN] ËøõÁ®ãÊ±†ÂÖ≥Èó≠Â§±Ë¥•: {_pc}")

    # ÁªìÊùüÔºö‰øùÂ≠òÊêúÁ¥¢ÂéÜÂè≤‰∏éÂΩìÂâçËÆ≠ÁªÉÈõÜÊúÄ‰ºòÁ®ãÂ∫è
    save_search_history(agent.best_history, args.save_history)
    try:
        final_prog, final_score = agent.get_best_program()
        
        # È™åËØÅÊúÄÁªàÁ®ãÂ∫èÊÄßËÉΩÔºåÂ¶ÇÊûúÊØî warm_start ÂéüÁ®ãÂ∫èÂ∑ÆÔºå‰øùÁïôÂéüÁ®ãÂ∫è
        warm_start_prog = None
        warm_start_score = None
        if args.warm_start_cmaes and args.warm_start_path:
            try:
                with open(args.warm_start_path, 'r', encoding='utf-8') as _wsf:
                    ws_data = json.load(_wsf)
                    if 'rules' in ws_data and ws_data['rules']:
                        warm_start_prog = deserialize_program(ws_data)
                        # Ëé∑ÂèñÂéüÈ™åËØÅÂàÜÊï∞
                        warm_start_score = ws_data.get('meta', {}).get('verified_score', None)
                        if warm_start_score is None:
                            # Â¶ÇÊûúÊ≤°ÊúâÈ™åËØÅÂàÜÊï∞ÔºåÂø´ÈÄüËØÑ‰º∞‰∏ÄÊ¨°
                            print("[PostValidate] Warm-start Á®ãÂ∫èÊó† verified_scoreÔºåÂø´ÈÄüËØÑ‰º∞...")
                            warm_start_score = float(short_eval_all(warm_start_prog))
            except Exception as _ws_load_e:
                print(f"[PostValidate][WARN] Êó†Ê≥ïËΩΩÂÖ• warm_start Á®ãÂ∫è: {_ws_load_e}")
        
        # Âø´ÈÄüËØÑ‰º∞ÊúÄÁªàÁ®ãÂ∫è
        final_validated_score = float(short_eval_all(final_prog))
        print(f"[PostValidate] ËÆ≠ÁªÉÂêéÁ®ãÂ∫èÂæóÂàÜ: {final_validated_score:.6f}")
        
        prog_to_save = final_prog
        score_to_save = final_validated_score
        
        if warm_start_prog is not None and warm_start_score is not None:
            print(f"[PostValidate] Warm-start ÂéüÁ®ãÂ∫èÂæóÂàÜ: {warm_start_score:.6f}")
            if final_validated_score < warm_start_score * 0.98:  # Â¶ÇÊûú‰∏ãÈôçË∂ÖËøá2%
                print(f"[PostValidate][WARN] ËÆ≠ÁªÉÂêéÊÄßËÉΩ‰∏ãÈôç {((warm_start_score - final_validated_score)/warm_start_score*100):.1f}%Ôºå‰øùÁïôÂéüÁ®ãÂ∫è")
                prog_to_save = warm_start_prog
                score_to_save = warm_start_score
            else:
                print(f"[PostValidate][OK] ËÆ≠ÁªÉÂêéÊÄßËÉΩÊèêÂçáÊàñÊåÅÂπ≥Ôºå‰øùÂ≠òÊñ∞Á®ãÂ∫è")
        
        meta = {
            'best_score': float(final_score if isinstance(final_score,(int,float)) else float('nan')),
            'validated_score': float(score_to_save),
            'iters': int(getattr(agent, 'total_iterations_done', 0)),
            'trajectories': traj_names,
            'aggregate': args.aggregate,
            'disturbance': args.disturbance,
        }
        save_program_json(prog_to_save, args.save_program, meta=meta)
        print(f"[Summary] ËÆ≠ÁªÉÂÆåÊàêÔºåbest_train={meta['best_score']:.6f}, validated={meta['validated_score']:.6f}\nSaved program => {args.save_program}\nSaved history => {args.save_history}")
    except Exception as _sum_e:
        print(f"[Summary][WARN] ‰øùÂ≠òÊúÄÁªàÁ®ãÂ∫èÂ§±Ë¥•ÔºàÂ∑≤‰øùÂ≠òÂéÜÂè≤Ôºâ: {_sum_e}")

if __name__ == '__main__':
    main()
