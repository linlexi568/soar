"""gnn_policy_nn_v2.py

åˆ†å±‚åŒç½‘ç»œ (Hierarchical Dual Network) ç‰ˆæœ¬:
- ç»“æ„ç¼–ç å™¨ (StructureEncoder): æ·±å±‚ GATv2Conv å¤„ç†ç¨‹åºå›¾çš„ç»“æ„ä¸è¯­ä¹‰å…³ç³»ã€‚
- ç‰¹å¾ç¼–ç å™¨ (FeatureEncoder): å°†åŸå§‹èŠ‚ç‚¹ç‰¹å¾åºåˆ—åŒ–åç» TransformerEncoder æ•è·è·¨èŠ‚ç‚¹çš„é¡ºåº/ç»„åˆæ¨¡å¼ã€‚
- èåˆå±‚ (CrossFusion): ä½¿ç”¨å¤šå¤´äº¤å‰æ³¨æ„åŠ› (structure embedding ä½œä¸º query, feature tokens ä½œä¸º key/value) è·å¾—èåˆè¡¨ç¤ºã€‚
- Policy / Value Heads: æ›´æ·±çš„ä¸¤å±‚ MLP è¾“å‡ºç­–ç•¥ logits ä¸ä»·å€¼æ ‡é‡ã€‚

è®¾è®¡ç›®æ ‡:
1. å‚æ•°é‡ ~2.5M (åœ¨ 8GB GPU å¯æ¥å—èŒƒå›´å†…,ä¸ºåç»­é›¶å…ˆéªŒæ‰©å±•åšå®¹é‡é¢„ç•™)ã€‚
2. ä¸ç°æœ‰ create_gnn_policy_value_net æ¥å£å…¼å®¹,æ–¹ä¾¿ A/B æ›¿æ¢ã€‚
3. æ”¯æŒå¯å˜å¤§å°å›¾ (batch å†…ä¸åŒèŠ‚ç‚¹æ•°) ä»¥åŠ padding maskã€‚
4. å°½é‡é¿å…å¼•å…¥é¢å¤–å¤æ‚ä¾èµ– (ä»…ä½¿ç”¨ PyTorch / PyTorch Geometric)ã€‚

ä½¿ç”¨æ–¹å¼:
from 01_soar.gnn_policy_nn_v2 import create_gnn_policy_value_net_v2
model = create_gnn_policy_value_net_v2(node_feature_dim=24, policy_output_dim=14)
policy_logits, value = model(graph_batch)

æµ‹è¯•:
python 01_soar/gnn_policy_nn_v2.py  # å°†è¿è¡Œè‡ªæ£€ (éšæœºå›¾) å¹¶æ‰“å°å‚æ•°ç»Ÿè®¡ä¸ä¸€æ¬¡å‰å‘+åå‘æ—¶é—´ã€‚
"""
from __future__ import annotations
import math
import time
import threading
from dataclasses import dataclass
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from torch_geometric.nn import GATv2Conv, global_mean_pool
    from torch_geometric.data import Data, Batch
    from torch_geometric.utils import add_self_loops, remove_self_loops, coalesce
except ImportError:
    raise ImportError("éœ€è¦å®‰è£… torch-geometric æ‰èƒ½ä½¿ç”¨è¯¥æ¨¡å—ã€‚")

# ----------------------------- å·¥å…·å‡½æ•° ----------------------------- #

def count_parameters(model: nn.Module) -> int:
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# padding åºåˆ—çš„ç®€æ˜“å‡½æ•°

def pad_sequences(seqs: list[torch.Tensor], pad_value: float = 0.0) -> Tuple[torch.Tensor, torch.Tensor]:
    """å°†é•¿åº¦ä¸ç­‰çš„ [L_i, D] å¼ é‡åˆ—è¡¨ pad æˆ [B, L_max, D] å¹¶è¿”å› mask:[B, L_max] (True=æœ‰æ•ˆ)ã€‚"""
    if not seqs:
        return torch.empty(0), torch.empty(0)
    max_len = max(s.size(0) for s in seqs)
    dim = seqs[0].size(1)
    batch = len(seqs)
    out = seqs[0].new_full((batch, max_len, dim), pad_value)
    mask = torch.zeros(batch, max_len, dtype=torch.bool, device=seqs[0].device)
    for i, s in enumerate(seqs):
        l = s.size(0)
        out[i, :l] = s
        mask[i, :l] = True
    return out, mask

# ----------------------------- ç»“æ„ç¼–ç å™¨ ----------------------------- #

class StructureEncoder(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int = 256, num_layers: int = 5, heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.proj_in = nn.Linear(in_dim, hidden_dim)
        self.layers = nn.ModuleList()
        # ğŸ”§ æ¢å¤ GATv2Convï¼Œä½†æ·»åŠ å¥å£®çš„è¾¹ç´¢å¼•å¤„ç†
        out_per_head = hidden_dim // heads
        for i in range(num_layers):
            conv = GATv2Conv(
                in_channels=hidden_dim,
                out_channels=out_per_head,
                heads=heads,
                dropout=dropout,
                edge_dim=None,
                add_self_loops=False,  # æ‰‹åŠ¨æ·»åŠ è‡ªç¯ä»¥ç¡®ä¿æ­£ç¡®æ€§
                share_weights=False,
            )
            self.layers.append(conv)
        self.norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)
        # GATv2Conv åœ¨ forward å†…éƒ¨é€šè¿‡ self._alpha æš‚å­˜æ³¨æ„åŠ›ï¼Œä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼›
        # å¼‚æ­¥è®­ç»ƒ+MCTS å¹¶å‘è®¿é—®æ—¶éœ€è¦é”ä»¥é¿å… alpha=None æ–­è¨€ã€‚
        self._conv_lock = threading.Lock()

    def forward(self, data: Batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        # è¾“å…¥èŠ‚ç‚¹ç‰¹å¾: x [N_total, in_dim]
        x = self.proj_in(data.x)  # [N, hidden]
        edge_index = data.edge_index
        num_nodes = x.size(0)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰è‡ªç¯
        # GATv2Convè¦æ±‚æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘æœ‰ä¸€æ¡è¾¹ï¼ˆå¦åˆ™alpha=Noneï¼‰
        # ç­–ç•¥ï¼šä½¿ç”¨PyGçš„add_self_loopsï¼Œå®ƒä¼šæ­£ç¡®å¤„ç†æ‰¹æ¬¡ç´¢å¼•
        from torch_geometric.utils import add_self_loops, remove_self_loops, coalesce
        
        # å…ˆç§»é™¤å¯èƒ½çš„è‡ªç¯ï¼Œé¿å…é‡å¤
        edge_index, _ = remove_self_loops(edge_index)
        
        # ä½¿ç”¨PyGçš„add_self_loopsï¼Œå®ƒä¼šè‡ªåŠ¨ä¸º0åˆ°num_nodes-1çš„æ‰€æœ‰èŠ‚ç‚¹æ·»åŠ è‡ªç¯
        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)
        
        # ç¡®ä¿è¾¹ç´¢å¼•å»é‡å¹¶æ’åºï¼ˆcoalesceä¼šåˆå¹¶é‡å¤è¾¹ï¼‰
        edge_index = coalesce(edge_index, num_nodes=num_nodes)
        
        with self._conv_lock:
            for conv, ln in zip(self.layers, self.norms):
                # GATv2Conv ç°åœ¨ä¿è¯æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰è¾¹ï¼Œä¸ä¼šå¯¼è‡´ alpha=None
                h = conv(x, edge_index)
                # æ®‹å·® + å±‚å½’ä¸€åŒ–
                x = ln(x + self.dropout(F.elu(h)))
        # å›¾çº§æ± åŒ–
        graph_emb = global_mean_pool(x, data.batch)  # [B, hidden]
        return graph_emb, x, data.batch

# ----------------------------- ç‰¹å¾ç¼–ç å™¨ ----------------------------- #

class FeatureEncoder(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int = 256, num_layers: int = 3, nhead: int = 8, dropout: float = 0.1, ff_multiplier: int = 4):
        super().__init__()
        self.proj = nn.Linear(in_dim, hidden_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=nhead,
            dim_feedforward=hidden_dim * ff_multiplier,
            dropout=dropout,
            activation="gelu",
            batch_first=True,
            norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.dropout = nn.Dropout(dropout)
        # å¯å­¦ä¹ çš„ CLS token åˆå§‹å‘é‡
        self.cls_token = nn.Parameter(torch.randn(hidden_dim))

    def forward(self, node_features: torch.Tensor, batch_index: torch.Tensor) -> Tuple[torch.Tensor, list[torch.Tensor]]:
        # node_features: [N_total, hidden_dim] (æ¥è‡ªç»“æ„ç¼–ç å™¨çš„èŠ‚ç‚¹åµŒå…¥, æˆ–åŸå§‹ x å†æŠ•å½±)
        # batch_index: [N_total]
        # å°†èŠ‚ç‚¹æŒ‰å›¾åˆ†ç»„
        device = node_features.device
        num_graphs = int(batch_index.max().item()) + 1 if batch_index.numel() > 0 else 0
        sequences = []
        for g in range(num_graphs):
            mask = batch_index == g
            seq = node_features[mask]  # [L_g, hidden]
            # æ’å…¥ CLS token
            if seq.numel() == 0:
                seq = self.cls_token.unsqueeze(0).expand(1, -1)
            else:
                cls = self.cls_token.unsqueeze(0).expand(1, -1)
                seq = torch.cat([cls, seq], dim=0)  # [L_g+1, hidden]
            sequences.append(seq)
        if not sequences:
            return torch.empty(0, device=device), []
        # pad -> [B, L_max, hidden]
        padded, mask = pad_sequences(sequences, pad_value=0.0)
        # æŠ•å½± (è‹¥ä¸Šæ¸¸ä¸æ˜¯ hidden_dim å¯åœ¨æ­¤æŠ•å½±; è¿™é‡Œè¾“å…¥å·²ç»æ˜¯ hidden_dim, ä½†ä¸ºäº†çµæ´»æ€§ä»è°ƒç”¨ self.proj è‹¥å½¢çŠ¶ä¸åŒ)
        if padded.size(-1) != self.proj.out_features:
            padded = self.proj(padded)
        padded = self.dropout(padded)
        # Transformer ç¼–ç  (ä½¿ç”¨ src_key_padding_mask: False=ä¿ç•™, True=å¿½ç•¥)
        key_padding_mask = ~mask  # [B, L_max]
        encoded = self.encoder(padded, src_key_padding_mask=key_padding_mask)  # [B, L_max, hidden]
        # å– CLS ä½ç½®ä½œä¸ºå›¾çº§ç‰¹å¾
        cls_emb = encoded[:, 0]  # [B, hidden]
        return cls_emb, sequences

# ----------------------------- èåˆä¸æœ€ç»ˆç½‘ç»œ ----------------------------- #

class GNNPolicyValueNetV2(nn.Module):
    def __init__(
        self,
        node_feature_dim: int,
        policy_output_dim: int,
        structure_hidden: int = 256,
        structure_layers: int = 5,
        structure_heads: int = 8,
        feature_layers: int = 3,
        feature_heads: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()
        # ç»“æ„ç¼–ç å™¨ + ç‰¹å¾ç¼–ç å™¨
        self.structure_encoder = StructureEncoder(
            in_dim=node_feature_dim,
            hidden_dim=structure_hidden,
            num_layers=structure_layers,
            heads=structure_heads,
            dropout=dropout,
        )
        self.feature_encoder = FeatureEncoder(
            in_dim=structure_hidden,
            hidden_dim=structure_hidden,
            num_layers=feature_layers,
            nhead=feature_heads,
            dropout=dropout,
        )
        # äº¤å‰æ³¨æ„åŠ› (æŸ¥è¯¢=ç»“æ„, é”®å€¼=ç‰¹å¾åºåˆ—) è¿™é‡Œç®€åŒ–ä¸ºå†ä¸€æ¬¡èåˆå±‚
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=structure_hidden,
            num_heads=structure_heads,
            dropout=dropout,
            batch_first=True,
        )
        self.cross_norm = nn.LayerNorm(structure_hidden)
        # èåˆçº¿æ€§
        self.fuse = nn.Sequential(
            nn.Linear(structure_hidden * 2, structure_hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.LayerNorm(structure_hidden),
        )
        # Policy head (deeper)
        self.policy_head = nn.Sequential(
            nn.Linear(structure_hidden, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, policy_output_dim),
        )
        # Value head: å¤šä»»åŠ¡è¾“å‡ºï¼ˆæ€»ä»·å€¼æ ‡é‡ + 8ä¸ªç»„ä»¶å‘é‡ï¼‰
        # å…±äº«ä¸»å¹²
        self.value_backbone = nn.Sequential(
            nn.Linear(structure_hidden, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.Dropout(dropout),
        )
        # æ€»ä»·å€¼å¤´ï¼ˆæ ‡é‡ï¼Œç”¨äºMCTSï¼‰
        self.value_scalar_head = nn.Sequential(
            nn.Linear(256, 1),
            nn.Tanh(),
        )
        # ç»„ä»¶ä»·å€¼å¤´ï¼ˆ8ç»´å‘é‡ï¼šposition/settling/control/smoothness/gain/saturation/peak/high_freqï¼‰
        self.value_components_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(128, 8),  # 8ä¸ªç»„ä»¶
            nn.Tanh(),  # å„ç»„ä»¶å½’ä¸€åŒ–åˆ°[-1,1]
        )

    def forward(self, data: Batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼Œè¿”å›å¤šä»»åŠ¡è¾“å‡º
        
        Returns:
            policy_logits: [B, action_dim] ç­–ç•¥logits
            value_scalar: [B] æ€»ä»·å€¼æ ‡é‡ï¼ˆç”¨äºMCTSï¼‰
            value_components: [B, 8] ç»„ä»¶ä»·å€¼å‘é‡ï¼ˆç”¨äºè®­ç»ƒæ¢¯åº¦å¯†é›†åŒ–ï¼‰
        """
        # 1. ç»“æ„ç¼–ç  (å¾—åˆ°å›¾åµŒå…¥ + èŠ‚ç‚¹åµŒå…¥)
        graph_emb, node_emb, batch_index = self.structure_encoder(data)  # [B, H], [N, H]
        # 2. ç‰¹å¾ç¼–ç  (ä½¿ç”¨èŠ‚ç‚¹åµŒå…¥ä½œä¸ºåºåˆ—è¾“å…¥, äº§å‡º CLS token å›¾ç‰¹å¾)
        feature_graph_emb, sequences = self.feature_encoder(node_emb, batch_index)  # [B, H]
        # 3. äº¤å‰æ³¨æ„åŠ›: query=ç»“æ„å›¾åµŒå…¥, key/value=ç‰¹å¾ CLS æ‰©å±•å or æ•´ä¸ªåºåˆ— (ç®€åŒ–: ä½¿ç”¨ç‰¹å¾ CLS Emb å• token)
        # ä¸ºäº†è®©æ³¨æ„åŠ›æœ‰æ„ä¹‰, æˆ‘ä»¬æ„å»º key/value = feature_graph_emb ä½œä¸ºå•é•¿åº¦åºåˆ—
        B, H = graph_emb.size()
        q = graph_emb.unsqueeze(1)  # [B, 1, H]
        kv = feature_graph_emb.unsqueeze(1)  # [B, 1, H]
        attn_out, _ = self.cross_attn(q, kv, kv)  # [B, 1, H]
        attn_out = attn_out.squeeze(1)
        attn_out = self.cross_norm(attn_out + graph_emb)  # æ®‹å·® + LN
        # 4. èåˆ
        fused = self.fuse(torch.cat([attn_out, feature_graph_emb], dim=-1))  # [B, H]
        # ç¼“å­˜èåˆåµŒå…¥ï¼Œä¾› get_embedding() ä½¿ç”¨ï¼ˆç”¨äºrankingç½‘ç»œï¼‰
        self._cached_embedding = fused
        # 5. heads è¾“å‡º
        policy_logits = self.policy_head(fused)
        # å¤šä»»åŠ¡ä»·å€¼è¾“å‡º
        value_features = self.value_backbone(fused)  # [B, 256]
        value_scalar = self.value_scalar_head(value_features).squeeze(-1)  # [B]
        value_components = self.value_components_head(value_features)  # [B, 8]
        return policy_logits, value_scalar, value_components

    def get_embedding(self, data: Batch) -> torch.Tensor:
        """
        æå–ç¨‹åºçš„åµŒå…¥è¡¨ç¤ºï¼ˆfusionåçš„128ç»´å‘é‡ï¼‰ï¼Œä¾›rankingç½‘ç»œä½¿ç”¨
        
        Args:
            data: PyG Batchå¯¹è±¡
            
        Returns:
            embedding: [B, hidden] èåˆåçš„ç¨‹åºåµŒå…¥
        """
        # 1. ç»“æ„ç¼–ç 
        graph_emb, node_emb, batch_index = self.structure_encoder(data)  # [B, H], [N, H]
        # 2. ç‰¹å¾ç¼–ç 
        feature_graph_emb, sequences = self.feature_encoder(node_emb, batch_index)  # [B, H]
        # 3. äº¤å‰æ³¨æ„åŠ›
        B, H = graph_emb.size()
        q = graph_emb.unsqueeze(1)  # [B, 1, H]
        kv = feature_graph_emb.unsqueeze(1)  # [B, 1, H]
        attn_out, _ = self.cross_attn(q, kv, kv)  # [B, 1, H]
        attn_out = attn_out.squeeze(1)
        attn_out = self.cross_norm(attn_out + graph_emb)  # æ®‹å·® + LN
        # 4. èåˆï¼ˆè¿™æ˜¯æˆ‘ä»¬è¦æå–çš„åµŒå…¥ï¼‰
        fused = self.fuse(torch.cat([attn_out, feature_graph_emb], dim=-1))  # [B, H]
        return fused

# ----------------------------- å·¥å‚å‡½æ•° ----------------------------- #

def create_gnn_policy_value_net_v2(
    node_feature_dim: int,
    policy_output_dim: int,
    structure_hidden: int = 256,
    structure_layers: int = 5,
    structure_heads: int = 8,
    feature_layers: int = 3,
    feature_heads: int = 8,
    dropout: float = 0.1,
) -> GNNPolicyValueNetV2:
    model = GNNPolicyValueNetV2(
        node_feature_dim=node_feature_dim,
        policy_output_dim=policy_output_dim,
        structure_hidden=structure_hidden,
        structure_layers=structure_layers,
        structure_heads=structure_heads,
        feature_layers=feature_layers,
        feature_heads=feature_heads,
        dropout=dropout,
    )
    return model

# ----------------------------- è‡ªæ£€ä¸å¿«é€Ÿæµ‹è¯• ----------------------------- #

def _synthetic_graph_batch(batch_size: int = 8, avg_nodes: int = 18, node_feature_dim: int = 24, edge_prob: float = 0.15) -> Batch:
    import random
    datas = []
    for _ in range(batch_size):
        n = max(4, int(random.gauss(avg_nodes, 3)))
        x = torch.randn(n, node_feature_dim)
        # éšæœºæ— å‘å›¾ -> è½¬ä¸ºåŒå‘è¾¹
        edges = []
        for i in range(n):
            for j in range(n):
                if i != j and random.random() < edge_prob:
                    edges.append((i, j))
        if not edges:
            # è‡³å°‘è¿ä¸€æ¡è¾¹
            if n > 1:
                edges.append((0, 1))
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
        data = Data(x=x, edge_index=edge_index)
        datas.append(data)
    return Batch.from_data_list(datas)


def _run_quick_test():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch = _synthetic_graph_batch(batch_size=12).to(device)
    model = create_gnn_policy_value_net_v2(node_feature_dim=24, policy_output_dim=14).to(device)
    print("æ¨¡å‹å‚æ•°æ€»æ•°:", count_parameters(model))
    t0 = time.time()
    policy_logits, value_scalar, value_components = model(batch)
    t1 = time.time()
    print("å‰å‘è¾“å‡º shapes:", policy_logits.shape, value_scalar.shape, value_components.shape)
    loss = policy_logits.mean() + value_scalar.mean() + value_components.mean()
    loss.backward()
    t2 = time.time()
    total_grad = 0.0
    for p in model.parameters():
        if p.grad is not None:
            total_grad += p.grad.abs().mean().item()
    print(f"å‰å‘è€—æ—¶: {t1 - t0:.4f}s, åå‘è€—æ—¶: {t2 - t1:.4f}s, å¹³å‡æ¢¯åº¦: {total_grad:.4f}")
    # å†…å­˜å ç”¨ä¼°è®¡ (ä»… GPU)
    if device.type == "cuda":
        torch.cuda.synchronize()
        mem = torch.cuda.memory_allocated(device) / (1024**2)
        print(f"GPU å½“å‰æ˜¾å­˜å ç”¨ ~{mem:.2f} MB")

if __name__ == "__main__":
    _run_quick_test()
