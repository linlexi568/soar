"""åœ¨çº¿è®­ç»ƒä¸»å¾ªç¯ - AlphaZeroå¼ç¨‹åºåˆæˆ

ä»é›¶å¼€å§‹è®­ç»ƒï¼šNNéšæœºåˆå§‹åŒ– â†’ MCTSæœç´¢ â†’ æ”¶é›†æ ·æœ¬ â†’ æ›´æ–°NN â†’ å¾ªç¯
"""
from __future__ import annotations

# ã€ä¿®å¤Python 3.13å…¼å®¹æ€§ã€‘ç¦ç”¨PyTorchç¼–è¯‘åŠŸèƒ½
import os
os.environ['PYTORCH_JIT'] = '0'
os.environ['TORCH_COMPILE_DISABLE'] = '1'

import argparse, time, json, random, os
from typing import List, Dict, Any, Tuple, Optional
from collections import deque
import numpy as np

# å¯¼å…¥ç°æœ‰æ¨¡å— - ç®€åŒ–å¯¼å…¥,åªå¯¼å…¥å¿…éœ€ç»„ä»¶
import sys, pathlib
_SCRIPT_DIR = pathlib.Path(__file__).resolve().parent
_PKG_ROOT = _SCRIPT_DIR.parent
if str(_PKG_ROOT) not in sys.path:
    sys.path.insert(0, str(_PKG_ROOT))
if str(_SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(_SCRIPT_DIR))

# Ensure Isaac Gym python bindings are importable (repo vendor path)
try:
    _REPO_ROOT = _PKG_ROOT.parent
    _GYM_PY = _REPO_ROOT / 'isaacgym' / 'python'
    if _GYM_PY.exists() and str(_GYM_PY) not in sys.path:
        sys.path.insert(0, str(_GYM_PY))
    # æå‰å¯¼å…¥ isaacgymï¼Œç¡®ä¿å…¶å…ˆäº torch å¯¼å…¥
    try:
        from isaacgym import gymapi  # type: ignore
    except Exception:
        pass
except Exception:
    pass

# ç›´æ¥å¯¼å…¥å¿…éœ€æ¨¡å—ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
from mcts_training.mcts import MCTS_Agent, MCTSNode
from mcts_training.policy.policy_nn import EDIT_TYPES  # PolicyValueNNLarge å·²ç§»é™¤ (å›ºå®šç‰¹å¾ç½‘ç»œå¼ƒç”¨)
from core.ast_pipeline import to_ast_program, has_u_set, to_serializable_dict  # AST-first pipeline

# GNN v2æ¨¡å—ï¼ˆåˆ†å±‚æ¶æ„ï¼‰
try:
    from models.gnn_features import ast_to_pyg_graph, batch_programs_to_graphs
    from models.gnn_policy_nn_v2 import create_gnn_policy_value_net_v2
    from torch_geometric.data import Batch as PyGBatch
    GNN_V2_AVAILABLE = True
except ImportError as e:
    print(f"[Warning] GNN v2æ¨¡å—ä¸å¯ç”¨: {e}")

# Ranking Value Networkï¼ˆç”¨äºè‡ªé€‚åº”å¥–åŠ±å­¦ä¹ ï¼Œæ‰“ç ´å¹³å¦å¥–åŠ±å›°å¢ƒï¼‰
try:
    from models.ranking_value_net import (
        RankingValueNet, PairwiseRankingBuffer,
        compute_ranking_loss, generate_program_pairs,
        setup_ranking_training, train_ranking_step
    )
    RANKING_AVAILABLE = True
except ImportError as e:
    print(f"[Warning] Rankingç½‘ç»œä¸å¯ç”¨: {e}")
    RANKING_AVAILABLE = False
    GNN_V2_AVAILABLE = False
    ast_to_pyg_graph = None
    batch_programs_to_graphs = None
    create_gnn_policy_value_net_v2 = None  # type: ignore
    PyGBatch = None

# å¯¼å…¥batch_evaluationï¼ˆå¯èƒ½éœ€è¦Isaac Gymï¼‰ï¼›ç¡®ä¿åœ¨å¯¼å…¥ torch ä¹‹å‰å°è¯•å¯¼å…¥ isaacgym
try:
    from utils.batch_evaluation import BatchEvaluator
    BATCH_EVAL_AVAILABLE = True
except Exception as e:
    print(f"[Warning] BatchEvaluatorä¸å¯ç”¨: {e}")
    BATCH_EVAL_AVAILABLE = False
    BatchEvaluator = None  # type: ignore

try:
    from utilities.trajectory_presets import get_scg_trajectory_config
except Exception as e:
    raise ImportError(f"æ— æ³•å¯¼å…¥ safe-control-gym è½¨è¿¹åŠ©æ‰‹: {e}")

try:
    from utils.prior_scoring import PRIOR_PROFILES
except Exception:
    PRIOR_PROFILES = {"none": (0.0, 0.0)}

try:
    from utils.program_constraints import validate_program, HARD_CONSTRAINT_PENALTY
except Exception:
    try:
        from program_constraints import validate_program, HARD_CONSTRAINT_PENALTY  # type: ignore
    except Exception:
        def validate_program(_program):  # type: ignore
            return True, ""
        HARD_CONSTRAINT_PENALTY = -1e6  # type: ignore

# ç°åœ¨å†å¯¼å…¥ torch åŠå…¶å­æ¨¡å—ï¼Œé¿å…ç ´å isaacgym çš„å¯¼å…¥é¡ºåºè¦æ±‚
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# å¯¼å…¥serialization
try:
    from core.serialization import save_program_json as _save_prog
    def save_program_json(program, path, meta=None):  # type: ignore
        _save_prog(program, path, meta=meta)
except Exception:
    def save_program_json(program, path, meta=None):  # type: ignore
        import json, os, time
        # ç®€åŒ–ç‰ˆä¿å­˜ï¼ˆä¸åŒ…å«èŠ‚ç‚¹å¯¹è±¡ï¼‰
        simplified = []
        for rule in program:
            simple_rule = {
                'name': rule.get('name', 'rule'),
                'multiplier': rule.get('multiplier', [1.0, 1.0, 1.0])
            }
            simplified.append(simple_rule)
        
        payload = {'rules': simplified, 'note': 'Simplified format'}
        if meta:
            payload['meta'] = meta
        payload.setdefault('meta', {})['saved_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            json.dump(payload, f, indent=2)


def _normalize_constants_for_hash(obj):
    """é€’å½’æ›¿æ¢æ‰€æœ‰æµ®ç‚¹å¸¸æ•°ä¸ºå ä½ç¬¦ï¼Œç”¨äºç»“æ„å“ˆå¸Œ
    
    ç›®çš„ï¼šè®©ç»“æ„ç›¸åŒä½†å‚æ•°ä¸åŒçš„ç¨‹åºå…±äº«åŒä¸€ä¸ªGNNç¼“å­˜
    """
    if isinstance(obj, dict):
        result = {}
        for k, v in obj.items():
            if k == 'value' and isinstance(v, (int, float)):
                # å¸¸æ•°å€¼ç»Ÿä¸€æ›¿æ¢ä¸ºå ä½ç¬¦
                result[k] = '<CONST>'
            else:
                result[k] = _normalize_constants_for_hash(v)
        return result
    elif isinstance(obj, list):
        return [_normalize_constants_for_hash(item) for item in obj]
    elif isinstance(obj, (int, float)):
        return '<CONST>'
    else:
        return obj


def get_program_hash(program, ignore_constants=True):
    """ç”Ÿæˆç¨‹åºçš„ç¨³å®šå“ˆå¸Œå€¼ç”¨äºç¼“å­˜ã€‚
    
    ä½¿ç”¨ASTåºåˆ—åŒ–åçš„JSONï¼ˆæ’åºé”®ï¼‰è®¡ç®—blake2sï¼Œé¿å…å†…å­˜åœ°å€å¯¼è‡´çš„ä¼ªå·®å¼‚ï¼Œ
    æå¤§æé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚
    
    Args:
        program: ç¨‹åºè¡¨ç¤ºï¼ˆå¯ä»¥æ˜¯DSLç¨‹åºã€ASTç­‰ï¼‰
        ignore_constants: æ˜¯å¦å¿½ç•¥å¸¸æ•°å€¼ï¼ˆä»…ä¿ç•™ç»“æ„ï¼‰ï¼Œé»˜è®¤True
                         Trueæ—¶ï¼Œæ‰€æœ‰å¸¸æ•°å€¼æ›¿æ¢ä¸º'<CONST>'ï¼Œå¤§å¹…æé«˜BOåœºæ™¯ä¸‹çš„ç¼“å­˜å‘½ä¸­ç‡
        
    Returns:
        str: ç¨‹åºçš„å“ˆå¸Œå€¼ï¼ˆ16è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
    """
    try:
        import json, hashlib
        from core.serialization import to_serializable_dict
        serial = to_serializable_dict(program)  # {'rules': ...}
        
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šå¿½ç•¥å¸¸æ•°å€¼ï¼ŒåªåŸºäºç»“æ„å“ˆå¸Œ
        if ignore_constants:
            serial = _normalize_constants_for_hash(serial)
        
        s = json.dumps(serial, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
        return hashlib.blake2s(s.encode('utf-8')).hexdigest()
    except Exception:
        # å›é€€ï¼šä½¿ç”¨å­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆå°½é‡ç¨³å®šï¼‰ï¼›å¤±è´¥åˆ™ä½¿ç”¨idï¼ˆæœ€å·®æƒ…å†µï¼‰
        try:
            return str(program)
        except Exception:
            return str(id(program))


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆæ”¯æŒå›ºå®šç‰¹å¾å’ŒGNNå›¾æ•°æ®ï¼‰"""
    
    def __init__(self, capacity: int = 50000, use_gnn: bool = False):
        self.capacity = capacity
        self.use_gnn = use_gnn
        self.buffer = deque(maxlen=capacity)
    
    def push(self, sample: Dict[str, Any]):
        """æ·»åŠ æ ·æœ¬
        
        GNNæ¨¡å¼: sample = {'graph': PyG Data, 'policy_target': tensor}
        æ³¨æ„ï¼šå·²ç§»é™¤value_targetï¼Œåªè®­ç»ƒpolicy
        """
        self.buffer.append(sample)
    
    def sample(self, batch_size: int) -> List[Dict[str, Any]]:
        """éšæœºé‡‡æ ·"""
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))
    
    def __len__(self):
        return len(self.buffer)


class OnlineTrainer:
    """åœ¨çº¿è®­ç»ƒå™¨ - AlphaZeroèŒƒå¼"""
    
    def __init__(self, args):
        self.args = args
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"[Trainer] ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç»Ÿä¸€ä½¿ç”¨ GNN v2ï¼Œè‹¥ä¸å¯ç”¨ç›´æ¥æŠ¥é”™ç»ˆæ­¢ï¼ˆç§»é™¤å›ºå®šç‰¹å¾å›é€€ï¼‰
        if not GNN_V2_AVAILABLE:
            raise ImportError("GNN v2 æ¨¡å—ä¸å¯ç”¨ï¼Œå·²ç§»é™¤å›ºå®šç‰¹å¾ç½‘ç»œå›é€€ï¼Œè¯·å®‰è£… torch-geometric ç­‰ä¾èµ–ã€‚")
        self.use_gnn = True
        
        # åˆå§‹åŒ–NNï¼ˆGNNç»Ÿä¸€ä½¿ç”¨v2åˆ†å±‚æ¶æ„ï¼‰
        gnn_structure_hidden = getattr(args, 'gnn_structure_hidden', 256)
        gnn_structure_layers = getattr(args, 'gnn_structure_layers', 5)
        gnn_structure_heads = getattr(args, 'gnn_structure_heads', 8)
        gnn_feature_layers = getattr(args, 'gnn_feature_layers', 3)
        gnn_feature_heads = getattr(args, 'gnn_feature_heads', 8)
        gnn_dropout = getattr(args, 'gnn_dropout', 0.1)
        
        print(f"[Trainer] ä½¿ç”¨ GNN v2 (Hierarchical Dual) åˆ†å±‚ç½‘ç»œ")
        print(f"[Trainer] GNNæ¶æ„: structure({gnn_structure_hidden}dÃ—{gnn_structure_layers}LÃ—{gnn_structure_heads}H), "
              f"feature({gnn_feature_layers}LÃ—{gnn_feature_heads}H), dropout={gnn_dropout}")
        
        self.nn_model = create_gnn_policy_value_net_v2(
            node_feature_dim=24,
            policy_output_dim=len(EDIT_TYPES),
            structure_hidden=gnn_structure_hidden,
            structure_layers=gnn_structure_layers,
            structure_heads=gnn_structure_heads,
            feature_layers=gnn_feature_layers,
            feature_heads=gnn_feature_heads,
            dropout=gnn_dropout
        ).to(self.device)
        
        # ç¦ç”¨torch compileé¿å…Python 3.13å…¼å®¹æ€§é—®é¢˜
        try:
            import os
            os.environ['PYTORCH_JIT'] = '0'
            os.environ['TORCH_COMPILE_DISABLE'] = '1'
        except Exception:
            pass
        
        try:
            self.optimizer = optim.Adam(
                self.nn_model.parameters(),
                lr=args.learning_rate,
                weight_decay=1e-4
            )
        except KeyboardInterrupt:
            raise
        except Exception as e:
            # å¦‚æœæ ‡å‡†Adamå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨åˆ›å»º
            print(f"[Warning] Adamåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ: {e}")
            self.optimizer = optim.SGD(
                self.nn_model.parameters(),
                lr=args.learning_rate,
                momentum=0.9
            )
        
        print(f"[Trainer] NNåˆå§‹åŒ–å®Œæˆ (å‚æ•°: {sum(p.numel() for p in self.nn_model.parameters())})")
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(capacity=args.replay_capacity, use_gnn=self.use_gnn)

        # å…ˆéªŒé…ç½®ï¼ˆç»“æ„/ç¨³å®šæ€§ï¼‰
        prior_profile_key = getattr(args, 'prior_profile', 'none')
        preset_structure, preset_stability = PRIOR_PROFILES.get(prior_profile_key, PRIOR_PROFILES.get('none', (0.0, 0.0)))
        structure_override = getattr(args, 'structure_prior_weight', None)
        stability_override = getattr(args, 'stability_prior_weight', None)
        self.structure_prior_weight = float(preset_structure if structure_override is None else structure_override)
        self.stability_prior_weight = float(preset_stability if stability_override is None else stability_override)
        print(f"[Trainer] å…ˆéªŒé…ç½®: profile={prior_profile_key} => structure={self.structure_prior_weight:.3f}, stability={self.stability_prior_weight:.3f}")
        
        # è°ƒè¯•ï¼šé€æ­¥å¥–åŠ±ä¸é›¶åŠ¨ä½œç»Ÿè®¡
        if getattr(args, 'debug_rewards', False):
            os.environ['DEBUG_STEPWISE'] = '1'
        if not BATCH_EVAL_AVAILABLE:
            raise RuntimeError("BatchEvaluator ä¸å¯ç”¨ï¼šè¯·ç¡®ä¿ Isaac Gym åŠç›¸å…³ä¾èµ–å·²æ­£ç¡®å®‰è£…ã€‚é¡¹ç›®å·²ç§»é™¤ DummyEvaluator å›é€€ï¼Œæ‰€æœ‰è®­ç»ƒå¿…é¡»ä½¿ç”¨çœŸå®å¥–åŠ±ã€‚")

        print("[Trainer] ä½¿ç”¨ BatchEvaluatorï¼ˆçœŸå® Isaac Gym å¥–åŠ±ï¼‰")
        self.evaluator = BatchEvaluator(
            trajectory_config=self._build_trajectory(),
            duration=args.duration,
            isaac_num_envs=args.isaac_num_envs,
            device=str(self.device),
            replicas_per_program=getattr(args, 'eval_replicas_per_program', 1),
            min_steps_frac=getattr(args, 'min_steps_frac', 0.0),
            reward_reduction=getattr(args, 'reward_reduction', 'sum'),
            reward_profile=getattr(args, 'reward_profile', 'safe_control_tracking'),  # SCG-only reward profile
            use_scg_exact_reward=True,
            strict_no_prior=True,  # ç»Ÿä¸€ä½¿ç”¨ç›´æ¥u_*æ§åˆ¶ï¼ˆä¸ä¾èµ–å†…ç½®PIDæ¡†æ¶ï¼‰
            zero_action_penalty=float(getattr(args, 'zero_action_penalty', 0.0)),  # å‚æ•°åŒ–é›¶åŠ¨ä½œæƒ©ç½š
            complexity_bonus=0.0,  # AlphaZeroå“²å­¦ï¼šè®©NNè‡ªå·±å­¦ä¹ å¤æ‚åº¦æƒè¡¡
            use_fast_path=getattr(args, 'use_fast_path', False),
            use_gpu_expression_executor=not getattr(args, 'disable_gpu_expression', False),
            action_scale_multiplier=float(getattr(args, 'action_scale_multiplier', 1.0)),  # åŠ¨ä½œç¼©æ”¾ç³»æ•°
            structure_prior_weight=self.structure_prior_weight,
            stability_prior_weight=self.stability_prior_weight,
            enable_output_mad=getattr(args, 'enable_output_mad', True),
            mad_min_fz=float(getattr(args, 'mad_min_fz', 0.0)),
            mad_max_fz=float(getattr(args, 'mad_max_fz', 7.5)),
            mad_max_xy=float(getattr(args, 'mad_max_xy', 0.12)),
            mad_max_yaw=float(getattr(args, 'mad_max_yaw', 0.04)),
            mad_max_delta_fz=float(getattr(args, 'mad_max_delta_fz', 1.5)),
            mad_max_delta_xy=float(getattr(args, 'mad_max_delta_xy', 0.03)),
            mad_max_delta_yaw=float(getattr(args, 'mad_max_delta_yaw', 0.02)),
            enable_bayesian_tuning=getattr(args, 'enable_bayesian_tuning', False),
            bo_batch_size=getattr(args, 'bo_batch_size', 50),
            bo_iterations=getattr(args, 'bo_iterations', 3),
            # å‚æ•°èŒƒå›´å…œåº•ï¼šå¦‚æœèŠ‚ç‚¹è‡ªèº«æ²¡æœ‰ min/maxï¼Œåˆ™ä½¿ç”¨æ­¤å¤„é»˜è®¤å€¼
            bo_param_ranges={'default': (-3.0, 3.0)}
        )
        
        # ç»Ÿè®¡
        self.iteration = 0
        self.best_reward = -float('inf')
        self.best_program = None
        self.best_program_copy = None  # ğŸ”’ æ·±æ‹·è´ä¿æŠ¤,é˜²æ­¢cleanup_treeæ¸…ç†
        self.training_stats = []
        self._mcts_stats = {}  # MCTSæ€§èƒ½ç»Ÿè®¡
        
        # ğŸ”„ å¼‚æ­¥è®­ç»ƒæ”¯æŒ
        self.async_training = getattr(args, 'async_training', False)
        self.async_trainer = None  # ç¨ååˆå§‹åŒ–
        
        # ï¿½ ä¸‰åˆä¸€ä¼˜åŒ–å¼€å…³
        self.enable_ranking_mcts_bias = getattr(args, 'enable_ranking_mcts_bias', True)
        self.ranking_bias_beta = getattr(args, 'ranking_bias_beta', 0.3)
        self.enable_value_head = getattr(args, 'enable_value_head', True)
        self.enable_ranking_reweight = getattr(args, 'enable_ranking_reweight', True)
        self.ranking_reweight_beta = getattr(args, 'ranking_reweight_beta', 0.2)
        
        if self.enable_ranking_mcts_bias:
            print(f"[Trainer] âœ… Rankingâ†’MCTSåç½®å·²å¯ç”¨ (beta={self.ranking_bias_beta})")
        if self.enable_value_head:
            print(f"[Trainer] âœ… Valueå¤´è¾…åŠ©è®­ç»ƒå·²å¯ç”¨ï¼ˆçº¯è®­ç»ƒä¿¡å·ï¼ŒMCTSä»ç”¨çœŸå®ä»¿çœŸï¼‰")
        if self.enable_ranking_reweight:
            print(f"[Trainer] âœ… Rankingâ†’Policyé‡åŠ æƒå·²å¯ç”¨ (beta={self.ranking_reweight_beta})")

        # Progressive Widening å¼€å…³ï¼ˆå¯é€‰å®Œå…¨æ”¾å¼€æ ‘å®½ï¼‰
        self.disable_progressive_widening = bool(getattr(args, 'disable_progressive_widening', False))
        if self.disable_progressive_widening:
            print("[Trainer] âš ï¸ Progressive Widening å·²ç¦ç”¨ï¼šèŠ‚ç‚¹å°†ç›´æ¥æŒ‰å…¨éƒ¨å¯å˜å¼‚æ•°æ‰©å±•")
        
        # ğŸš€ æ‚¬åœæ¨åŠ›çº¦æŸé…ç½®ï¼ˆHover Thrust Constraintï¼‰
        # å¼ºåˆ¶ u_fz = hover_thrust + deltaï¼Œç¡®ä¿æ— äººæœºå§‹ç»ˆæœ‰æœ€å°å‡åŠ›
        self._enforce_hover_thrust = getattr(args, 'enforce_hover_thrust', True)
        self._hover_thrust_value = float(getattr(args, 'hover_thrust_value', 0.265))
        self._hover_thrust_min = float(getattr(args, 'hover_thrust_min', 0.20))
        self._hover_thrust_max = float(getattr(args, 'hover_thrust_max', 0.35))
        self._hover_delta_max = float(getattr(args, 'hover_delta_max', 2.0))
        if self._enforce_hover_thrust:
            print(f"[Trainer] ğŸš æ‚¬åœæ¨åŠ›çº¦æŸå·²å¯ç”¨: hover={self._hover_thrust_value:.3f}N [{self._hover_thrust_min:.2f}, {self._hover_thrust_max:.2f}], delta_max={self._hover_delta_max:.1f}N")
        else:
            print(f"[Trainer] âš ï¸ æ‚¬åœæ¨åŠ›çº¦æŸå·²ç¦ç”¨ï¼ˆå…è®¸ç¨‹åºè¾“å‡ºé›¶æ¨åŠ›ï¼‰")
        
        # ğŸ† ç²¾è‹±ç¨‹åºæ±  (Elite Archive) - ä¿ç•™Top-Kæœ€ä¼˜ç¨‹åº
        self.elite_archive = []  # [(reward, program_copy, iter_idx), ...]
        self.elite_archive_size = getattr(args, 'elite_archive_size', 100)  
        print(f"[Trainer] ğŸ† ç²¾è‹±ç¨‹åºæ± : ä¿ç•™Top-{self.elite_archive_size}æœ€ä¼˜ç¨‹åº")

        # MCTS æœç´¢å‚æ•°å¯¹å¤–å°é—­ï¼šå›ºå®šä¸ºå†…éƒ¨å¸¸é‡ï¼ˆä»…ä¿ç•™â€œæ¨¡æ‹Ÿæ¬¡æ•°â€å¯è°ƒï¼‰
        # è¿™äº›å‚æ•°ä¸é€šè¿‡ CLI æš´éœ²ï¼Œç¡®ä¿â€œåªè°ƒ NNâ€ç­–ç•¥
        self._exploration_weight = 2.5
        self._puct_c = 1.5
        self._max_depth = getattr(args, 'max_depth', 3)  # ä»å‘½ä»¤è¡Œè¯»å–ï¼Œé»˜è®¤3
        # æ³¨æ„ï¼šå·²ç§»é™¤value headï¼Œå…¨éƒ¨ä½¿ç”¨çœŸå®ä»¿çœŸ
        # Dirichlet / æ¸©åº¦æ¢ç´¢å‚æ•°ï¼ˆå†…éƒ¨å›ºå®š + é€€ç«æ—¥ç¨‹ï¼‰
        # ğŸ”¥ Meta-RL æˆ–å¯å‘å¼è¡°å‡å‚æ•°é…ç½®
        self.use_meta_rl = getattr(args, 'use_meta_rl', False)
        self.meta_rl_controller = None
        
        if self.use_meta_rl:
            # åŠ è½½ Meta-RL RNN æ§åˆ¶å™¨
            from meta_rl.controller import MetaRLController
            meta_ckpt = getattr(args, 'meta_rl_checkpoint', 'meta_rl/checkpoints/meta_policy.pt')
            print(f"[Trainer] ğŸ§  å¯ç”¨ Meta-RL åŠ¨æ€è°ƒå‚ (æ¨¡å‹: {meta_ckpt})")
            self.meta_rl_controller = MetaRLController(checkpoint_path=meta_ckpt, device=self.device)
            # Meta-RL æ¨¡å¼ä¸‹åˆå§‹å€¼ç”±æ§åˆ¶å™¨å†³å®š
            self._root_dirichlet_eps = 0.25
            self._root_dirichlet_alpha = 0.30
        else:
            # å¯å‘å¼è¡°å‡æ¨¡å¼ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–
            if getattr(args, 'root_dirichlet_eps_init', None) is not None:
                # ç”¨æˆ·æŒ‡å®šäº†å¯å‘å¼å‚æ•°
                self._root_dirichlet_eps_init = float(args.root_dirichlet_eps_init)
                self._root_dirichlet_eps_final = float(getattr(args, 'root_dirichlet_eps_final', self._root_dirichlet_eps_init))
                self._root_dirichlet_alpha_init = float(getattr(args, 'root_dirichlet_alpha_init', args.root_dirichlet_alpha))
                self._root_dirichlet_alpha_final = float(getattr(args, 'root_dirichlet_alpha_final', self._root_dirichlet_alpha_init))
                self._root_dirichlet_decay_iters = int(getattr(args, 'heuristic_decay_window', 200))
                print(f"[Trainer] ğŸ“‰ å¯å‘å¼é€€ç«: eps={self._root_dirichlet_eps_init:.2f}â†’{self._root_dirichlet_eps_final:.2f}, alpha={self._root_dirichlet_alpha_init:.2f}â†’{self._root_dirichlet_alpha_final:.2f} (çª—å£={self._root_dirichlet_decay_iters})")
            else:
                # ä½¿ç”¨å†…éƒ¨é»˜è®¤å€¼
                self._root_dirichlet_eps_init = 0.60
                self._root_dirichlet_eps_final = 0.15
                self._root_dirichlet_alpha_init = 0.50
                self._root_dirichlet_alpha_final = 0.30
                self._root_dirichlet_decay_iters = 600
                print(f"[Trainer] ğŸ“‰ é»˜è®¤é€€ç«æ—¥ç¨‹: eps={self._root_dirichlet_eps_init:.2f}â†’{self._root_dirichlet_eps_final:.2f}, alpha={self._root_dirichlet_alpha_init:.2f}â†’{self._root_dirichlet_alpha_final:.2f}")
            
            self._root_dirichlet_eps = self._root_dirichlet_eps_init
            self._root_dirichlet_alpha = self._root_dirichlet_alpha_init
        # æ¸©åº¦é€€ç«æ—¥ç¨‹ï¼šä»é«˜æ¸©ï¼ˆæ¢ç´¢ï¼‰é€æ­¥é™è‡³ä½æ¸©ï¼ˆåˆ©ç”¨ï¼‰
        self._policy_temperature_init = 2.0  # ğŸ”§ æé«˜åˆå§‹æ¸©åº¦ï¼š1.5â†’2.0 - æ›´å¼ºæ¢ç´¢
        self._policy_temperature_final = 0.8  # ğŸ”§ æé«˜æœ€ç»ˆæ¸©åº¦ï¼š0.5â†’0.8 - ä¿æŒæ¢ç´¢æ€§
        self._policy_temperature_decay_iters = 500  # 500è½®å†…å®Œæˆé€€ç«
        self._policy_temperature = self._policy_temperature_init
        print(
            f"[Trainer] MCTSå‚æ•°å·²å°é—­: exploration_weight=2.5, puct_c=1.5, max_depth={self._max_depth}, "
            f"root_dirichlet=(eps={self._root_dirichlet_eps}, alpha={self._root_dirichlet_alpha})ï¼›ä»… --mcts-simulations å¯è°ƒï¼›å…¨éƒ¨ä½¿ç”¨çœŸå®ä»¿çœŸ"
        )
        print(f"[Trainer] æ¸©åº¦é€€ç«: {self._policy_temperature_init:.2f} â†’ {self._policy_temperature_final:.2f} (over {self._policy_temperature_decay_iters} iters)")
        # æ ¹èŠ‚ç‚¹å…ˆéªŒè¦†ç›–ç‡é˜ˆå€¼ï¼ˆç”¨äºè‡ªé€‚åº”æœ€å°åˆ†æ”¯æ§åˆ¶ï¼Œé¿å…æ‰‹å·¥æŒ‡å®šå›ºå®šåˆ†æ”¯æ•°ï¼‰
        self._root_prior_coverage_tau = 0.80
        print(f"[Trainer] Rootæœ€å°åˆ†æ”¯è‡ªé€‚åº”ï¼šå…ˆéªŒç´¯è®¡è¦†ç›–ç‡ Ï„={self._root_prior_coverage_tau:.2f}")
        # NN å‚æ•°æ ¡éªŒï¼šè®°å½•ä¸€æ¬¡å‚æ•°æ ¡éªŒå’Œç”¨äºåç»­å¾®å°å˜æ›´è§‚æµ‹ï¼ˆä¸å½±å“è®­ç»ƒï¼‰
        self._last_param_checksum = self._compute_param_checksum()

        # ä¸€å…ƒåŸè¯­å‚æ•°ç½‘æ ¼è¯¾ç¨‹ï¼šå…ˆç²—åç»†ï¼Œæå‡ prior å¬å›
        self._unary_grid_stage1_iters = 200  # ç²—ç½‘æ ¼é˜¶æ®µ
        self._unary_grid_stage2_iters = 600  # è¿‡æ¸¡é˜¶æ®µ

        # ç²¾è‹±ç¨‹åºæ ¹ç§å­ï¼šé€‚åº¦åˆ©ç”¨å†å²æœ€ä¼˜ï¼Œæå‡ prior å¤ç”¨
        self._elite_seed_prob = 0.25
        self._elite_seed_topk = 5
        self._elite_seed_delay = 20  # è‡³å°‘ç§¯ç´¯è‹¥å¹²è½®åå†å¯ç”¨
        
        # ğŸš€ Ranking Value Networkï¼ˆè‡ªé€‚åº”å¥–åŠ±å­¦ä¹ ï¼Œè§£å†³å¹³å¦å¥–åŠ±é—®é¢˜ï¼‰
        self.use_ranking = getattr(args, 'use_ranking', True) and RANKING_AVAILABLE
        if self.use_ranking:
            print(f"[Trainer] ğŸ”¥ å¯ç”¨ Ranking Value Network (Ranking Policy Gradient)")
            self.ranking_net, self.ranking_buffer, self.ranking_optimizer = setup_ranking_training(
                gnn_model=self.nn_model,  # ä¼ é€’GNNæ¨¡å‹
                device=self.device,
                learning_rate=getattr(args, 'ranking_lr', 1e-3),
                embed_dim=gnn_structure_hidden  # ä½¿ç”¨ GNN çš„å®é™… hidden size
            )
            # æ··åˆç³»æ•°ï¼šé€æ­¥ä»MCTS valueè¿‡æ¸¡åˆ°ranking value
            self.ranking_blend_factor = float(getattr(args, 'ranking_blend_init', 0.3))
            self.ranking_blend_max = float(getattr(args, 'ranking_blend_max', 0.8))
            self.ranking_blend_warmup_iters = int(getattr(args, 'ranking_blend_warmup', 100))
            print(f"[Trainer] Rankingæ··åˆ: åˆå§‹={self.ranking_blend_factor:.2f} â†’ æœ€å¤§={self.ranking_blend_max:.2f} (warmup={self.ranking_blend_warmup_iters}è½®)")
            # Ranking æ ·æœ¬è´¨é‡æ§åˆ¶ï¼šä»…ä¿ç•™å¥–åŠ±å·®è¶³å¤Ÿå¤§çš„æ ·æœ¬å¯¹ï¼Œé™ä½å™ªå£°ï¼ˆå†…éƒ¨å¸¸é‡ï¼Œä¸æš´éœ²CLIï¼‰
            self._ranking_min_delta = 0.05
            print(f"[Trainer] Rankingå¯¹è¿‡æ»¤: |Î”reward| â‰¥ {self._ranking_min_delta:.2f}")
        else:
            self.ranking_net = None
            self.ranking_buffer = None
            self.ranking_optimizer = None
            print(f"[Trainer] âš ï¸ Rankingç½‘ç»œæœªå¯ç”¨ (use_ranking={getattr(args, 'use_ranking', True)}, available={RANKING_AVAILABLE})")
    
    def _build_trajectory(self) -> Dict[str, Any]:
        """æ„å»ºä¸ safe-control-gym å¯¹é½çš„è½¨è¿¹é…ç½®ã€‚
        
        èµ·ç‚¹è§„èŒƒ (t=0):
        - Square:  [0, 0, 1]   (ä¸­å¿ƒï¼Œå…ˆå‘ +y ç§»åŠ¨)
        - Circle:  [R, 0, 1]   (åœ†å‘¨å³ä¾§ï¼ŒR=0.9æ—¶ä¸º [0.9, 0, 1])
        - Figure8: [0, 0, 1]   (ä¸­å¿ƒ)
        - Hover:   center      (æ‚¬åœç‚¹)
        """
        traj_cfg = get_scg_trajectory_config(self.args.traj)
        params = dict(traj_cfg.params)
        
        # è®¡ç®— t=0 æ—¶åˆ»è½¨è¿¹ä¸Šçš„ä½ç½®ä½œä¸ºåˆå§‹ä½ç½®
        initial_xyz = self._compute_trajectory_start(traj_cfg)

        if traj_cfg.task == 'hover':
            import random as _r
            curriculum = getattr(self.args, 'curriculum_mode', 'none')
            stage = getattr(self, '_curriculum_stage', 1)
            if curriculum != 'none':
                if stage == 1:
                    initial_xyz = list(traj_cfg.center)
                else:
                    amp_xy = 0.2 if stage == 2 else 0.5
                    amp_z = 0.1 if stage == 2 else 0.3
                    initial_xyz = [
                        traj_cfg.center[0] + _r.uniform(-amp_xy, amp_xy),
                        traj_cfg.center[1] + _r.uniform(-amp_xy, amp_xy),
                        traj_cfg.center[2] + _r.uniform(-amp_z, amp_z),
                    ]
            else:
                initial_xyz = [
                    traj_cfg.center[0] + _r.uniform(-0.5, 0.5),
                    traj_cfg.center[1] + _r.uniform(-0.5, 0.5),
                    traj_cfg.center[2] + _r.uniform(-0.3, 0.3),
                ]

        return {
            'type': traj_cfg.task,
            'initial_xyz': initial_xyz,
            'params': params,
        }
    
    def _compute_trajectory_start(self, traj_cfg) -> list:
        """è®¡ç®— t=0 æ—¶åˆ»è½¨è¿¹ä¸Šçš„ä½ç½®ã€‚
        
        è¿™ç¡®ä¿æ— äººæœºä»è½¨è¿¹çš„èµ·ç‚¹å¼€å§‹ï¼Œè€Œä¸æ˜¯è½¨è¿¹çš„ä¸­å¿ƒã€‚
        """
        from utilities.trajectory_presets import scg_position
        
        # ä½¿ç”¨ scg_position è®¡ç®— t=0 æ—¶åˆ»çš„ä½ç½®
        pos_t0 = scg_position(traj_cfg.task, t=0.0, params=traj_cfg.params, center=traj_cfg.center)
        return pos_t0.tolist()

    def _compute_param_checksum(self) -> float:
        """è®¡ç®—æ¨¡å‹å‚æ•°çš„ç®€å•æ ¡éªŒå’Œï¼ˆL2èŒƒæ•°æ±‚å’Œï¼‰ï¼Œç”¨äºè§‚æµ‹å‚æ•°æ˜¯å¦å‘ç”Ÿæ›´æ–°ã€‚"""
        with torch.no_grad():
            s = 0.0
            for p in self.nn_model.parameters():
                if p is not None and p.requires_grad:
                    try:
                        s += float(p.data.norm(2).item())
                    except Exception:
                        pass
            return float(s)
    
    def _generate_random_program(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆéšæœºåˆå§‹ç¨‹åº"""
        # ä½¿ç”¨MCTSçš„éšæœºç”Ÿæˆé€»è¾‘
        mcts = MCTS_Agent(
            evaluation_function=lambda p: 0.0,  # å ä½ç¬¦
            dsl_variables=['pos_err', 'vel_err'],
            dsl_constants=[0.0, 1.0],
            dsl_operators=['+', '-', '*'],
            structure_prior_weight=self.structure_prior_weight,
            stability_prior_weight=self.stability_prior_weight
        )
        # å•è½´æœç´¢ï¼šé»˜è®¤ä»… Roll é€šé“ï¼Œé¿å…ç©ºé—´çˆ†ç‚¸
        mcts._active_channels = ['u_tx']
        return mcts._generate_random_segmented_program()
    
    def _load_program_from_json(self, path: str) -> Optional[List[Dict[str, Any]]]:
        """ä» JSON æ–‡ä»¶åŠ è½½ç¨‹åºï¼ˆç”¨äº warm startï¼‰"""
        try:
            import json
            with open(path, 'r') as f:
                data = json.load(f)
            
            # å°è¯•æå– rules å­—æ®µ
            if isinstance(data, dict) and 'rules' in data:
                rules = data['rules']
            elif isinstance(data, list):
                rules = data
            else:
                print(f"[Warning] æ— æ³•è§£æç¨‹åºæ–‡ä»¶æ ¼å¼: {path}")
                return None
            
            # ç®€å•éªŒè¯
            if not isinstance(rules, list) or len(rules) == 0:
                print(f"[Warning] ç¨‹åºæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {path}")
                return None
            
            print(f"[Trainer] âœ… ä» {path} åŠ è½½äº† {len(rules)} æ¡è§„åˆ™")
            return rules
        except Exception as e:
            print(f"[Warning] åŠ è½½ç¨‹åºå¤±è´¥: {e}")
            return None
    
    def _quick_action_features(self, program: List[Dict[str, Any]]) -> List[float]:
        """å¿«é€Ÿæå–ç¨‹åºçš„åŠ¨ä½œå¹…åº¦ç‰¹å¾ï¼ˆç”¨äºRanking NNï¼‰
        
        ç®€åŒ–å®ç°ï¼šè¿”å›ç¨‹åºç»“æ„ç»Ÿè®¡ä½œä¸ºä»£ç†ç‰¹å¾
        é¿å…å¯¼å…¥program_executorï¼ˆå¯èƒ½æœ‰å¾ªç¯ä¾èµ–é—®é¢˜ï¼‰
        
        Returns:
            [fz_mean, fz_std, fz_max, tx_mean, tx_std, tx_max]
            å®é™…è¿”å›: [num_rules, num_vars, max_depth, 0, 0, 0]ï¼ˆç»“æ„ä»£ç†ï¼‰
        """
        try:
            # ç»Ÿè®¡ç¨‹åºç»“æ„ç‰¹å¾ä½œä¸ºåŠ¨ä½œå¹…åº¦çš„ä»£ç†
            # å‡è®¾ï¼šå¤æ‚ç¨‹åº â†’ æ›´å¤šè§„åˆ™/å˜é‡ â†’ æ›´å¤§åŠ¨ä½œå¹…åº¦
            num_rules = len([r for r in program if r.get('node') is not None])
            
            # ç»Ÿè®¡å”¯ä¸€å˜é‡æ•°
            unique_vars = set()
            def extract_vars(node):
                if node is None:
                    return
                if isinstance(node, dict):
                    if node.get('type') == 'variable':
                        unique_vars.add(node.get('name', ''))
                    for child in ['left', 'right', 'condition', 'true_branch', 'false_branch']:
                        if child in node:
                            extract_vars(node[child])
            
            for rule in program:
                extract_vars(rule.get('node'))
            
            num_vars = len(unique_vars)
            
            # è®¡ç®—æœ€å¤§æ·±åº¦ï¼ˆå¤æ‚åº¦æŒ‡æ ‡ï¼‰
            def max_depth(node):
                if node is None or not isinstance(node, dict):
                    return 0
                depths = []
                for child in ['left', 'right', 'condition', 'true_branch', 'false_branch']:
                    if child in node:
                        depths.append(max_depth(node[child]))
                return 1 + max(depths) if depths else 1
            
            depths = [max_depth(r.get('node')) for r in program if r.get('node') is not None]
            max_d = max(depths) if depths else 0
            
            # è¿”å›ç»“æ„ç‰¹å¾ä½œä¸ºåŠ¨ä½œç‰¹å¾çš„ä»£ç†
            # [è§„åˆ™æ•°, å˜é‡æ•°, æœ€å¤§æ·±åº¦, 0, 0, 0]
            # ç½‘ç»œå¯ä»¥å­¦ä¹ ï¼š"æ›´å¤æ‚çš„ç¨‹åºé€šå¸¸æœ‰æ›´å¤§åŠ¨ä½œ"
            return [
                float(num_rules) / 10.0,  # å½’ä¸€åŒ–
                float(num_vars) / 5.0,
                float(max_d) / 5.0,
                0.0,  # å ä½
                0.0,  # å ä½
                0.0   # å ä½
            ]
        except Exception as e:
            # å¤±è´¥æ—¶è¿”å›é›¶ç‰¹å¾
            return [0.0] * 6
    
    def _curriculum_config(self, iter_idx: int) -> Tuple[List[str], List[str]]:
        """æ ¹æ®å½“å‰è¿­ä»£è¿”å›è¯¾ç¨‹å­¦ä¹ é™åˆ¶çš„å˜é‡ä¸ç®—å­é›†åˆã€‚
        é˜¶æ®µåˆ’åˆ† (basic æ¨¡å¼):
          Stage 1 (0%~33%): ä»…ä½ç½®è¯¯å·® pos_err_x/y/z, å…è®¸ '+' '*' (çº¿æ€§/ç¼©æ”¾)ï¼›è¡¨è¾¾å¼æ·±åº¦ç”± MCTS è‡ªæ§ä½†ç®—å­å°‘ã€‚
          Stage 2 (33%~66%): åŠ å…¥é€Ÿåº¦ vel_x/y/z ä¸ç®€å•å‡æ³•ï¼›å…è®¸ '+' '-' '*' '/'.
          Stage 3 (66%~100%): å®Œæ•´ prior_level å¯¹åº”å˜é‡é›†åˆä¸å…¨ç®—å­ã€‚
        è¿”å› (allowed_vars, allowed_ops)ã€‚è‹¥ curriculum_mode=none åˆ™è¿”å›ç©ºè¡¨ç¤ºä¸é™åˆ¶ã€‚
        """
        mode = getattr(self.args, 'curriculum_mode', 'none')
        if mode == 'none':
            return [], []  # ä¸é™åˆ¶
        progress = (iter_idx + 1) / float(self.args.total_iters)
        if progress <= 0.33:
            self._curriculum_stage = 1
            return ['pos_err_x', 'pos_err_y', 'pos_err_z'], ['+', '*']
        elif progress <= 0.66:
            self._curriculum_stage = 2
            return ['pos_err_x', 'pos_err_y', 'pos_err_z', 'vel_x', 'vel_y', 'vel_z'], ['+', '-', '*', '/']
        else:
            self._curriculum_stage = 3
            return [], []  # Stage 3 ä¸åšé¢å¤–è£å‰ªï¼ˆä½¿ç”¨ prior_level å®Œæ•´é›†åˆï¼‰

    def _analyze_program(self, program: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æå–ç¨‹åºä½¿ç”¨çš„å˜é‡ä¸åŸºæœ¬ç»“æ„ä¿¡æ¯"""
        used = set()
        def collect(node):
            # ä½¿ç”¨åŒ…å¼å¯¼å…¥ï¼šä¼˜å…ˆå°è¯•ä» core.dsl å¯¼å…¥ï¼Œè‹¥è„šæœ¬ç›´æ¥è¿è¡Œåˆ™å›é€€æ·»åŠ é¡¹ç›®æ ¹åˆ° sys.path
            try:
                from core.dsl import TerminalNode, UnaryOpNode, BinaryOpNode, IfNode  # type: ignore
            except Exception:
                import sys, pathlib
                # å°†é¡¹ç›®æ ¹åŠ å…¥ sys.pathï¼Œä¿è¯ `from core.dsl import ...` å¯ç”¨
                _parent = pathlib.Path(__file__).resolve().parent.parent
                if str(_parent) not in sys.path:
                    sys.path.insert(0, str(_parent))
                from core.dsl import TerminalNode, UnaryOpNode, BinaryOpNode, IfNode  # type: ignore

            if node is None:
                return
            if isinstance(node, TerminalNode) and isinstance(node.value, str):
                used.add(node.value)
            elif isinstance(node, UnaryOpNode):
                collect(node.child)
            elif isinstance(node, BinaryOpNode):
                collect(node.left); collect(node.right)
            elif isinstance(node, IfNode):
                collect(node.condition); collect(node.then_branch); collect(node.else_branch)
        for rule in program:
            cond = rule.get('condition')
            collect(cond)
            for act in rule.get('action', []):
                try:
                    # 'set' äºŒå…ƒ: left æ˜¯è¾“å‡ºé”®, right æ˜¯è¡¨è¾¾å¼
                    if hasattr(act, 'op') and act.op == 'set' and hasattr(act, 'right'):
                        collect(act.right)
                except Exception:
                    pass
        return {
            'rule_count': len(program),
            'used_variables': sorted(list(used))
        }

    def _program_to_str(self, program: List[Dict[str, Any]], max_rules: int = 3) -> str:
        """å°†ç¨‹åºè½¬æˆå¯è¯»å­—ç¬¦ä¸²ï¼Œä¾¿äºè¿­ä»£æ—¥å¿—æ‰“å°ã€‚
        ä»…æ‰“å°å‰ max_rules æ¡è§„åˆ™ï¼Œé¿å…è¿‡é•¿è¾“å‡ºã€‚
        """
        try:
            from core.dsl import TerminalNode, ConstantNode, UnaryOpNode, BinaryOpNode, IfNode  # type: ignore
        except Exception:
            import sys, pathlib
            _parent = pathlib.Path(__file__).resolve().parent.parent
            if str(_parent) not in sys.path:
                sys.path.insert(0, str(_parent))
            from core.dsl import TerminalNode, ConstantNode, UnaryOpNode, BinaryOpNode, IfNode  # type: ignore

        def ast_str(node):
            if node is None:
                return "None"
            if isinstance(node, BinaryOpNode):
                return f"({ast_str(node.left)} {node.op} {ast_str(node.right)})"
            if isinstance(node, UnaryOpNode):
                return f"{node.op}({ast_str(node.child)})"
            if isinstance(node, IfNode):
                # Conditions are disabled; show then-branch only
                return ast_str(node.then_branch)
            if isinstance(node, ConstantNode):
                name = f"{node.name}=" if node.name else ""
                return f"Const({name}{node.value:.3f})"
            if isinstance(node, TerminalNode):
                return str(node.value)
            if isinstance(node, dict):
                # JSON dict fallback
                ntype = node.get('type', '')
                if ntype == 'BinaryOpNode' or ntype == 'Binary':
                    return f"({ast_str(node.get('left'))} {node.get('op')} {ast_str(node.get('right'))})"
                if ntype == 'UnaryOpNode' or ntype == 'Unary':
                    return f"{node.get('op')}({ast_str(node.get('child'))})"
                if ntype == 'ConstantNode' or ntype == 'Constant':
                    name = node.get('name')
                    name_prefix = f"{name}=" if name else ""
                    return f"Const({name_prefix}{node.get('value')})"
                if ntype == 'TerminalNode' or ntype == 'Terminal':
                    return str(node.get('value'))
            return str(node)

        parts = []
        for i, rule in enumerate(program or []):
            if i >= max_rules:
                parts.append("...")
                break
            cond = rule.get('condition')
            acts = rule.get('action', []) or []
            act_strs = []
            for a in acts:
                if hasattr(a, 'op') and a.op == 'set':
                    lhs = getattr(a.left, 'value', '?') if hasattr(a, 'left') else '?'
                    rhs = ast_str(getattr(a, 'right', None))
                    act_strs.append(f"set {lhs} = {rhs}")
                elif isinstance(a, dict) and a.get('op') == 'set':
                    lhs = a.get('left', {}).get('value', '?')
                    rhs = ast_str(a.get('right'))
                    act_strs.append(f"set {lhs} = {rhs}")
            # Conditions are disabled; only print actions
            parts.append(f"[{'; '.join(act_strs)}]")
        return " | ".join(parts) if parts else "<empty>"

    def _append_program_history(self, iter_idx: int, reward: float, program: List[Dict[str, Any]]):
        path = getattr(self.args, 'program_history_path', None)
        if not path:
            return
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            info = self._analyze_program(program)
            # å¦‚æœè¯„ä¼°å™¨æ”¯æŒç»†ç²’åº¦ç»„ä»¶å¥–åŠ±,ä¸€å¹¶è®°å½•
            components = None
            if hasattr(self.evaluator, 'evaluate_single_with_metrics'):
                try:
                    r_total, comp = self.evaluator.evaluate_single_with_metrics(program)
                    components = comp
                except Exception:
                    components = None
            rec = {
                'iter': iter_idx + 1,
                'reward': reward,
                **info,
                'reward_components': components
            }
            with open(path, 'a') as f:
                f.write(json.dumps(rec, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"[History] è¿½åŠ ç¨‹åºè®°å½•å¤±è´¥: {e}")
    
    def _get_saved_program_reward(self, save_path: str) -> float:
        """
        è¯»å–å·²ä¿å­˜ç¨‹åºæ–‡ä»¶ä¸­çš„å¥–åŠ±å€¼
        
        Returns:
            å·²ä¿å­˜ç¨‹åºçš„å¥–åŠ±å€¼ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥åˆ™è¿”å›è´Ÿæ— ç©·
        """
        if not os.path.exists(save_path):
            return float('-inf')  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»»ä½•æ–°ç¨‹åºéƒ½åº”è¯¥ä¿å­˜
        
        try:
            import json
            with open(save_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å°è¯•ä»metaä¸­è¯»å–reward
            if 'meta' in data and 'reward' in data['meta']:
                saved_reward = float(data['meta']['reward'])
                return saved_reward
            else:
                # æ—§ç‰ˆæœ¬æ–‡ä»¶å¯èƒ½æ²¡æœ‰metaï¼Œè¿”å›è´Ÿæ— ç©·ä»¥å…è®¸ä¿å­˜
                return float('-inf')
        except Exception as e:
            print(f"  âš ï¸  è¯»å–å·²ä¿å­˜ç¨‹åºå¥–åŠ±å¤±è´¥: {e}ï¼Œå°†å…è®¸ä¿å­˜")
            return float('-inf')  # è¯»å–å¤±è´¥ï¼Œå…è®¸ä¿å­˜ä»¥é˜²ä¸‡ä¸€

    def mcts_search(self, root_program: List[Dict[str, Any]], num_simulations: int = 800, iter_idx: int = 0) -> Tuple[List[Any], List[int]]:
        """
        æ‰§è¡ŒMCTSæœç´¢ï¼ˆä½¿ç”¨å½“å‰NNå¼•å¯¼ï¼‰
        
        Returns:
            children: æ‰€æœ‰å­èŠ‚ç‚¹
            visit_counts: è®¿é—®æ¬¡æ•°åˆ†å¸ƒ
        """
        # å˜é‡é›†åˆï¼šæŒ‰ prior_level åˆ†çº§è£å‰ª
        # level 2 (ä¸­åº¦é›¶å…ˆéªŒ): ä¿ç•™ä¸‰è½´åˆ†é‡+å§¿æ€ï¼Œå»æ‰æ¨¡é•¿èšåˆ/ç§¯åˆ†/å¾®åˆ†
        # level 3 (ä¸¥æ ¼é›¶å…ˆéªŒ): ä»…ä¿ç•™ä½ç½®è¯¯å·®ã€é€Ÿåº¦ã€è§’é€Ÿåº¦ä¸‰è½´åŸå§‹åˆ†é‡
        prior_level = getattr(self.args, 'prior_level', 2)
        
        if prior_level == 3:
            # ä¸¥æ ¼é›¶å…ˆéªŒï¼šä»…æœ€åŸå§‹ä¿¡å·
            dsl_variables = [
                'pos_err_x', 'pos_err_y', 'pos_err_z',
                'vel_x', 'vel_y', 'vel_z',
                'ang_vel_x', 'ang_vel_y', 'ang_vel_z'
            ]
        elif prior_level == 2:
            # ä¸­åº¦é›¶å…ˆéªŒ + PIDå®Œæ•´æ”¯æŒï¼šä¿ç•™ä¸‰è½´+å§¿æ€+ç§¯åˆ†é¡¹+å¾®åˆ†é¡¹ï¼ˆç”¨äºåˆæˆPIDæ§åˆ¶å™¨ï¼‰
            dsl_variables = [
                'pos_err_x', 'pos_err_y', 'pos_err_z',
                'vel_x', 'vel_y', 'vel_z',
                'ang_vel_x', 'ang_vel_y', 'ang_vel_z',
                'err_p_roll', 'err_p_pitch', 'err_p_yaw',
                # ç§¯åˆ†é¡¹ï¼ˆPIDçš„Iï¼‰
                'err_i_x', 'err_i_y', 'err_i_z',
                # å¾®åˆ†é¡¹ï¼ˆPIDçš„Dï¼Œå§¿æ€ä¸“ç”¨ï¼‰
                'err_d_roll', 'err_d_pitch', 'err_d_yaw'
            ]
        else:
            # å›é€€åˆ°å…¨ç‰¹å¾ï¼ˆä¸æ¨èï¼Œä»…ç”¨äºè°ƒè¯•ï¼‰
            dsl_variables = [
                'pos_err_x','pos_err_y','pos_err_z','pos_err_xy','pos_err_z_abs',
                'vel_x','vel_y','vel_z','vel_err',
                'ang_vel_x','ang_vel_y','ang_vel_z','ang_vel','ang_vel_mag',
                'err_i_x','err_i_y','err_i_z',
                'err_p_roll','err_p_pitch','err_p_yaw','rpy_err_mag',
                'err_d_x','err_d_y','err_d_z','err_d_roll','err_d_pitch','err_d_yaw'
            ]

        # è¯¾ç¨‹å­¦ä¹ è£å‰ª
        curriculum_vars, curriculum_ops = self._curriculum_config(iter_idx)
        if curriculum_vars:  # éç©ºè¡¨ç¤ºé˜¶æ®µé™åˆ¶å˜é‡é›†åˆ
            dsl_variables = [v for v in dsl_variables if v in curriculum_vars]
        # ç®—å­è£å‰ªï¼šå»æ‰é™¤æ³•ï¼Œä¿ç•™å¸¸ç”¨å®‰å…¨ç®—å­ï¼Œé™ä½æœç´¢çˆ†ç‚¸ä¸æ•°å€¼ä¸ç¨³
        base_ops_full = ['+','-','*','max','min','abs','sqrt','log1p']
        if curriculum_ops:  # é™åˆ¶ç®—å­é›†åˆ
            dsl_operators = [op for op in base_ops_full if op in curriculum_ops]
        else:
            dsl_operators = list(base_ops_full)
        # é»˜è®¤åŠ å…¥æ—¶åº/ç¨³å®šæ€§ä¸€å…ƒåŸè¯­ï¼ˆå‚æ•°åŒ–ä¸ºä¸åŒopåï¼Œä¾¿äºMCTSé€‰æ‹©ï¼‰
        # é‡‡ç”¨è¯¾ç¨‹åŒ–çš„å‚æ•°ç½‘æ ¼ï¼šå…ˆç²—åç»†ï¼Œæé«˜ prior å¬å›
        temporal_ops: List[str] = []
        stage1 = int(getattr(self, '_unary_grid_stage1_iters', 200))
        stage2 = int(getattr(self, '_unary_grid_stage2_iters', 600))
        if iter_idx < stage1:
            ema_list = [0.2]
            k_list = [1]  # delay/diff ä»… k=1ï¼Œé¿å…é•¿æ—¶å»¶å¼•å…¥ä¸ç¨³
            clamp_list = [(-2.0, 2.0)]
            dz_list = [0.05]
            rate_list = [1.0]
            smooth_list = [1.0]
        elif iter_idx < stage2:
            ema_list = [0.1, 0.5]
            k_list = [1]  # ä»é™åˆ¶ k=1ï¼Œå…ˆæ”¶æ•›åå†æ”¾å®½
            clamp_list = [(-1.0, 1.0), (-2.0, 2.0)]
            dz_list = [0.01, 0.1]
            rate_list = [0.5, 2.0]
            smooth_list = [0.5, 2.0]
        else:
            ema_list = [0.1, 0.2, 0.5]
            k_list = [1, 2]  # åæœŸå…è®¸åˆ°2æ­¥
            clamp_list = [(-1.0, 1.0), (-2.0, 2.0), (-5.0, 5.0)]
            dz_list = [0.01, 0.05, 0.1]
            rate_list = [0.5, 1.0, 2.0]
            smooth_list = [0.5, 1.0, 2.0]
        for a in ema_list:
            temporal_ops.append(f'ema:{a}')
        for k in k_list:
            temporal_ops.append(f'delay:{k}')
            temporal_ops.append(f'diff:{k}')
        for lo, hi in clamp_list:
            temporal_ops.append(f'clamp:{lo}:{hi}')
        for eps in dz_list:
            temporal_ops.append(f'deadzone:{eps}')
        for r in rate_list:
            temporal_ops.append(f'rate:{r}')
        for s in smooth_list:
            temporal_ops.append(f'smooth:{s}')
        # åˆå¹¶å»é‡ï¼ˆä¿æŒåŸæœ‰åŸºç¡€ç®—å­ + æ—¶åºåŸè¯­ï¼‰
        dsl_operators = list(dict.fromkeys(dsl_operators + temporal_ops))
        # ğŸ”» ç²¾ç®€ä¸€å…ƒç®—å­ï¼šä¿ç•™ clampã€emaã€diffã€deadzone ä»¥åŠ smooth/delay/rate
        def _is_unary_keep(op:str)->bool:
            base = op.split(':',1)[0]
            return base in ('clamp','ema','diff','deadzone','smooth','delay','rate')
        base_ops = [op for op in dsl_operators if ':' not in op]
        unary_ops = [op for op in dsl_operators if ':' in op and _is_unary_keep(op)]
        dsl_operators = base_ops + unary_ops

        # ğŸ”§ å•è½´æœç´¢åŒ…è£…è¯„ä¼°å‡½æ•°ï¼šåœ¨è¯„ä¼°å‰è‡ªåŠ¨é•œåƒæ‰©å±•ï¼Œè¡¥ä¸Š u_ty/u_tz/u_fz
        # è¿™æ · MCTS æœç´¢åªäº§ç”Ÿ u_tx ç¨‹åºï¼Œä½†è¯„ä¼°æ—¶ä¼šè‡ªåŠ¨è¡¥å…¨å…¶ä»–é€šé“ï¼Œé¿å…æ— æ¨åŠ›å è½
        def _single_axis_eval_wrapper(program):
            """å•è½´æœç´¢è¯„ä¼°åŒ…è£…ï¼šè‡ªåŠ¨é•œåƒæ‰©å±•åè¯„ä¼°"""
            try:
                expanded = self.evaluator._mirror_expand_single_axis_program(program)
            except Exception:
                expanded = program  # å¤±è´¥åˆ™ä½¿ç”¨åŸç¨‹åº
            return self.evaluator.evaluate_single(expanded)

        # åˆ›å»ºMCTS agent
        mcts = MCTS_Agent(
            evaluation_function=_single_axis_eval_wrapper,  # ä½¿ç”¨åŒ…è£…å‡½æ•°ï¼Œè‡ªåŠ¨é•œåƒæ‰©å±•
            dsl_variables=dsl_variables,
            dsl_constants=[0.0, 0.05, 0.1, 0.3, 0.5, 1.0, 2.0, 3.0, 5.0, 8.0],  # æ‰©å¤§å¸¸æ•°èŒƒå›´ä»¥äº§ç”Ÿæ›´å¤§åŠ¨ä½œå¹…åº¦
            dsl_operators=dsl_operators,
            exploration_weight=self._exploration_weight,
            max_depth=self._max_depth,
            structure_prior_weight=self.structure_prior_weight,
            stability_prior_weight=self.stability_prior_weight
        )
        # å•è½´æœç´¢ï¼šé»˜è®¤ä»… Roll é€šé“ï¼ˆå¯æ”¹ä¸º ['u_ty'] / ['u_tz'] / ['u_fz']ï¼‰
        mcts._active_channels = ['u_tx']
        
        # ğŸš€ æ‚¬åœæ¨åŠ›çº¦æŸé…ç½®
        mcts._enforce_hover_thrust = getattr(self, '_enforce_hover_thrust', True)
        mcts._hover_thrust_value = getattr(self, '_hover_thrust_value', 0.265)
        mcts._hover_thrust_min = getattr(self, '_hover_thrust_min', 0.20)
        mcts._hover_thrust_max = getattr(self, '_hover_thrust_max', 0.35)
        mcts._hover_delta_max = getattr(self, '_hover_delta_max', 2.0)
        
        # ğŸŒŸ è®¾ç½® ranking ç½‘ç»œç”¨äº MCTS biasï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
        if self.enable_ranking_mcts_bias and self.use_ranking and hasattr(self, 'ranking_net') and self.ranking_net is not None:
            mcts.ranking_net = self.ranking_net
            mcts.gnn_encoder = self.nn_model
            mcts.ranking_bias_beta = self.ranking_bias_beta
            mcts.ranking_device = self.device
        else:
            # ç¡®ä¿å…³é—­æ—¶ ranking_net ä¸º None
            mcts.ranking_net = None
        
        # è®¾ç½®rootï¼ˆAST å†…éƒ¨è¡¨ç¤ºï¼‰
        root_ast = to_ast_program(root_program)
        root = MCTSNode(root_ast, parent=None, depth=0)
        mcts.root = root
        
        # ğŸ”§ ä¼˜åŒ–1: GNNå…ˆéªŒç¼“å­˜ (é¿å…é‡å¤æ¨ç†) - é™åˆ¶å¤§å°é˜²æ­¢å†…å­˜æ³„æ¼
        # ä¹‹å‰å®ç°: æ¯æ¬¡ mcts_search éƒ½é‡æ–°åˆ›å»ºå±€éƒ¨ç¼“å­˜ â†’ å¯¼è‡´è·¨è¿­ä»£å‘½ä¸­ç‡å§‹ç»ˆä¸º 0%
        # æ”¹è¿›: ä½¿ç”¨è®­ç»ƒå™¨çº§åˆ«çš„æŒä¹… LRU ç¼“å­˜ (self._global_prior_cache) åœ¨æ‰€æœ‰è¿­ä»£ä¹‹é—´å¤ç”¨
        from collections import OrderedDict
        if not hasattr(self, '_global_prior_cache'):
            self._global_prior_cache = OrderedDict()  # é¦–æ¬¡åˆ›å»º
        gnn_prior_cache = self._global_prior_cache      # å¼•ç”¨åŒä¸€å¯¹è±¡
        MAX_CACHE_SIZE = 5000  # æ‰©å¤§ä¸Šé™ï¼Œè·¨è¿­ä»£å¯ç§¯ç´¯æ›´å¤šç»“æ„
        
        # æ³¨æ„ï¼šget_program_hash ç°åœ¨æ˜¯é¡¶å±‚å‡½æ•°ï¼ˆåœ¨æ–‡ä»¶å¼€å¤´å®šä¹‰ï¼‰ï¼Œå¯ä»¥è¢«å…¶ä»–æ¨¡å—å¯¼å…¥
        
        def add_to_cache(prog_hash, value):
            """æ·»åŠ /æ›´æ–°ç¼“å­˜ï¼ˆLRUï¼‰ã€‚è¶…è¿‡MAX_CACHE_SIZEæ—¶æ‰¹é‡æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„20%ã€‚"""
            # å¦‚æœå·²å­˜åœ¨åˆ™æ›´æ–°å¹¶ç§»åŠ¨åˆ°å°¾éƒ¨
            gnn_prior_cache[prog_hash] = value
            try:
                gnn_prior_cache.move_to_end(prog_hash, last=True)
            except Exception:
                pass
            # LRU æ¸…ç†ï¼šè¶…è¿‡é™åˆ¶æ—¶åˆ é™¤æœ€æ—§çš„20%
            if len(gnn_prior_cache) > MAX_CACHE_SIZE:
                remove_count = max(1, int(MAX_CACHE_SIZE * 0.2))
                for _ in range(remove_count):
                    try:
                        gnn_prior_cache.popitem(last=False)
                    except Exception:
                        break
        
        # ğŸ”§ ä¼˜åŒ–2: æ‰¹é‡GNNæ¨ç†ç¼“å†²åŒº
        pending_gnn_nodes = []  # æ”¶é›†éœ€è¦GNNæ¨ç†çš„æ–°èŠ‚ç‚¹
        
        # ğŸ”§ æ‰¹é‡è¯„ä¼°ä¼˜åŒ–ï¼šæ”¶é›†å¾…è¯„ä¼°çš„leaf nodes
        pending_evals = []  # [(leaf, path, use_real_sim)]
        
        # æ‰§è¡ŒMCTSæ¨¡æ‹Ÿï¼ˆåªåšæ ‘æ‰©å±•ï¼Œå»¶è¿ŸGNNæ¨ç†ï¼‰
        # Root Dirichlet å™ªå£°ä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆé’ˆå¯¹ç¼–è¾‘ç±»å‹å…ˆéªŒï¼‰ï¼Œä»…ç¬¬ä¸€è½®é€‰æ‹©é˜¶æ®µä½¿ç”¨
        root_dirichlet_noise = None
        if self._root_dirichlet_eps > 0.0:
            try:
                import numpy as _np
                alpha = float(self._root_dirichlet_alpha)
                noise = _np.random.gamma(alpha, 1.0, size=len(EDIT_TYPES))
                noise = noise / max(1e-12, noise.sum())
                root_dirichlet_noise = noise
            except Exception:
                root_dirichlet_noise = None

        # æ ¹èŠ‚ç‚¹è‡ªé€‚åº”æœ€å°åˆ†æ”¯æ•°ï¼šæ ¹æ®NNå…ˆéªŒçš„ç´¯è®¡è¦†ç›–ç‡ç¡®å®šKï¼ˆè€Œéå›ºå®šå¸¸æ•°ï¼‰
        # é€»è¾‘ï¼šå–æœ€å°Kï¼Œä½¿å¾—æŒ‰é™åºæ’åºçš„å…ˆéªŒæ¦‚ç‡ç´¯è®¡å’Œ â‰¥ tauï¼ŒK ä½œä¸ºæ ¹èŠ‚ç‚¹ progressive widening çš„ä¸‹é™
        root_min_cap_k = 2
        try:
            with torch.no_grad():
                root_graph = ast_to_pyg_graph(root.program)
                from torch_geometric.data import Batch as _PyGBatch
                _g = _PyGBatch.from_data_list([root_graph]).to(self.device)
                _logits, _, _ = self.nn_model(_g)
                _probs = F.softmax(_logits.squeeze(0), dim=-1).detach().cpu().numpy()
                # ä¸ Dirichlet ä¸€è‡´ï¼šè‹¥é…ç½®å¯ç”¨ï¼ŒæŒ‰ç›¸åŒ eps æ··å…¥å™ªå£°ï¼ˆé’ˆå¯¹ç¼–è¾‘ç±»å‹ï¼‰
                if root_dirichlet_noise is not None and self._root_dirichlet_eps > 0.0:
                    _probs = (1.0 - float(self._root_dirichlet_eps)) * _probs + float(self._root_dirichlet_eps) * root_dirichlet_noise
                _probs = _probs.clip(1e-12, 1.0)
                order = _probs.argsort()[::-1]
                tau = getattr(self, '_root_prior_coverage_tau', 0.80)
                csum = 0.0
                k = 0
                for idx in order:
                    csum += float(_probs[idx])
                    k += 1
                    if csum >= tau:
                        break
                # âœ… ä¿®å¤2: è®¾ç½®min_capç¡¬ä¸‹é™, é˜²æ­¢NNè¿‡åº¦è‡ªä¿¡å¯¼è‡´æ¢ç´¢å´©æºƒ
                root_min_cap_k = max(5, min(k, len(EDIT_TYPES)))
        except Exception:
            root_min_cap_k = 5  # å¼‚å¸¸æ—¶ä¹Ÿä¿è¯æœ€å°æ¢ç´¢å®½åº¦

        # ğŸš€ Leaf Parallelization: åˆ†æ‰¹è¯„ä¼°å¶èŠ‚ç‚¹
        leaf_batch_size = getattr(self.args, 'mcts_leaf_batch_size', 128)
        num_batches = (num_simulations + leaf_batch_size - 1) // leaf_batch_size
        
        if num_simulations > 0:
            print(f"[LeafParallel] MCTS simulations={num_simulations}, batch_size={leaf_batch_size}, num_batches={num_batches}")
        
        for batch_idx in range(num_batches):
            batch_start = batch_idx * leaf_batch_size
            batch_end = min((batch_idx + 1) * leaf_batch_size, num_simulations)
            batch_pending_evals = []  # å½“å‰æ‰¹æ¬¡çš„å¾…è¯„ä¼°èŠ‚ç‚¹
            
            for sim_idx in range(batch_start, batch_end):
                # Selection + Expansionï¼ˆä½¿ç”¨NNå…ˆéªŒ + Progressive Wideningï¼‰
                node = root
                path = [node]
                
                if sim_idx == 0 or sim_idx == num_simulations - 1:  # DEBUG: é¦–æ¬¡å’Œæœ€åä¸€æ¬¡æ¨¡æ‹Ÿ
                    print(f"[PW-DEBUG] sim={sim_idx}, root.visits={root.visits}, root.children={len(root.children)}")
                
                # Selectioné˜¶æ®µ (è€ƒè™‘Progressive Widening)
                while node.children:
                    # Progressive Wideningæ£€æŸ¥ï¼šæ˜¯å¦å¯ä»¥æ‰©å±•æ›´å¤šchildren
                    pw_c = 1.5
                    pw_alpha = 0.6
                    # Progressive Wideningåˆå§‹è¡Œä¸ºä¿®æ­£ï¼š
                    # - ä»¥ (visits+1) è®¡ç®—ï¼Œé¿å… visits==0 æ—¶ä¸Šé™ä¸º0
                    # - æ ¹èŠ‚ç‚¹çš„æœ€å°åˆ†æ”¯æ•°ç”± NN å…ˆéªŒç´¯è®¡è¦†ç›–ç‡è‡ªé€‚åº”ç¡®å®šï¼ˆä¸ä½¿ç”¨å›ºå®šå¸¸æ•°ï¼‰
                    vis = max(0, int(node.visits))
                    num_mutations = len(node.untried_mutations) if hasattr(node, 'untried_mutations') else 0

                    if self.disable_progressive_widening:
                        # å®Œå…¨æ”¾å¼€ï¼šå…è®¸ä¸€æ¬¡æ€§æ‰©å±•æ‰€æœ‰å¯èƒ½å˜å¼‚
                        max_children = num_mutations
                    else:
                        base_cap = int(pw_c * ((vis + 1) ** pw_alpha))
                        min_cap = (root_min_cap_k if node.depth == 0 else 1)
                        max_children = max(min_cap, base_cap)

                    can_expand = len(node.children) < max_children and len(node.children) < num_mutations
                    
                    if sim_idx == 0 and node.depth == 0:  # DEBUG: åªåœ¨ç¬¬ä¸€æ¬¡æ¨¡æ‹Ÿæ‰“å°rootä¿¡æ¯
                        print(f"[PW-DEBUG] sim_idx={sim_idx}, depth={node.depth}, visits={node.visits}, max_children={max_children}, len(children)={len(node.children)}, num_mutations={num_mutations}, can_expand={can_expand}")
                    
                    if can_expand:
                        # å¯ä»¥æ‰©å±•æ›´å¤šchildrenï¼Œåœæ­¢selection
                        break
                    
                    if node.is_fully_expanded():
                        # å®Œå…¨æ‰©å±•ï¼Œåœæ­¢selection
                        break
                    
                    # ç»§ç»­å‘ä¸‹é€‰æ‹©
                    node = self._select_child_puct(node, root_dirichlet_noise if node.depth == 0 else None)
                    path.append(node)
                
                # Expansioné˜¶æ®µ
                if not node.is_fully_expanded():
                    # ç”Ÿæˆæ–°å­èŠ‚ç‚¹ï¼Œåˆ†é…NNå…ˆéªŒ
                    mcts._ensure_mutations(node)
                    
                    if node.untried_mutations and len(node.expanded_actions) < len(node.untried_mutations):
                        # é€‰æ‹©ä¸€ä¸ªæœªæ‰©å±•çš„å˜å¼‚
                        unexpanded_idx = [i for i in range(len(node.untried_mutations)) 
                                         if i not in node.expanded_actions][0]
                        mutation = node.untried_mutations[unexpanded_idx]
                        
                        # å…‹éš†ç¨‹åºå¹¶åº”ç”¨å˜å¼‚
                        child_program = [mcts._clone_rule(r) for r in node.program]
                        mcts._apply_mutation(child_program, mutation)
                        # å˜å¼‚åä¹Ÿè½¬æ¢ä¸ºASTï¼Œç¡®ä¿å†…éƒ¨ä¸€è‡´
                        child_program = to_ast_program(child_program)
                        
                        # â”€â”€ è°ƒè¯•ï¼šæ‰“å°è¢«æ‰©å±•çš„ç¨‹åºæ‘˜è¦ï¼ˆä»…æ ¹ä¸å…¶ä¸‹ä¸€å±‚ï¼Œé™æ•°é‡ï¼‰â”€â”€
                        try:
                            if getattr(self.args, 'debug_programs', False) and (node.depth <= 1):
                                if not hasattr(self, '_debug_prog_count'):
                                    self._debug_prog_count = 0
                                limit = int(getattr(self.args, 'debug_programs_limit', 20))
                                if self._debug_prog_count < limit:
                                    def _summarize_rule(rule):
                                        try:
                                            if isinstance(rule, dict):
                                                if 'op' in rule:
                                                    op = rule.get('op')
                                                    var = rule.get('var')
                                                    expr = rule.get('expr')
                                                    cond = rule.get('condition')
                                                    expr_type = (expr or {}).get('type') if isinstance(expr, dict) else type(expr).__name__
                                                    has_cond = cond not in (None, False)
                                                    return f"{op}:{var}|{expr_type}|cond={has_cond}"
                                                if 'set' in rule:
                                                    s = rule.get('set')
                                                    if isinstance(s, (list, tuple)) and len(s) >= 2:
                                                        return f"set:{s[0]}|const|cond=False"
                                                    return "set:?"
                                                if 'if' in rule:
                                                    return "if:..."
                                            return str(type(rule).__name__)
                                        except Exception:
                                            return "<err>"
                                    sets = []
                                    uses_u = False
                                    has_if = False
                                    # ç»Ÿè®¡ AST 'set' äºŒå…ƒæ“ä½œ
                                    for rr in child_program:
                                        try:
                                            if isinstance(rr, dict):
                                                cond = rr.get('condition')
                                                if hasattr(cond, 'op') and getattr(cond, 'op', None) in ('if',):
                                                    has_if = True
                                                for act in rr.get('action', []) or []:
                                                    if hasattr(act, 'op') and act.op == 'set' and hasattr(act, 'left') and hasattr(act.left, 'value'):
                                                        var = str(getattr(act.left, 'value', ''))
                                                        sets.append(var)
                                                        if var.startswith('u_'):
                                                            uses_u = True
                                        except Exception:
                                            pass
                                    digest = ", ".join(_summarize_rule(r) for r in child_program[:6])
                                    print(f"[Prog] depth={node.depth+1} rules={len(child_program)} u_sets={sets} uses_u={uses_u} :: {digest}")
                                    self._debug_prog_count += 1
                        except Exception:
                            pass

                        # åˆ›å»ºå­èŠ‚ç‚¹
                        child = MCTSNode(child_program, parent=node, depth=node.depth + 1)
                        edit_type = mutation[0]
                        child._edit_type = edit_type
                        
                        # ğŸš€ ä¼˜åŒ–: æ£€æŸ¥å…ˆéªŒç¼“å­˜ï¼ˆä¸ç¼“å­˜valueï¼‰
                        prog_hash = get_program_hash(child_program)
                        if prog_hash in gnn_prior_cache:
                            # å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨å…ˆéªŒ + LRU: ç§»åŠ¨åˆ°é˜Ÿå°¾
                            child._prior_p = gnn_prior_cache[prog_hash]
                            try:
                                gnn_prior_cache.move_to_end(prog_hash, last=True)
                            except Exception:
                                pass
                            # ç»Ÿè®¡ï¼šå…ˆéªŒï¼ˆchild æ‰©å±•é˜¶æ®µï¼‰ç¼“å­˜å‘½ä¸­
                            if hasattr(self, '_mcts_stats'):
                                self._mcts_stats['prior_cached'] = self._mcts_stats.get('prior_cached', 0) + 1
                            # å¯é€‰è°ƒè¯•: ä»…å‰è‹¥å¹²æ¬¡å‘½ä¸­æ‰“å°ï¼ˆé¿å…åˆ·å±ï¼‰
                            if getattr(self, '_debug_prior_hit_printed', 0) < 10:
                                try:
                                    print(f"[PriorCacheHit] depth={child.depth} hash={prog_hash[:10]} prior_p={child._prior_p:.4f}")
                                except Exception:
                                    pass
                                self._debug_prior_hit_printed = getattr(self, '_debug_prior_hit_printed', 0) + 1
                        else:
                            # æœªå‘½ä¸­ï¼ŒåŠ å…¥æ‰¹é‡æ¨ç†é˜Ÿåˆ—
                            child._prior_p = 1.0 / len(EDIT_TYPES)  # é»˜è®¤å…ˆéªŒ
                            child._prog_hash = prog_hash
                            pending_gnn_nodes.append((child, edit_type))
                        
                        node.children.append(child)
                        node.expanded_actions.add(unexpanded_idx)
                        path.append(child)
                
                # ğŸ”§ æ”¶é›†leafå¾…æ‰¹é‡è¯„ä¼°ï¼ˆä¸ç«‹å³è¯„ä¼°ï¼Œå…¨éƒ¨ä½¿ç”¨çœŸå®ä»¿çœŸï¼‰
                leaf = path[-1]
                batch_pending_evals.append((leaf, path.copy()))  # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¿…é¡»ä½¿ç”¨pathçš„å‰¯æœ¬ï¼
                pending_evals.append((leaf, path.copy()))  # ä¹Ÿä¿ç•™åœ¨å…¨å±€åˆ—è¡¨ä¸­ï¼ˆç”¨äºGNNæ¨ç†ï¼‰
                
                # âœ… ä¿®å¤1: ç«‹å³æ›´æ–°visits (åœ¨æ¨¡æ‹Ÿå¾ªç¯å†…, ä¿è¯PWæ­£ç¡®è®¡ç®—)
                for node in reversed(path):
                    node.visits += 1
            
            # ğŸš€ æ‰¹é‡è¯„ä¼°å½“å‰æ‰¹æ¬¡çš„å¶èŠ‚ç‚¹
            if batch_pending_evals:
                invalid_reasons = {}
                valid_programs: List[List[Dict[str, Any]]] = []
                valid_refs: List[Tuple[MCTSNode, List[MCTSNode]]] = []
                for idx, (leaf, path) in enumerate(batch_pending_evals):
                    program = leaf.program
                    ok, reason = validate_program(program)
                    if ok:
                        valid_programs.append(program)
                        valid_refs.append((leaf, path))
                    else:
                        invalid_reasons[idx] = reason or "violates hard constraint"

                rewards_valid: List[float] = []
                if valid_programs:
                    rewards_valid = self.evaluator.evaluate_batch(valid_programs)

                valid_iter = iter(rewards_valid)
                for idx, (leaf, path) in enumerate(batch_pending_evals):
                    if idx in invalid_reasons:
                        reason = invalid_reasons[idx]
                        print(f"[HardConstraint] Reject program before sim: {reason}")
                        reward = float(HARD_CONSTRAINT_PENALTY)
                    else:
                        reward = float(next(valid_iter))
                    for node in reversed(path):
                        # visitså·²åœ¨æ¨¡æ‹Ÿå¾ªç¯å†…æ›´æ–°, è¿™é‡Œåªæ›´æ–°value_sum
                        node.value_sum += reward
        
        # ğŸš€ æ‰¹é‡GNNæ¨ç†é˜¶æ®µ (ä¸€æ¬¡æ¨ç†æ‰€æœ‰æ–°èŠ‚ç‚¹ï¼Œä»…è·å–å…ˆéªŒ)
        if pending_gnn_nodes:
            try:
                with torch.no_grad():
                    # æ‰¹é‡æ„å»ºå›¾ (ä»…GNNè·¯å¾„)
                    graphs = [ast_to_pyg_graph(child.program) for child, _ in pending_gnn_nodes]
                    from torch_geometric.data import Batch
                    batch_graph = Batch.from_data_list(graphs).to(self.device)
                    policy_logits, _, _ = self.nn_model(batch_graph)  # ä»…ä½¿ç”¨policyè¾“å‡º
                    
                    # åˆ†é…å…ˆéªŒå¹¶ç¼“å­˜
                    policy_probs = F.softmax(policy_logits, dim=-1)
                    for idx, (child, edit_type) in enumerate(pending_gnn_nodes):
                        if edit_type in EDIT_TYPES:
                            type_idx = EDIT_TYPES.index(edit_type)
                            prior_p = policy_probs[idx, type_idx].item()
                        else:
                            prior_p = 1.0 / len(EDIT_TYPES)
                        
                        child._prior_p = float(prior_p)  # è½¬ä¸ºPythonåŸç”Ÿç±»å‹é¿å…å¼ é‡å¼•ç”¨
                        
                        # æ›´æ–°ç¼“å­˜ - LRUæ·˜æ±°ç­–ç•¥ï¼ˆä»…ç¼“å­˜å…ˆéªŒï¼‰
                        if hasattr(child, '_prog_hash'):
                            add_to_cache(child._prog_hash, float(prior_p))
            except Exception as e:
                # æ‰¹é‡æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                for child, _ in pending_gnn_nodes:
                    child._prior_p = 1.0 / len(EDIT_TYPES)
        
        # ğŸ“Š æ€§èƒ½ç»Ÿè®¡ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        if hasattr(self, '_mcts_stats'):
            # ç»Ÿè®¡ï¼šå…ˆéªŒGNNè°ƒç”¨æ•°é‡ï¼ˆchild æ‰©å±•é˜¶æ®µï¼‰
            self._mcts_stats['prior_gnn_nodes'] = self._mcts_stats.get('prior_gnn_nodes', 0) + len(pending_gnn_nodes)
            # ç»Ÿè®¡ï¼šå½“å‰ç¼“å­˜å¤§å°
            self._mcts_stats['cache_size'] = len(gnn_prior_cache)
        
        # è¿”å›rootçš„å­èŠ‚ç‚¹å’Œè®¿é—®åˆ†å¸ƒ
        if root.children:
            visit_counts = [child.visits for child in root.children]
            result_children = root.children
            result_visits = visit_counts
            
            # ğŸ§¹ MCTSå†…å­˜æ¸…ç†ï¼šé€’å½’æ¸…é™¤æ‰€æœ‰èŠ‚ç‚¹çš„å¼•ç”¨,é˜²æ­¢å†…å­˜æ³„æ¼
            # âš ï¸  æ³¨æ„ï¼šåªæ¸…ç†æ·±å±‚å­æ ‘(depth>=2),ä¿æŠ¤rootçš„ç›´æ¥children
            def cleanup_tree(node, preserve_depth=1):
                if node is None:
                    return
                # å¦‚æœæ˜¯éœ€è¦ä¿æŠ¤çš„æ·±åº¦,ä¸æ¸…ç†
                if node.depth < preserve_depth:
                    return
                # é€’å½’æ¸…ç†å­èŠ‚ç‚¹
                for child in node.children:
                    cleanup_tree(child, preserve_depth)
                # æ¸…é™¤å¼•ç”¨(åªæ¸…ç†æ·±å±‚èŠ‚ç‚¹)
                if node.depth >= preserve_depth:
                    node.children = []
                    node.parent = None
                    # æ¸…é™¤ç¼“å­˜çš„value
                    if hasattr(node, '_cached_value'):
                        delattr(node, '_cached_value')
            
            # ä¿å­˜éœ€è¦è¿”å›çš„æ•°æ®å,æ¸…ç†æ·±å±‚å­æ ‘(depth>=2)
            # root(depth=0)å’Œå®ƒçš„ç›´æ¥children(depth=1)éƒ½ä¿ç•™
            for child in root.children:
                cleanup_tree(child, preserve_depth=2)  # åªæ¸…ç†depth>=2çš„èŠ‚ç‚¹
            
            return result_children, result_visits
        else:
            return [], []
    
    def _select_child_puct(self, node: MCTSNode, root_noise: Optional['np.ndarray']=None) -> MCTSNode:
        """PUCTé€‰æ‹©ï¼ˆä½¿ç”¨NNå…ˆéªŒï¼‰"""
        if not node.children:
            return node
        
        best_score = -float('inf')
        best_child = None
        
        sqrt_n = np.sqrt(node.visits)
        c_puct = self._puct_c
        
        for idx, child in enumerate(node.children):
            q = child.value_sum / child.visits if child.visits > 0 else 0.0
            prior = getattr(child, '_prior_p', 1.0 / len(node.children))
            # æ ¹èŠ‚ç‚¹æ··å…¥ Dirichlet å™ªå£°ï¼ˆå°†ç¼–è¾‘ç±»å‹æ˜ å°„åˆ° children é¡ºåºçš„å¹³å‡ï¼‰
            if root_noise is not None and node.depth == 0:
                # è‹¥ child æœ‰ç¼–è¾‘ç±»å‹ï¼Œå°†å™ªå£°æ˜ å°„åˆ°å¯¹åº” EDIT_TYPE ç´¢å¼•
                et = getattr(child, '_edit_type', None)
                if et in EDIT_TYPES:
                    et_idx = EDIT_TYPES.index(et)
                    prior = (1.0 - self._root_dirichlet_eps) * prior + self._root_dirichlet_eps * float(root_noise[et_idx])
            u = c_puct * prior * sqrt_n / (1 + child.visits)
            
            score = q + u
            
            if score > best_score:
                best_score = score
                best_child = child
        
        return best_child if best_child else node.children[0]
    
    def train_step(self):
        """å•æ­¥è®­ç»ƒï¼ˆAlphaZeroé£æ ¼ï¼šä»ç¬¬ä¸€ä¸ªæ ·æœ¬å°±å¼€å§‹å­¦ä¹ ï¼‰"""
        # ç©ºbufferæ— æ³•è®­ç»ƒ
        if len(self.replay_buffer) == 0:
            return None
        
        # é‡‡æ ·batchï¼ˆä½¿ç”¨å®é™…bufferå¤§å°å’Œbatch_sizeçš„è¾ƒå°å€¼ï¼‰
        actual_batch_size = min(self.args.batch_size, len(self.replay_buffer))
        batch = self.replay_buffer.sample(actual_batch_size)
        
        # æ„å»ºtensorï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
        # ä»…ä¿ç•™ GNN æ¨¡å¼
        graph_list = [s['graph'] for s in batch]
        batch_graph = PyGBatch.from_data_list(graph_list).to(self.device)
        policy_targets = torch.stack([s['policy_target'] for s in batch]).to(self.device)
        
        # å‰å‘ä¼ æ’­ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨valueå¤´
        policy_logits, value_scalar, value_components = self.nn_model(batch_graph)
        
        # ===== è¯Šæ–­ï¼šç­–ç•¥ç›®æ ‡çš„è´¨é‡ä¸åˆ†å¸ƒ =====
        with torch.no_grad():
            # æ¯ä¸ªæ ·æœ¬ç›®æ ‡å’Œï¼ˆåº”â‰ˆ1ï¼‰
            pt_sums = policy_targets.sum(dim=-1)
            # éé›¶é¡¹ä¸ªæ•°
            pt_nz = (policy_targets > 1e-8).sum(dim=-1).float()
            # ç›®æ ‡ç†µï¼ˆè¶Šå¤§è¶Šåˆ†æ•£ï¼Œæœ€å¤§å€¼çº¦ä¸º log(len(EDIT_TYPES))ï¼‰
            pt_entropy = (-(policy_targets.clamp(min=1e-12) * policy_targets.clamp(min=1e-12).log()).sum(dim=-1))
            # å¼‚å¸¸ä¾¦æµ‹ï¼šè‹¥å­˜åœ¨å’Œä¸º0æˆ–NaNï¼Œè®°å½•æ ‡è®°
            any_zero_sum = bool((pt_sums <= 1e-8).any().item())
            any_nan_sum = bool(torch.isnan(pt_sums).any().item())
            # é¢„æµ‹ä¾§è¯Šæ–­ï¼šæ­£ç¡®ç±»æ¦‚ç‡ä¸Top-1å‡†ç¡®ç‡
            pred_probs = F.softmax(policy_logits, dim=-1)
            tgt_idx = torch.argmax(policy_targets, dim=-1)
            batch_indices = torch.arange(pred_probs.size(0), device=pred_probs.device)
            correct_prob = pred_probs[batch_indices, tgt_idx]
            pred_top1 = torch.argmax(pred_probs, dim=-1)
            top1_acc = (pred_top1 == tgt_idx).float()
        # æŸå¤±è®¡ç®—
        # ç­–ç•¥æŸå¤±ï¼šäº¤å‰ç†µï¼ˆMCTSè®¿é—®åˆ†å¸ƒä½œä¸ºç›®æ ‡ï¼‰
        policy_loss = -(policy_targets * F.log_softmax(policy_logits, dim=-1)).sum(dim=-1).mean()
        # é¢„æµ‹åˆ†å¸ƒç†µï¼šé¼“åŠ±éé›¶ç†µï¼Œé¿å…æ—©æœŸå¡Œç¼©
        policy_probs = F.softmax(policy_logits, dim=-1)
        policy_entropy = (-(policy_probs.clamp(min=1e-12) * policy_probs.clamp(min=1e-12).log()).sum(dim=-1)).mean()
        _ENTROPY_COEFF = 0.01  # å›ºå®šç³»æ•°ï¼ŒNNå†…éƒ¨æ­£åˆ™ï¼Œä¸æš´éœ²ä¸ºå¤–å‚
        
        # Value head æŸå¤±ï¼ˆæ¡ä»¶å¯ç”¨ï¼‰
        value_loss = torch.tensor(0.0, device=self.device)
        if self.enable_value_head:
            # æå–çœŸå®å¥–åŠ±ä½œä¸ºvalue target
            reward_targets = torch.tensor([s['reward_true'] for s in batch], device=self.device, dtype=torch.float32)
            # å½’ä¸€åŒ–åˆ°[-1, 1]ï¼ˆå‡è®¾å¥–åŠ±èŒƒå›´åœ¨[-10, 0]ä¹‹é—´ï¼‰
            reward_targets_norm = torch.tanh(reward_targets / 5.0)
            # MSE loss for value scalar
            value_loss = F.mse_loss(value_scalar, reward_targets_norm)
        
        # æ€»æŸå¤±ï¼ˆç­–ç•¥ + ç†µæ­£åˆ™ + valueï¼‰
        total_loss = policy_loss - _ENTROPY_COEFF * policy_entropy
        if self.enable_value_head:
            total_loss = total_loss + value_loss
        
        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåå‘ä¼ æ’­å‰æ¸…ç†CUDAç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(self.nn_model.parameters(), 1.0)
        self.optimizer.step()
        
        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šè®­ç»ƒæ­¥åç«‹å³æ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # è½»é‡å‚æ•°æ›´æ–°ç›‘æ§ï¼šæ¯”è¾ƒè®­ç»ƒå‰åå‚æ•°æ ¡éªŒå’Œå˜åŒ–ç™¾åˆ†æ¯”
        new_checksum = self._compute_param_checksum()
        delta = new_checksum - self._last_param_checksum
        rel = (delta / (abs(self._last_param_checksum) + 1e-9)) * 100.0
        # è‹¥å˜åŒ–æå°ï¼ˆ<0.001%ï¼‰ï¼Œæ ‡è®°æç¤ºï¼›ä»…ç¬¬ä¸€æ­¥æˆ–æ¯è‹¥å¹²æ­¥è¾“å‡ºä¸€æ¬¡ç”±å¤–å±‚è°ƒç”¨æ§åˆ¶ï¼Œè¿™é‡Œè¿”å›æŒ‡æ ‡
        changed_flag = rel >= 0.001
        self._last_param_checksum = new_checksum

        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item() if self.enable_value_head else 0.0,
            'total_loss': total_loss.item(),
            'grad_norm': float(getattr(grad_norm, 'item', lambda: grad_norm)() if hasattr(grad_norm, 'item') else float(grad_norm)),
            'param_delta': float(delta),
            'param_delta_pct': float(rel),
            'param_changed': bool(changed_flag),
            # è¯Šæ–­æŒ‡æ ‡ï¼ˆä»…ç”¨äºæ‰“å°ä¸å®šä½é—®é¢˜ï¼‰
            'pt_sum_min': float(pt_sums.min().item()),
            'pt_sum_max': float(pt_sums.max().item()),
            'pt_sum_mean': float(pt_sums.mean().item()),
            'pt_nz_mean': float(pt_nz.mean().item()),
            'pt_entropy_mean': float(pt_entropy.mean().item()),
            'pt_any_zero_sum': any_zero_sum,
            'pt_any_nan_sum': any_nan_sum,
            'pred_correct_prob_mean': float(correct_prob.mean().item()),
            'pred_top1_acc': float(top1_acc.mean().item()),
            'policy_entropy': float(policy_entropy.item()),
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'='*80}")
        print(f"å¼€å§‹åœ¨çº¿è®­ç»ƒ - AlphaZeroå¼ç¨‹åºåˆæˆ")
        print(f"{'='*80}")
        print(f"æ€»è¿­ä»£æ•°: {self.args.total_iters}")
        print(f"MCTSæ¨¡æ‹Ÿæ•°/è¿­ä»£: {self.args.mcts_simulations}")
        print(f"NNæ›´æ–°é¢‘ç‡: æ¯{self.args.update_freq}æ¬¡è¿­ä»£")
        print(f"æ‰¹é‡å¤§å°: {self.args.batch_size}")
        print(f"ğŸš€ GNNç»“æ„ç¼“å­˜: å·²å¯ç”¨ï¼ˆå¿½ç•¥å¸¸æ•°å€¼ï¼ŒBOè°ƒå‚æ—¶å¤ç”¨ç»“æ„å…ˆéªŒï¼‰")
        
        # é›¶åŠ¨ä½œæƒ©ç½šè¯¾ç¨‹åŒ–é…ç½®
        zero_action_penalty_init = float(getattr(self.args, 'zero_action_penalty', 0.0))
        zero_action_penalty_decay = float(getattr(self.args, 'zero_action_penalty_decay', 1.0))
        zero_action_penalty_min = float(getattr(self.args, 'zero_action_penalty_min', 0.1))
        current_zero_penalty = zero_action_penalty_init
        
        if zero_action_penalty_init > 0 and zero_action_penalty_decay < 1.0:
            print(f"é›¶åŠ¨ä½œæƒ©ç½šè¯¾ç¨‹åŒ–: åˆå§‹={zero_action_penalty_init:.2f}, è¡°å‡={zero_action_penalty_decay:.3f}/è½®, ä¸‹é™={zero_action_penalty_min:.2f}")
        print(f"{'='*80}\n")
        
        # åˆå§‹åŒ–ç¨‹åºï¼ˆæ”¯æŒä»æ–‡ä»¶åŠ è½½ï¼‰
        if hasattr(self.args, 'warm_start') and self.args.warm_start:
            loaded_program = self._load_program_from_json(self.args.warm_start)
            if loaded_program:
                current_program = loaded_program
                print(f"[Trainer] ğŸ”¥ Warm Start: ä½¿ç”¨é¢„è®­ç»ƒç¨‹åº ({len(current_program)} æ¡è§„åˆ™)")
            else:
                current_program = self._generate_random_program()
                print(f"[Trainer] âš ï¸ Warm Start å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        else:
            current_program = self._generate_random_program()
        
        # ğŸ”„ åˆå§‹åŒ–å¼‚æ­¥è®­ç»ƒå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.async_training:
            from utils.async_trainer import create_trainer
            print(f"[Trainer] ğŸ”„ å¯ç”¨å¼‚æ­¥è®­ç»ƒæ¨¡å¼ï¼ˆMCTSä¸NNå¹¶è¡Œï¼‰")
            self.async_trainer = create_trainer(
                train_fn=lambda: self.train_step(),
                async_mode=True,
                update_interval=getattr(self.args, 'async_update_interval', 0.1),
                max_steps_per_iter=getattr(self.args, 'async_max_steps_per_iter', None)
            )
            self.async_trainer.start()
        else:
            from utils.async_trainer import create_trainer
            self.async_trainer = create_trainer(
                train_fn=lambda: self.train_step(),
                async_mode=False
            )
        
        for iter_idx in range(self.args.total_iters):
            if self.async_training and self.async_trainer is not None:
                self.async_trainer.reset_iter()
            iter_start_time = time.time()
            
            # ï¿½ï¸ æ¸©åº¦é€€ç«ï¼šé€æ­¥ä»æ¢ç´¢è½¬å‘åˆ©ç”¨
            if iter_idx < self._policy_temperature_decay_iters:
                progress = iter_idx / self._policy_temperature_decay_iters
                self._policy_temperature = self._policy_temperature_init + \
                    (self._policy_temperature_final - self._policy_temperature_init) * progress
                if (iter_idx + 1) % 50 == 0:  # æ¯50è½®æ‰“å°ä¸€æ¬¡
                    print(f"[æ¸©åº¦é€€ç«] T={self._policy_temperature:.3f}")
            else:
                self._policy_temperature = self._policy_temperature_final
            # ğŸŒªï¸ æ ¹ Dirichlet å™ªå£°è°ƒæ•´ï¼šMeta-RL åŠ¨æ€æ§åˆ¶ æˆ– å¯å‘å¼é€€ç«
            if self.use_meta_rl and self.meta_rl_controller is not None:
                # Meta-RL æ¨¡å¼ï¼šæ ¹æ®è®­ç»ƒæŒ‡æ ‡åŠ¨æ€è°ƒæ•´è¶…å‚æ•°
                if iter_idx > 0:  # è·³è¿‡ç¬¬ä¸€è½®ï¼ˆæ²¡æœ‰å†å²æ•°æ®ï¼‰
                    try:
                        hyperparams = self.meta_rl_controller.predict(
                            reward_history=[s['reward'] for s in self.training_stats[-20:]],  # æœ€è¿‘20è½®å¥–åŠ±
                            best_reward=self.best_reward,
                            current_iter=iter_idx
                        )
                        self._root_dirichlet_eps = hyperparams['root_dirichlet_eps']
                        self._root_dirichlet_alpha = hyperparams['root_dirichlet_alpha']
                        if (iter_idx + 1) % 50 == 0:
                            print(f"[Meta-RL] eps={self._root_dirichlet_eps:.3f}, alpha={self._root_dirichlet_alpha:.3f}")
                    except Exception as e:
                        print(f"[Meta-RL] é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            else:
                # å¯å‘å¼é€€ç«æ¨¡å¼
                if iter_idx < self._root_dirichlet_decay_iters:
                    p = iter_idx / max(1, self._root_dirichlet_decay_iters)
                    self._root_dirichlet_eps = self._root_dirichlet_eps_init + (self._root_dirichlet_eps_final - self._root_dirichlet_eps_init) * p
                    self._root_dirichlet_alpha = self._root_dirichlet_alpha_init + (self._root_dirichlet_alpha_final - self._root_dirichlet_alpha_init) * p
                    if (iter_idx + 1) % 100 == 0:
                        print(f"[Dirichleté€€ç«] eps={self._root_dirichlet_eps:.2f}, alpha={self._root_dirichlet_alpha:.2f}")
                else:
                    self._root_dirichlet_eps = self._root_dirichlet_eps_final
                    self._root_dirichlet_alpha = self._root_dirichlet_alpha_final
            
            # ï¿½ğŸ“ é›¶åŠ¨ä½œæƒ©ç½šè¯¾ç¨‹åŒ–ï¼šæ¯è½®è¡°å‡
            if iter_idx > 0 and zero_action_penalty_decay < 1.0 and current_zero_penalty > zero_action_penalty_min:
                current_zero_penalty = max(zero_action_penalty_min, current_zero_penalty * zero_action_penalty_decay)
                # åŠ¨æ€æ›´æ–°è¯„ä¼°å™¨çš„é›¶åŠ¨ä½œæƒ©ç½š
                if hasattr(self.evaluator, 'zero_action_penalty'):
                    self.evaluator.zero_action_penalty = current_zero_penalty
                if (iter_idx + 1) % 10 == 0:  # æ¯10è½®æ‰“å°ä¸€æ¬¡
                    print(f"[Curriculum] é›¶åŠ¨ä½œæƒ©ç½šè¡°å‡è‡³: {current_zero_penalty:.3f}")
            
            penalty_info = f" | ZeroPenalty={current_zero_penalty:.2f}" if current_zero_penalty > 0 else ""
            
            # â­ ç®€åŒ–è¾“å‡ºæ¨¡å¼ï¼šä»…æ¯ N è½®æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯ï¼ˆé»˜è®¤æ¯10è½®ï¼‰
            verbose_interval = int(os.environ.get('TRAIN_VERBOSE_INTERVAL', '10'))
            show_iter_detail = (iter_idx + 1) % verbose_interval == 0 or iter_idx == 0 or (iter_idx + 1) == self.args.total_iters
            
            if show_iter_detail:
                print(f"\n[Iter {iter_idx+1}/{self.args.total_iters}] MCTSæœç´¢ä¸­...{penalty_info}")
            
            # MCTSæœç´¢ï¼ˆå¸¦ç²¾è‹±æ ¹ç§å­ï¼‰
            seeded_program = current_program
            try:
                if (iter_idx + 1) >= int(getattr(self, '_elite_seed_delay', 20)) and self.elite_archive:
                    import random as _r
                    if _r.random() < float(getattr(self, '_elite_seed_prob', 0.25)):
                        k = min(int(getattr(self, '_elite_seed_topk', 5)), len(self.elite_archive))
                        cand = self.elite_archive[:k]
                        _, seeded_program, src_iter = _r.choice(cand)
                        if show_iter_detail:
                            print(f"[Seed] ä½¿ç”¨ç²¾è‹±æ ¹ç§å­ (Top-{k} å†…) | æ¥è‡ªè¿­ä»£ {src_iter}")
            except Exception:
                seeded_program = current_program
            children, visit_counts = self.mcts_search(seeded_program, self.args.mcts_simulations, iter_idx)
            
            # ğŸ§¹ æ¯æ¬¡MCTSåç«‹å³æ¸…ç†å†…å­˜
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ğŸŒ³ æ ¹èŠ‚ç‚¹æ¢ç´¢å¤šæ ·æ€§ç»Ÿè®¡ï¼ˆæ¯10è½®è¾“å‡ºï¼‰
            if show_iter_detail and children:
                total_visits = sum(visit_counts)
                entropy = 0.0
                if total_visits > 0:
                    probs = [v / total_visits for v in visit_counts]
                    entropy = -sum(p * np.log(p + 1e-12) for p in probs if p > 0)
                top3_visits = sorted(visit_counts, reverse=True)[:3]
                print(f"  [æ ¹ç»Ÿè®¡] å­èŠ‚ç‚¹æ•°={len(children)}, æ€»è®¿é—®={total_visits}, ç†µ={entropy:.3f}, Top3è®¿é—®={top3_visits}")
            
            if not children:
                if show_iter_detail:
                    print(f"[Iter {iter_idx+1}] âš ï¸ æœªç”Ÿæˆå­èŠ‚ç‚¹ï¼Œè·³è¿‡")
                continue
            
            # é€‰æ‹©è®¿é—®æœ€å¤šçš„å­èŠ‚ç‚¹
            # ä¾æ® policy_temperature é€‰æ‹©æ ¹åŠ¨ä½œï¼šT>0 è¿›è¡ŒæŒ‰è®¿é—®æ¦‚ç‡é‡‡æ ·ï¼ŒT=0 å–æœ€å¤§
            if children:
                if self._policy_temperature > 1e-8:
                    import numpy as _np
                    counts = _np.array(visit_counts, dtype=_np.float64)
                    # æ¸©åº¦ç¼©æ”¾ï¼šp_i âˆ (N_i)^{1/T}
                    scaled = counts ** (1.0 / max(1e-6, self._policy_temperature))
                    ps = scaled / max(1e-12, scaled.sum())
                    choice = int(_np.random.choice(len(children), p=ps))
                    best_child = children[choice]
                else:
                    best_child_idx = np.argmax(visit_counts)
                    best_child = children[best_child_idx]
            else:
                best_child = None
            if best_child is None:
                print(f"[Iter {iter_idx+1}] âš ï¸ æ ¹èŠ‚ç‚¹æ— å­èŠ‚ç‚¹ï¼Œä¿æŒåŸç¨‹åº")
                next_program = current_program
            else:
                next_program = best_child.program
            
            # è¿­ä»£è¯Šæ–­ï¼šå˜é‡ä½¿ç”¨ä¸æ˜¯å¦åŒ…å« u_* æ§åˆ¶é”®
            try:
                info = self._analyze_program(next_program)
                uses_u = False
                try:
                    if hasattr(self.evaluator, '_program_uses_u'):
                        uses_u = bool(self.evaluator._program_uses_u(next_program))  # type: ignore
                except Exception:
                    uses_u = False
                print(f"[Iter {iter_idx+1}] è¯Šæ–­: variables={info.get('used_variables', [])[:8]} | rules={info.get('rule_count')} | uses_u={uses_u}")
                try:
                    prog_str = self._program_to_str(next_program, max_rules=3)
                    print(f"[Iter {iter_idx+1}] ç¨‹åº: {prog_str}")
                except Exception:
                    pass
            except Exception:
                pass

            # çœŸå®è¯„ä¼°ï¼ˆæ¯æ¬¡è¿­ä»£è‡³å°‘1æ¬¡ï¼‰
            # ä¼˜å…ˆä½¿ç”¨ç»„ä»¶çº§æ¥å£è·å–ç»†ç²’åº¦æŒ‡æ ‡
            # ğŸ” åˆ†ç¦»è®­ç»ƒå¥–åŠ±å’ŒçœŸå®å¥–åŠ±
            reward_train = 0.0  # è®­ç»ƒä¿¡å·ï¼ˆå«æƒ©ç½šï¼‰â†’ ç”¨äºNNå’Œbest_rewardæ¯”è¾ƒ
            reward_true = 0.0   # çœŸå®å¥–åŠ±ï¼ˆä¸å«æƒ©ç½šï¼‰â†’ ç”¨äºä¿å­˜å’Œè¾“å‡º
            reward_components = None

            # ğŸ”„ å•è½´ç¨‹åºåœ¨è¯„ä¼°å‰å¼ºåˆ¶é•œåƒï¼Œè¡¥ä¸Š yaw/thrust ç¨³å®šå™¨ï¼Œé¿å…æ— æ¨åŠ›å è½
            eval_program = next_program
            try:
                if hasattr(self.evaluator, '_mirror_expand_single_axis_program'):
                    # å¿«é€Ÿæ£€æµ‹ï¼šæ˜¯å¦ä»…è®¾ç½® u_txï¼ˆASTèŠ‚ç‚¹æ ¼å¼ï¼‰
                    targets = set()
                    for rule in next_program or []:
                        for act in rule.get('action', []) or []:
                            if hasattr(act, 'op') and act.op == 'set' and hasattr(act, 'left') and hasattr(act.left, 'value'):
                                targets.add(str(getattr(act.left, 'value', '')))
                    
                    if targets == {'u_tx'}:
                        # ç›´æ¥é•œåƒï¼ˆ_mirror_expand_single_axis_program å†…éƒ¨èƒ½å¤„ç† ASTï¼‰
                        try:
                            eval_program = self.evaluator._mirror_expand_single_axis_program(next_program)
                            print(f"[Iter {iter_idx+1}] ğŸ” å•è½´ u_tx å·²æ‰©å±•: +u_ty +u_tz +u_fz")
                        except Exception as _mirror_exc:
                            print(f"  âš ï¸ é•œåƒå¤±è´¥ï¼Œä½¿ç”¨åŸç¨‹åº: {_mirror_exc}")
            except Exception as _outer_exc:
                pass  # é™é»˜å¤±è´¥ï¼Œä½¿ç”¨åŸç¨‹åº

            if hasattr(self.evaluator, 'evaluate_single_with_metrics'):
                try:
                    print(f"[Iter {iter_idx+1}] ğŸ” å¼€å§‹è¯„ä¼°...")
                    reward_train, reward_true, reward_components = self.evaluator.evaluate_single_with_metrics(eval_program)
                    print(f"[Iter {iter_idx+1}] âœ… è¯„ä¼°å®Œæˆ")
                    # æ‰“å°ç»„ä»¶ç”¨äºè¯Šæ–­
                    if reward_components:
                        state_c = reward_components.get('state_cost', 0.0)
                        action_c = reward_components.get('action_cost', 0.0)
                        print(f"[Iter {iter_idx+1}] ç»„ä»¶: state={state_c:.3f} | action={action_c:.3e}")
                        print(f"[Iter {iter_idx+1}] å¥–åŠ±: çœŸå®={reward_true:.4f}, è®­ç»ƒ={reward_train:.4f}")
                except Exception as e:
                    print(f"  âš ï¸  evaluate_single_with_metrics å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    reward_train = self.evaluator.evaluate_single(eval_program)
                    reward_true = reward_train
            else:
                reward_train = self.evaluator.evaluate_single(eval_program)
                reward_true = reward_true

            # æ”¶é›†è®­ç»ƒæ ·æœ¬
            # ç­–ç•¥æ ‡ç­¾ï¼šå°†æ ¹å­èŠ‚ç‚¹è®¿é—®åˆ†å¸ƒæŒ‰å…¶ç¼–è¾‘ç±»å‹èšåˆåˆ° EDIT_TYPES
            total_visits = sum(visit_counts)
            policy_target = torch.zeros(len(EDIT_TYPES))
            if total_visits > 0:
                for i, child in enumerate(children):
                    prob = float(visit_counts[i]) / float(total_visits)
                    et = getattr(child, '_edit_type', None)
                    if et in EDIT_TYPES:
                        policy_target[EDIT_TYPES.index(et)] += prob
                    else:
                        # è‹¥æœªçŸ¥ç±»å‹ï¼Œç­‰é‡åˆ†æ‘Šåˆ°æ‰€æœ‰ç»´åº¦ï¼Œé¿å…ä¸¢å¤±æ¦‚ç‡è´¨é‡
                        policy_target += prob / len(EDIT_TYPES)
                # å½’ä¸€åŒ–ï¼ˆæ•°å€¼å®‰å…¨ï¼‰
                s = float(policy_target.sum().item())
                if s > 0:
                    policy_target = policy_target / s
            else:
                # æ²¡æœ‰è®¿é—®è®¡æ•°æ—¶ï¼Œé€€åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
                policy_target += 1.0 / len(EDIT_TYPES)

                # --- NNå†…éƒ¨å¹³æ»‘ä¸æ¢ç´¢å¢å¼ºï¼ˆä¸æš´éœ²æˆå¤–éƒ¨è¶…å‚æ•°ï¼‰ ---
            # Label smoothing: é˜²æ­¢ç›®æ ‡è¿‡æ—©å•ä¸€åŒ–å¯¼è‡´ policy_loss=0
            _SMOOTH_EPS = 0.02  # å›ºå®šå¾®å°å€¼ï¼Œä¸ä½œä¸ºCLIå‚æ•°
            if policy_target.sum() > 0:  # ä¿è¯å·²å½’ä¸€åŒ–
                policy_target = (1.0 - _SMOOTH_EPS) * policy_target + _SMOOTH_EPS / len(EDIT_TYPES)
            # ç›®æ ‡ç†µæœ€å°æ­£åˆ™ï¼šè‹¥ç†µè¿‡ä½(æ¥è¿‘0)ï¼Œè½»å¾®æŠ¬é«˜éæœ€å¤§ç±»
            try:
                _entropy = float((-(policy_target.clamp(min=1e-12) * policy_target.clamp(min=1e-12).log()).sum()).item())
                _H_min = 0.15  # å…è®¸ä»å¾ˆå°–é”ï¼Œä½†é¿å…ç»å¯¹ one-hot
                if _entropy < _H_min:
                    # å¯¹æœ€å¤§æ¦‚ç‡ç±»åšå¾®ç¼©ï¼Œå…¶ä½™å‡åŒ€è¡¥å¿
                    _top_idx = int(policy_target.argmax().item())
                    _shrink = 0.05  # ç¼©å‡å¹…åº¦
                    top_val = float(policy_target[_top_idx].item())
                    if top_val > _shrink:
                        policy_target[_top_idx] = top_val - _shrink
                        # é‡æ–°åˆ†é…ç¼©å‡çš„æ¦‚ç‡åˆ°å…¶å®ƒç»´åº¦
                        _redistrib = _shrink / (len(EDIT_TYPES) - 1)
                        for _i in range(len(EDIT_TYPES)):
                            if _i != _top_idx:
                                policy_target[_i] += _redistrib
                    # å†æ¬¡å½’ä¸€åŒ–é¿å…æ•°å€¼æ¼‚ç§»
                    s2 = float(policy_target.sum().item())
                    if abs(s2 - 1.0) > 1e-6 and s2 > 0:
                        policy_target /= s2
            except Exception:
                pass
            
            # æ„å»ºæ ·æœ¬ï¼ˆåŒ…å«reward_trueç”¨äºvalue headè®­ç»ƒï¼‰
            sample = {
                'graph': ast_to_pyg_graph(current_program),
                'policy_target': policy_target,
                'reward_true': reward_true  # ç”¨äºè®­ç»ƒvalue head
            }
            
            self.replay_buffer.push(sample)
            
            # ğŸ”¥ æ”¶é›†ç¨‹åºå¯¹åˆ°ranking bufferï¼ˆè‹¥å¯ç”¨ï¼Œæ•´åˆåŠ¨ä½œç‰¹å¾ï¼‰
            if self.use_ranking and self.ranking_buffer is not None:
                pairs_collected = 0
                
                # ğŸ¯ å¼ºåˆ¶å¤šæ ·åŒ–ç­–ç•¥ï¼šå¦‚æœMCTSè¿”å›çš„childrenå¤ªå°‘ï¼Œäººå·¥ç”Ÿæˆæ›´å¤šå˜å¼‚ç¨‹åº
                augmented_programs = []
                if len(children) < 5:  # å¦‚æœchildrenä¸è¶³5ä¸ª
                    # æ·»åŠ MCTSçš„children
                    for child in children:
                        augmented_programs.append((child.program, getattr(child, 'value_sum', 0.0) / max(1, getattr(child, 'visits', 1))))
                    
                    # äººå·¥ç”Ÿæˆé¢å¤–çš„å˜å¼‚ç¨‹åº
                    import copy
                    for _ in range(min(10, 15 - len(children))):  # è¡¥è¶³åˆ°15ä¸ª
                        mutated_program = copy.deepcopy(current_program)
                        # éšæœºåº”ç”¨ä¸€ä¸ªå˜å¼‚
                        if len(mutated_program) > 0:
                            idx = np.random.randint(0, len(mutated_program))
                            # ç®€å•å˜å¼‚ï¼šè°ƒæ•´ä¸€ä¸ªè§„åˆ™çš„åŠ¨ä½œå¸¸æ•°
                            rule = mutated_program[idx]
                            if 'action' in rule and len(rule['action']) > 0:
                                # æ‰¾åˆ°å¹¶å¾®è°ƒä¸€ä¸ªå¸¸æ•°
                                for action in rule['action']:
                                    if hasattr(action, 'right') and hasattr(action.right, 'value') and isinstance(action.right.value, (int, float)):
                                        action.right.value = round(float(action.right.value) * np.random.uniform(0.85, 1.15), 4)
                                        break
                            # ç®€åŒ–è¯„ä¼°ï¼šä½¿ç”¨Qå€¼ä¼°è®¡ï¼ˆåŸºäºçœŸå®å¥–åŠ±ï¼‰
                            estimated_q = reward_true + np.random.uniform(-2.0, 2.0)  # æ·»åŠ å™ªå£°
                            augmented_programs.append((mutated_program, estimated_q))
                else:
                    # childrenè¶³å¤Ÿå¤šï¼Œç›´æ¥ä½¿ç”¨
                    for child in children:
                        augmented_programs.append((child.program, getattr(child, 'value_sum', 0.0) / max(1, getattr(child, 'visits', 1))))
                
                # 1ï¸âƒ£ å½“å‰ç¨‹åº vs augmented programs
                current_reward = reward_true  # å½“å‰æ ¹èŠ‚ç‚¹çš„çœŸå®å¥–åŠ±ï¼ˆç”¨äºrankingæ¯”è¾ƒï¼‰
                current_graph = ast_to_pyg_graph(current_program)
                current_action_feat = self._quick_action_features(current_program)
                
                for prog, prog_reward in augmented_programs:
                    prog_graph = ast_to_pyg_graph(prog)
                    prog_action_feat = self._quick_action_features(prog)
                    
                    # è¿‡æ»¤å¥–åŠ±å·®è¿‡å°çš„æ ·æœ¬å¯¹ï¼Œé™ä½å™ªå£°
                    if abs(float(prog_reward) - float(current_reward)) < getattr(self, '_ranking_min_delta', 0.0):
                        continue
                    if prog_reward != current_reward:
                        if prog_reward > current_reward:
                            self.ranking_buffer.push(current_graph, prog_graph, 1.0, 
                                                    current_action_feat, prog_action_feat)
                            pairs_collected += 1
                        elif prog_reward < current_reward:
                            self.ranking_buffer.push(prog_graph, current_graph, 0.0,
                                                    prog_action_feat, current_action_feat)
                            pairs_collected += 1
                
                # 2ï¸âƒ£ augmented programsä¹‹é—´äº’ç›¸æ¯”è¾ƒï¼ˆå¢åŠ æ•°æ®é‡ï¼‰
                # å–Qå€¼æœ€é«˜çš„top-kè¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ
                if len(augmented_programs) > 1:
                    top_k = min(5, len(augmented_programs))
                    top_programs = sorted(augmented_programs, key=lambda x: x[1], reverse=True)[:top_k]
                    
                    aug_pairs_before = pairs_collected
                    for i in range(len(top_programs)):
                        for j in range(i + 1, len(top_programs)):
                            prog_i, q_i = top_programs[i]
                            prog_j, q_j = top_programs[j]
                            if abs(float(q_i) - float(q_j)) < getattr(self, '_ranking_min_delta', 0.0):
                                continue
                            if q_i != q_j:
                                graph_i = ast_to_pyg_graph(prog_i)
                                graph_j = ast_to_pyg_graph(prog_j)
                                feat_i = self._quick_action_features(prog_i)
                                feat_j = self._quick_action_features(prog_j)
                                if q_i > q_j:
                                    self.ranking_buffer.push(graph_j, graph_i, 1.0, feat_j, feat_i)
                                else:
                                    self.ranking_buffer.push(graph_i, graph_j, 1.0, feat_i, feat_j)
                                pairs_collected += 1
                
                if pairs_collected > 0:
                    print(f"[Iter {iter_idx+1}] ğŸ“Š Ranking: æ”¶é›†{pairs_collected}å¯¹ç¨‹åº (bufferæ€»è®¡={len(self.ranking_buffer)}å¯¹)")
            
            # æ›´æ–°NNï¼ˆæ¯Næ¬¡è¿­ä»£ï¼‰
            nn_loss_info = ""
            if (iter_idx + 1) % self.args.update_freq == 0:
                if self.async_training:
                    # ğŸ”„ å¼‚æ­¥æ¨¡å¼ï¼šè·å–åå°è®­ç»ƒçš„æœ€æ–° metrics
                    metrics = self.async_trainer.get_metrics()
                    if metrics:
                        v_loss_str = f", v={metrics.get('value_loss', 0.0):.4f}" if self.enable_value_head else ""
                        nn_loss_info = f" | NN Loss: {metrics.get('total_loss', 0.0):.4f} (p={metrics.get('policy_loss', 0.0):.4f}{v_loss_str})"
                        print(f"[Iter {iter_idx+1}] ï¿½ å¼‚æ­¥è®­ç»ƒçŠ¶æ€: {metrics.get('policy_loss', 0.0):.4f}")
                    stats = self.async_trainer.get_stats()
                    print(f"  ï¿½ğŸ”„ åå°è®­ç»ƒ: {stats['total_steps']} steps, å¹³å‡ {stats.get('avg_time_per_step', 0)*1000:.1f}ms/step")
                else:
                    # åŒæ­¥æ¨¡å¼ï¼šåŸé€»è¾‘
                    print(f"[Iter {iter_idx+1}] ğŸ”„ æ›´æ–°NN...")
                    total_policy_loss = 0.0
                    total_value_loss = 0.0
                    total_loss = 0.0
                    for step_idx in range(self.args.train_steps_per_update):
                        losses = self.train_step()
                        if losses:
                            total_policy_loss += losses['policy_loss']
                            total_value_loss += losses.get('value_loss', 0.0)
                            total_loss += losses['total_loss']
                            if step_idx == 0 or (step_idx + 1) % 10 == 0:  # è¾“å‡ºé¦–æ¬¡å’Œæ¯10æ­¥
                                # é™„å¸¦ç­–ç•¥ç›®æ ‡åˆ†å¸ƒè¯Šæ–­ï¼Œå¸®åŠ©å®šä½ policy_loss=0 çš„æ ¹å› 
                                v_str = f", v={losses.get('value_loss', 0.0):.4f}" if self.enable_value_head else ""
                                diag_msg = (
                                    f"pt_sum(mean={losses['pt_sum_mean']:.3f}, min={losses['pt_sum_min']:.3f}, max={losses['pt_sum_max']:.3f}), "
                                    f"pt_nz(mean={losses['pt_nz_mean']:.1f}), "
                                    f"pt_H(mean={losses['pt_entropy_mean']:.3f}), "
                                    f"p(correct)_mean={losses['pred_correct_prob_mean']:.3f}, "
                                    f"top1_acc={losses['pred_top1_acc']:.2f}, "
                                    f"H_pred={losses.get('policy_entropy', 0.0):.3f}"
                                )
                                if losses.get('pt_any_zero_sum') or losses.get('pt_any_nan_sum'):
                                    diag_msg += " | ALERT: target_sum_zero_or_nan"
                                print(
                                    f"  Step {step_idx+1}/{self.args.train_steps_per_update}: "
                                    f"policy={losses['policy_loss']:.4f}{v_str}, "
                                    f"total={losses['total_loss']:.4f} | " + diag_msg
                                )
                    # å¹³å‡loss
                    n_steps = self.args.train_steps_per_update
                    avg_policy = total_policy_loss / n_steps
                    avg_value = total_value_loss / n_steps
                    avg_total = total_loss / n_steps
                    v_loss_str = f", v={avg_value:.4f}" if self.enable_value_head else ""
                    nn_loss_info = f" | NN Loss: {avg_total:.4f} (p={avg_policy:.4f}{v_loss_str})"
                    print(f"  âœ… å¹³å‡Loss: policy={avg_policy:.4f}{v_loss_str}, total={avg_total:.4f}")
                
                # ğŸ§¹ å®šæœŸå†…å­˜æ¸…ç†ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ğŸ”¥ è®­ç»ƒRankingç½‘ç»œï¼ˆè‹¥å¯ç”¨ä¸”bufferæœ‰è¶³å¤Ÿæ ·æœ¬ï¼‰
                if self.use_ranking and self.ranking_buffer is not None:
                    buffer_size = len(self.ranking_buffer)
                    if buffer_size >= 8:
                        print(f"  ğŸ”¥ è®­ç»ƒRankingç½‘ç»œ (buffer={buffer_size}å¯¹)...")
                    else:
                        print(f"  â¸ï¸  Rankingè®­ç»ƒè·³è¿‡ (buffer={buffer_size}å¯¹ < 8å¯¹æœ€å°å€¼)")
                
                ranking_paused_async = False
                if self.use_ranking and self.ranking_buffer is not None and len(self.ranking_buffer) >= 8:
                    if self.async_training and self.async_trainer is not None:
                        self.async_trainer.pause_and_wait()
                        ranking_paused_async = True
                        if torch.cuda.is_available():
                            torch.cuda.synchronize()
                            torch.cuda.empty_cache()

                    try:
                        ranking_loss_total = 0.0
                        ranking_acc_total = 0.0
                        ranking_steps = min(10, max(1, len(self.ranking_buffer) // 8))  # è‡ªé€‚åº”æ­¥æ•°ï¼ˆé™ä½æ‰¹æ¬¡å¤§å°ï¼‰
                        for _ in range(ranking_steps):
                            ranking_metrics = train_ranking_step(
                                ranking_net=self.ranking_net,
                                ranking_buffer=self.ranking_buffer,
                                ranking_optimizer=self.ranking_optimizer,
                                gnn_encoder=self.nn_model,
                                device=self.device,
                                batch_size=min(8, len(self.ranking_buffer))  # åŠ¨æ€batch size
                            )
                            if ranking_metrics:
                                ranking_loss_total += ranking_metrics['ranking_loss']
                                ranking_acc_total += ranking_metrics['ranking_accuracy']
                        avg_ranking_loss = ranking_loss_total / ranking_steps
                        avg_ranking_acc = ranking_acc_total / ranking_steps
                        print(f"  âœ… Rankingè®­ç»ƒå®Œæˆ: loss={avg_ranking_loss:.4f}, accuracy={avg_ranking_acc:.2%}")
                    finally:
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        if ranking_paused_async and self.async_trainer is not None:
                            self.async_trainer.resume()
                    
                    # ğŸ“ è¯¾ç¨‹å­¦ä¹ ï¼šé€æ­¥æé«˜rankingæ··åˆç³»æ•°
                    if iter_idx < self.ranking_blend_warmup_iters:
                        progress = (iter_idx + 1) / self.ranking_blend_warmup_iters
                        self.ranking_blend_factor = self.ranking_blend_factor + progress * (self.ranking_blend_max - self.ranking_blend_factor)
                        if (iter_idx + 1) % 10 == 0:
                            print(f"  ğŸ“ˆ Rankingæ··åˆç³»æ•°: {self.ranking_blend_factor:.2f}")
            
            # æ›´æ–°æœ€ä½³ç¨‹åº
            # ğŸŒŸ ä½¿ç”¨çœŸå®å¥–åŠ±è¿›è¡Œbest_rewardæ¯”è¾ƒï¼ˆé¿å…è®­ç»ƒæƒ©ç½šé¡¹é€€ç«å¯¼è‡´çš„è™šå‡è¿›æ­¥ï¼‰
            if reward_true > self.best_reward:
                self.best_reward = reward_true
                # ä¿ç•™åŸå§‹å•è½´ç¨‹åºç”¨äºæŒä¹…åŒ–ï¼Œè¿è¡Œæ—¶å¯æŒ‰éœ€é•œåƒ
                import copy
                self.best_program = copy.deepcopy(next_program)
                # ğŸ”’ æ·±æ‹·è´ä¿æŠ¤,é˜²æ­¢cleanup_treeæˆ–GCæ¸…ç†ï¼ˆä¿å­˜ç”¨åŸå§‹å•è½´ï¼‰
                self.best_program_copy = copy.deepcopy(next_program)
                # è¿è¡Œæ—¶ä»å¯ä½¿ç”¨é•œåƒç‰ˆæœ¬åšå¿«é€Ÿè¯„ä¼°/å¯¼å‡º
                expanded_program = self.evaluator._mirror_expand_single_axis_program(next_program)
                print(f"[Iter {iter_idx+1}] ğŸ‰ æ–°æœ€ä½³ï¼çœŸå®å¥–åŠ±: {reward_true:.4f} (è®­ç»ƒå¥–åŠ±: {reward_train:.4f})")
                
                # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šåªæœ‰æ¯”å·²ä¿å­˜æ–‡ä»¶æ›´ä¼˜æ‰è¦†ç›–ä¿å­˜ï¼ˆä½¿ç”¨çœŸå®å¥–åŠ±æ¯”è¾ƒï¼‰
                saved_reward = self._get_saved_program_reward(self.args.save_path)
                should_save = reward_true > saved_reward
                
                if should_save:
                    # æ„å»ºå…ƒæ•°æ®ï¼šè®°å½•è®­ç»ƒè¿›åº¦å’Œå¥–åŠ±ä¿¡æ¯ï¼ˆä¿å­˜çœŸå®å¥–åŠ±ï¼‰
                    program_meta = {
                        'iteration': iter_idx + 1,
                        'total_iterations': self.args.total_iters,
                        'reward': float(reward_true),  # ğŸŒŸ ä¿å­˜çœŸå®å¥–åŠ±
                        'reward_train': float(reward_train),  # é™„å¸¦è®­ç»ƒå¥–åŠ±ä¾›å‚è€ƒ
                        'best_reward': float(self.best_reward),  # å½“å‰æœ€ä½³çœŸå®å¥–åŠ±
                        'trajectory': getattr(self.args, 'traj', 'unknown'),
                        'duration': getattr(self.args, 'duration', 10),
                        'reward_profile': getattr(self.args, 'reward_profile', 'safe_control_tracking'),
                        'mcts_simulations': self.args.mcts_simulations,
                        'isaac_num_envs': self.args.isaac_num_envs,
                    }
                    
                    # æ·»åŠ å¥–åŠ±ç»„ä»¶è¯¦æƒ…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if reward_components:
                        program_meta['reward_components'] = {k: float(v) for k, v in reward_components.items()}
                    
                    # æ·»åŠ ç¨‹åºç»“æ„ä¿¡æ¯
                    program_info = self._analyze_program(self.best_program)
                    if program_info:
                        program_meta.update({
                            'num_rules': program_info.get('num_rules', 0),
                            'num_variables': program_info.get('num_variables', 0),
                            'depth': program_info.get('depth', 0),
                        })
                    
                    # ä¿å­˜ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
                    save_program_json(self.best_program, self.args.save_path, meta=program_meta)
                    if saved_reward == float('-inf'):
                        print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°: {self.args.save_path} (çœŸå®å¥–åŠ±: {reward_true:.4f})")
                    else:
                        print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°: {self.args.save_path} (çœŸå®å¥–åŠ±: {reward_true:.4f}, è¶…è¶Šå·²ä¿å­˜: {saved_reward:.4f})")
                    
                    # è¿½åŠ ç¨‹åºå†å²ï¼ˆä½¿ç”¨çœŸå®å¥–åŠ±ï¼‰
                    self._append_program_history(iter_idx, reward_true, self.best_program)
                else:
                    print(f"  â¸ï¸  æœªä¿å­˜ï¼šå½“å‰çœŸå®å¥–åŠ± {reward_true:.4f} â‰¤ å·²ä¿å­˜ {saved_reward:.4f}ï¼ˆè·³è¿‡è¦†ç›–ï¼‰")
            
            # ğŸ† æ›´æ–°ç²¾è‹±ç¨‹åºæ±  (ä¿ç•™Top-Kæœ€ä¼˜ï¼Œä½¿ç”¨çœŸå®å¥–åŠ±æ’åº)
            import copy
            self.elite_archive.append((reward_true, copy.deepcopy(next_program), iter_idx + 1))
            # æŒ‰çœŸå®rewardé™åºæ’åº,ä¿ç•™Top-K
            self.elite_archive.sort(key=lambda x: x[0], reverse=True)
            if len(self.elite_archive) > self.elite_archive_size:
                self.elite_archive = self.elite_archive[:self.elite_archive_size]
            
            # æ¯20è½®è¾“å‡ºç²¾è‹±æ± çŠ¶æ€
            if (iter_idx + 1) % 20 == 0:
                top3_rewards = [r for r, _, _ in self.elite_archive[:3]]
                print(f"  ğŸ† ç²¾è‹±æ± Top-3: {top3_rewards}")
            
            # æ›´æ–°å½“å‰ç¨‹åº
            current_program = next_program
            
            iter_time = time.time() - iter_start_time
            
            # ğŸ“Š MCTSæ€§èƒ½ç»Ÿè®¡ (æ¯10è½®è¾“å‡ºä¸€æ¬¡)
            mcts_info = ""
            if self._mcts_stats and (iter_idx + 1) % 10 == 0:
                prior_gnn = self._mcts_stats.get('prior_gnn_nodes', 0)
                prior_cached = self._mcts_stats.get('prior_cached', 0)
                cache_size = self._mcts_stats.get('cache_size', 0)
                # è®¡ç®—priorå‘½ä¸­ç‡
                prior_total = prior_gnn + prior_cached
                prior_rate = (prior_cached / prior_total * 100) if prior_total > 0 else 0.0
                mcts_info = (
                    f" | PriorGNN: {prior_gnn} | PriorHit: {prior_rate:.0f}%"
                    f" | CacheSize: {cache_size}"
                )
                # é‡ç½®ç»Ÿè®¡
                self._mcts_stats = {}
            
            # ğŸ§  å†…å­˜ç›‘æ§ï¼ˆæ¯10è½®è¾“å‡ºï¼‰
            mem_info = ""
            if (iter_idx + 1) % 10 == 0:
                import psutil
                process = psutil.Process()
                ram_mb = process.memory_info().rss / 1024 / 1024
                mem_info = f" | RAM: {ram_mb:.0f}MB"
                if torch.cuda.is_available():
                    gpu_mb = torch.cuda.memory_allocated() / 1024 / 1024
                    gpu_max_mb = torch.cuda.max_memory_allocated() / 1024 / 1024
                    mem_info += f" | GPU: {gpu_mb:.0f}MB (å³°å€¼{gpu_max_mb:.0f}MB)"
            
            # â­ ç®€åŒ–è¾“å‡ºï¼šä»…åœ¨æŒ‡å®šé—´éš”æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆä½¿ç”¨çœŸå®å¥–åŠ±ï¼‰
            if show_iter_detail:
                print(f"[Iter {iter_idx+1}] å®Œæˆ | çœŸå®å¥–åŠ±: {reward_true:.4f} | è€—æ—¶: {iter_time:.1f}s | Buffer: {len(self.replay_buffer)}{mcts_info}{nn_loss_info}{mem_info}")
            else:
                # ç®€æ´æ¨¡å¼ï¼šä»…æ˜¾ç¤ºè¿›åº¦ç™¾åˆ†æ¯”
                progress_pct = (iter_idx + 1) / self.args.total_iters * 100
                print(f"\r[è¿›åº¦ {progress_pct:.1f}%] {iter_idx+1}/{self.args.total_iters} è½® | çœŸå®å¥–åŠ±: {reward_true:.4f} | Buffer: {len(self.replay_buffer)}", end='', flush=True)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (iter_idx + 1) % self.args.checkpoint_freq == 0:
                checkpoint_path = f"{self.args.save_path.replace('.json', '')}_nn_iter_{iter_idx+1}.pt"
                torch.save(self.nn_model.state_dict(), checkpoint_path)
                print(f"[Iter {iter_idx+1}] ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # ğŸ”„ åœæ­¢å¼‚æ­¥è®­ç»ƒå™¨
        if self.async_trainer is not None:
            print(f"[Trainer] ğŸ›‘ åœæ­¢å¼‚æ­¥è®­ç»ƒå™¨...")
            self.async_trainer.stop(wait=True)
            stats = self.async_trainer.get_stats()
            print(f"  æ€»è®­ç»ƒæ­¥æ•°: {stats['total_steps']}, æ€»è€—æ—¶: {stats.get('total_time', 0):.1f}s")
        
        print(f"\n{'='*80}")
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³å¥–åŠ±: {self.best_reward:.4f}")
        print(f"{'='*80}\n")
        
        # ğŸ† ä¿å­˜ç²¾è‹±ç¨‹åºæ± 
        elite_save_path = self.args.save_path.replace('.json', '_elite_archive.json')
        try:
            elite_data = []
            for reward, program, iter_num in self.elite_archive:
                # å°†ç¨‹åºè½¬æ¢ä¸ºå¯åºåˆ—åŒ–å½¢å¼ï¼ˆAST -> dictï¼‰ï¼Œé¿å…ç›´æ¥åŒ…å«Nodeå¯¹è±¡
                serializable_rules = []
                try:
                    for rule in program:
                        node = rule.get('node')
                        node_ser = to_serializable_dict(node) if node is not None else None
                        serializable_rules.append({
                            'name': rule.get('name', 'rule'),
                            'multiplier': rule.get('multiplier', [1.0, 1.0, 1.0]),
                            'node': node_ser
                        })
                except Exception:
                    # å…œåº•ï¼šè‹¥åºåˆ—åŒ–å¤±è´¥ï¼Œä¿å­˜ä¸€ä¸ªç®€åŒ–ç»“æ„ï¼Œè‡³å°‘ä¿ç•™è§„åˆ™æ•°é‡å’Œå€å¢å™¨
                    serializable_rules = [
                        {
                            'name': r.get('name', 'rule'),
                            'multiplier': r.get('multiplier', [1.0, 1.0, 1.0]),
                            'node': None
                        } for r in program
                    ]
                elite_data.append({
                    'reward': float(reward),
                    'iter': int(iter_num),
                    'program': serializable_rules
                })
            with open(elite_save_path, 'w') as f:
                json.dump(elite_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ† ç²¾è‹±ç¨‹åºæ± å·²ä¿å­˜: {elite_save_path} (å…±{len(self.elite_archive)}ä¸ªç¨‹åº)")
        except Exception as e:
            print(f"âš ï¸  ç²¾è‹±æ± ä¿å­˜å¤±è´¥: {e}")
        
        # ğŸ”’ æœ€ç»ˆä¿å­˜ï¼šä½¿ç”¨æ·±æ‹·è´çš„best_program_copy(åŸå§‹å•è½´)ï¼Œç¡®ä¿ä¸è¢«cleanupå½±å“
        if self.best_program_copy is not None:
            try:
                final_save_path = self.args.save_path.replace('.json', '_final.json')
                
                # æ„å»ºæœ€ç»ˆå…ƒæ•°æ®
                final_meta = {
                    'final_iteration': self.args.total_iters,
                    'best_reward': float(self.best_reward),
                    'trajectory': getattr(self.args, 'traj', 'unknown'),
                    'duration': getattr(self.args, 'duration', 10),
                    'reward_profile': getattr(self.args, 'reward_profile', 'safe_control_tracking'),
                    'mcts_simulations': self.args.mcts_simulations,
                    'isaac_num_envs': self.args.isaac_num_envs,
                    'training_completed': True,
                }
                
                # æ·»åŠ ç¨‹åºç»“æ„ä¿¡æ¯
                program_info = self._analyze_program(self.best_program_copy)
                if program_info:
                    final_meta.update({
                        'num_rules': program_info.get('num_rules', 0),
                        'num_variables': program_info.get('num_variables', 0),
                        'depth': program_info.get('depth', 0),
                    })
                
                save_program_json(self.best_program_copy, final_save_path, meta=final_meta)
                print(f"ğŸ”’ æœ€ä¼˜ç¨‹åº(ä¿æŠ¤å‰¯æœ¬)å·²ä¿å­˜: {final_save_path}")
                print(f"   æœ€ç»ˆå¥–åŠ±: {self.best_reward:.4f} | è§„åˆ™æ•°: {final_meta.get('num_rules', 'N/A')}")
            except Exception as e:
                print(f"âš ï¸  æœ€ä¼˜ç¨‹åºä¿å­˜å¤±è´¥: {e}")


def parse_args(argv=None):
    p = argparse.ArgumentParser(description='åœ¨çº¿è®­ç»ƒ - AlphaZeroå¼ç¨‹åºåˆæˆ')
    
    # è®­ç»ƒå‚æ•°
    p.add_argument('--total-iters', type=int, default=5000, help='æ€»è¿­ä»£æ•°')
    p.add_argument('--mcts-simulations', type=int, default=800, help='æ¯æ¬¡è¿­ä»£çš„MCTSæ¨¡æ‹Ÿæ•°')
    p.add_argument('--update-freq', type=int, default=50, help='NNæ›´æ–°é¢‘ç‡')
    p.add_argument('--train-steps-per-update', type=int, default=10, help='æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒæ­¥æ•°')
    p.add_argument('--batch-size', type=int, default=128, help='æ‰¹é‡å¤§å°ï¼ˆé™ä½ä»¥èŠ‚çœæ˜¾å­˜ï¼‰')
    p.add_argument('--replay-capacity', type=int, default=50000, help='ç»éªŒå›æ”¾å®¹é‡')
    
    # NNå‚æ•°ï¼ˆå›ºå®šç‰¹å¾ç½‘ç»œå·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨GNN v2ï¼Œåªè®­ç»ƒpolicyï¼‰
    p.add_argument('--learning-rate', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    
    # GNN æ¶æ„å‚æ•°
    p.add_argument('--gnn-structure-hidden', type=int, default=256, help='GNNç»“æ„ç¼–ç å™¨éšè—å±‚ç»´åº¦ï¼ˆé»˜è®¤256ï¼‰')
    p.add_argument('--gnn-structure-layers', type=int, default=5, help='GNNç»“æ„ç¼–ç å™¨å±‚æ•°ï¼ˆé»˜è®¤5ï¼‰')
    p.add_argument('--gnn-structure-heads', type=int, default=8, help='GNNç»“æ„ç¼–ç å™¨æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤8ï¼‰')
    p.add_argument('--gnn-feature-layers', type=int, default=3, help='GNNç‰¹å¾ç¼–ç å™¨å±‚æ•°ï¼ˆé»˜è®¤3ï¼‰')
    p.add_argument('--gnn-feature-heads', type=int, default=8, help='GNNç‰¹å¾ç¼–ç å™¨æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤8ï¼‰')
    p.add_argument('--gnn-dropout', type=float, default=0.1, help='GNN Dropoutæ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼‰')
    
    # MCTSå‚æ•°
    p.add_argument('--exploration-weight', type=float, default=2.5, help='UCBæ¢ç´¢æƒé‡ (æé«˜ä»¥å¢å¼ºå¹¿åº¦æ¢ç´¢)')
    p.add_argument('--puct-c', type=float, default=1.5, help='PUCTå¸¸æ•°')
    p.add_argument('--max-depth', type=int, default=12, help='MCTSæœ€å¤§æ·±åº¦ï¼ˆé™ä½ä»¥å‡å°‘åˆ†æ”¯ç¨€é‡Šï¼‰')
    p.add_argument('--mcts-leaf-batch-size', type=int, default=1, help='MCTSå¶èŠ‚ç‚¹æ‰¹é‡è¯„ä¼°å¤§å°ï¼ˆ>1å¯ç”¨å¹¶è¡ŒåŒ–ï¼Œæ¨è4-10ï¼‰')
    p.add_argument('--disable-progressive-widening', action='store_true', help='ç¦ç”¨ Progressive Wideningï¼ŒèŠ‚ç‚¹ä¸€æ¬¡æ€§æ‰©å±•æ‰€æœ‰å¯å˜å¼‚ï¼ˆè­¦å‘Šï¼šæ ‘å®½å¯èƒ½çˆ†ç‚¸ï¼‰')
    p.add_argument('--async-training', action='store_true', help='å¯ç”¨å¼‚æ­¥è®­ç»ƒæ¨¡å¼ï¼šMCTSä¸NNè®­ç»ƒå¹¶è¡Œï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰')
    p.add_argument('--async-update-interval', type=float, default=0.1, help='å¼‚æ­¥è®­ç»ƒçº¿ç¨‹ä¸¤æ¬¡è®­ç»ƒä¹‹é—´çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰')
    p.add_argument('--async-max-steps-per-iter', type=int, default=None, help='æ¯è½®å…è®¸çš„å¼‚æ­¥è®­ç»ƒæ­¥æ•°ä¸Šé™ï¼ˆNoneè¡¨ç¤ºä¸é™ï¼‰')
    
    # é«˜çº§ä¼˜åŒ–å¼€å…³
    p.add_argument('--enable-ranking-mcts-bias', action='store_true', help='å¯ç”¨Rankingå¯¹MCTSå­èŠ‚ç‚¹å…ˆéªŒåŠ æƒï¼ˆæ‰“ç ´plateauï¼‰')
    p.add_argument('--ranking-bias-beta', type=float, default=0.3, help='Ranking biaså¼ºåº¦ï¼ˆé»˜è®¤0.3ï¼‰')
    p.add_argument('--enable-value-head', action='store_true', help='å¯ç”¨Valueå¤´è¾…åŠ©è®­ç»ƒï¼ˆä»…ç”¨äºæ¢¯åº¦ä¿¡å·ï¼Œä¸å½±å“MCTSï¼‰')
    p.add_argument('--enable-ranking-reweight', action='store_true', help='ç”¨Ranking scoreé‡æ–°åŠ æƒpolicy target')
    p.add_argument('--ranking-reweight-beta', type=float, default=0.2, help='Ranking reweightå¼ºåº¦ï¼ˆé»˜è®¤0.2ï¼‰')
    
    # æ³¨æ„ï¼šå·²ç§»é™¤ --real-sim-frac å’Œ --force-full-simï¼Œç°åœ¨å…¨éƒ¨ä½¿ç”¨çœŸå®ä»¿çœŸ
    # AlphaZero å¼æ¢ç´¢å¢å¼º
    p.add_argument('--root-dirichlet-eps', type=float, default=0.25, help='æ ¹èŠ‚ç‚¹å…ˆéªŒæ··åˆ Dirichlet å™ªå£°æ¯”ä¾‹ eps (0 å…³é—­)')
    p.add_argument('--root-dirichlet-alpha', type=float, default=0.3, help='æ ¹èŠ‚ç‚¹ Dirichlet å™ªå£° alpha å‚æ•°')
    p.add_argument('--policy-temperature', type=float, default=1.0, help='æ ¹èŠ‚ç‚¹æ ¹æ®è®¿é—®è®¡æ•°é‡‡æ ·çš„æ¸©åº¦ç³»æ•°ï¼Œ1 ä¸ºæŒ‰è®¿é—®è®¡æ•°æˆæ¯”ä¾‹é‡‡æ ·ï¼Œ0 ä¸ºè´ªå¿ƒ')
    
    # Meta-RL åœ¨çº¿è°ƒå‚ï¼ˆå¯é€‰ï¼‰
    p.add_argument('--use-meta-rl', action='store_true', help='å¯ç”¨ Meta-RL RNN æ§åˆ¶å™¨è¿›è¡ŒåŠ¨æ€è¶…å‚æ•°è°ƒæ•´ï¼ˆéœ€è¦é¢„è®­ç»ƒæ¨¡å‹ï¼‰')
    p.add_argument('--meta-rl-checkpoint', type=str, default='meta_rl/checkpoints/meta_policy.pt', help='Meta-RL æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    
    # å¯å‘å¼è¡°å‡å‚æ•°ï¼ˆå½“ä¸ä½¿ç”¨ Meta-RL æ—¶ç”Ÿæ•ˆï¼‰
    p.add_argument('--root-dirichlet-eps-init', type=float, default=None, help='Dirichlet eps åˆå§‹å€¼ï¼ˆå¯å‘å¼è¡°å‡æ¨¡å¼ï¼ŒNoneåˆ™ä½¿ç”¨--root-dirichlet-epsï¼‰')
    p.add_argument('--root-dirichlet-eps-final', type=float, default=None, help='Dirichlet eps ç»ˆæ­¢å€¼ï¼ˆå¯å‘å¼è¡°å‡æ¨¡å¼ï¼‰')
    p.add_argument('--root-dirichlet-alpha-init', type=float, default=None, help='Dirichlet alpha åˆå§‹å€¼ï¼ˆå¯å‘å¼è¡°å‡æ¨¡å¼ï¼ŒNoneåˆ™ä½¿ç”¨--root-dirichlet-alphaï¼‰')
    p.add_argument('--root-dirichlet-alpha-final', type=float, default=None, help='Dirichlet alpha ç»ˆæ­¢å€¼ï¼ˆå¯å‘å¼è¡°å‡æ¨¡å¼ï¼‰')
    p.add_argument('--heuristic-decay-window', type=int, default=200, help='å¯å‘å¼è¡°å‡çª—å£ï¼ˆå¤šå°‘è½®å†…å®Œæˆé€€ç«ï¼Œé»˜è®¤200ï¼‰')
    # æ‰“ç ´å¥–åŠ±å¸¸æ•°æ­»åŒºï¼šé›¶åŠ¨ä½œæƒ©ç½šå‚æ•°åŒ–ï¼ˆæ”¯æŒè¯¾ç¨‹åŒ–è¡°å‡ï¼‰
    p.add_argument('--zero-action-penalty', type=float, default=0.0, help='å¯¹æ•´é›†å§‹ç»ˆé›¶åŠ¨ä½œçš„ç¨‹åºæ–½åŠ æƒ©ç½šï¼ˆåˆå§‹å€¼ï¼›0=æ— æƒ©ç½šï¼‰')
    p.add_argument('--zero-action-penalty-decay', type=float, default=0.95, help='é›¶åŠ¨ä½œæƒ©ç½šæ¯è½®è¡°å‡å› å­ï¼ˆ<1å¯ç”¨è¯¾ç¨‹åŒ–ï¼›1=ä¸è¡°å‡ï¼›é»˜è®¤0.95ï¼‰')
    p.add_argument('--zero-action-penalty-min', type=float, default=0.1, help='é›¶åŠ¨ä½œæƒ©ç½šæœ€å°å€¼ï¼ˆè¯¾ç¨‹åŒ–ä¸‹é™ï¼›é»˜è®¤0.1ï¼‰')
    p.add_argument('--action-scale-multiplier', type=float, default=1.0, help='åŠ¨ä½œè¾“å‡ºå…¨å±€ç¼©æ”¾ç³»æ•°ï¼ˆä¸´æ—¶ç”¨äºéªŒè¯æ˜¯å¦æ­»åŒºï¼›1=ä¸ç¼©æ”¾ï¼‰')
    p.add_argument('--enable-output-mad', dest='enable_output_mad', action='store_true', help='å¯ç”¨è¾“å‡ºMADå®‰å…¨å£³ï¼ˆå¹…å€¼/æ–¹å‘/å˜åŒ–ç‡çº¦æŸï¼‰')
    p.add_argument('--disable-output-mad', dest='enable_output_mad', action='store_false', help='ç¦ç”¨è¾“å‡ºMADå®‰å…¨å£³ï¼ˆä¸å»ºè®®ï¼‰')
    p.set_defaults(enable_output_mad=True)
    p.add_argument('--mad-min-fz', type=float, default=0.0, help='è¾“å‡ºå®‰å…¨å£³ï¼šu_fz æœ€å°å€¼ï¼ˆç‰›é¡¿ï¼‰')
    p.add_argument('--mad-max-fz', type=float, default=7.5, help='è¾“å‡ºå®‰å…¨å£³ï¼šu_fz æœ€å¤§å€¼ï¼ˆç‰›é¡¿ï¼‰')
    p.add_argument('--mad-max-xy', type=float, default=0.12, help='è¾“å‡ºå®‰å…¨å£³ï¼šæ¨ªå‘åŠ›çŸ©/åŠ›å¹…å€¼ä¸Šé™')
    p.add_argument('--mad-max-yaw', type=float, default=0.04, help='è¾“å‡ºå®‰å…¨å£³ï¼šyaw åŠ›çŸ©å¹…å€¼ä¸Šé™')
    p.add_argument('--mad-max-delta-fz', type=float, default=1.5, help='è¾“å‡ºå®‰å…¨å£³ï¼šç›¸é‚»æ­¥ u_fz æœ€å¤§å˜åŒ–é‡')
    p.add_argument('--mad-max-delta-xy', type=float, default=0.03, help='è¾“å‡ºå®‰å…¨å£³ï¼šç›¸é‚»æ­¥æ¨ªå‘åŠ›çŸ©å˜åŒ–ä¸Šé™')
    p.add_argument('--mad-max-delta-yaw', type=float, default=0.02, help='è¾“å‡ºå®‰å…¨å£³ï¼šç›¸é‚»æ­¥ yaw åŠ›çŸ©å˜åŒ–ä¸Šé™')
    
    # æ‚¬åœæ¨åŠ›çº¦æŸï¼ˆHover Thrust Constraintï¼‰
    p.add_argument('--enforce-hover-thrust', dest='enforce_hover_thrust', action='store_true',
                   help='å¯ç”¨æ‚¬åœæ¨åŠ›çº¦æŸï¼šå¼ºåˆ¶ u_fz = hover_thrust + deltaï¼Œç¡®ä¿æ— äººæœºå§‹ç»ˆæœ‰æœ€å°å‡åŠ›')
    p.add_argument('--no-enforce-hover-thrust', dest='enforce_hover_thrust', action='store_false',
                   help='ç¦ç”¨æ‚¬åœæ¨åŠ›çº¦æŸï¼ˆå…è®¸ç¨‹åºè¾“å‡ºé›¶æ¨åŠ›ï¼‰')
    p.set_defaults(enforce_hover_thrust=True)
    p.add_argument('--hover-thrust-value', type=float, default=0.265,
                   help='æ‚¬åœæ¨åŠ›åŸºç¡€å€¼ï¼ˆç‰›é¡¿ï¼‰ï¼ŒCrazyflie é»˜è®¤ 0.265N = 0.027kg Ã— 9.81m/sÂ²')
    p.add_argument('--hover-thrust-min', type=float, default=0.20,
                   help='æ‚¬åœæ¨åŠ›æœç´¢ä¸‹é™ï¼ˆç”¨äº BO ä¼˜åŒ–ï¼‰')
    p.add_argument('--hover-thrust-max', type=float, default=0.35,
                   help='æ‚¬åœæ¨åŠ›æœç´¢ä¸Šé™ï¼ˆç”¨äº BO ä¼˜åŒ–ï¼‰')
    p.add_argument('--hover-delta-max', type=float, default=2.0,
                   help='u_fz æ§åˆ¶å¢é‡çš„æœ€å¤§å¹…åº¦ï¼ˆç›¸å¯¹äºæ‚¬åœæ¨åŠ›çš„åç§»é‡ï¼‰')
    
    # Ranking Value Networkå‚æ•°ï¼ˆè‡ªé€‚åº”å¥–åŠ±å­¦ä¹ ï¼Œæ‰“ç ´å¹³å¦å¥–åŠ±å›°å¢ƒï¼‰
    p.add_argument('--use-ranking', type=lambda x: str(x).lower() in ['true', '1', 'yes'], default=True, 
                   help='å¯ç”¨Ranking Value Networkè¿›è¡Œè‡ªé€‚åº”å¥–åŠ±å­¦ä¹ ï¼ˆé»˜è®¤Trueï¼‰')
    p.add_argument('--ranking-lr', type=float, default=1e-3, help='Rankingç½‘ç»œå­¦ä¹ ç‡ï¼ˆé»˜è®¤1e-3ï¼‰')
    p.add_argument('--ranking-blend-init', type=float, default=0.3, help='Ranking valueåˆå§‹æ··åˆç³»æ•°ï¼ˆé»˜è®¤0.3ï¼‰')
    p.add_argument('--ranking-blend-max', type=float, default=0.8, help='Ranking valueæœ€å¤§æ··åˆç³»æ•°ï¼ˆé»˜è®¤0.8ï¼‰')
    p.add_argument('--ranking-blend-warmup', type=int, default=100, help='Rankingæ··åˆç³»æ•°warmupè½®æ•°ï¼ˆé»˜è®¤100ï¼‰')
    
    # ä»¿çœŸå‚æ•°ï¼ˆä»…Isaac Gymï¼‰
    # é»˜è®¤ç›´æ¥ä½¿ç”¨ safe-control-gym quadrotor_3D_track å¯¹é½é…ç½®
    p.add_argument('--traj', type=str, default='figure8', choices=['hover', 'figure8', 'circle', 'helix', 'square'])
    p.add_argument('--duration', type=int, default=5, help='ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä¸ safe-control-gym quadrotor_3D_track ä¸€è‡´')
    p.add_argument('--isaac-num-envs', type=int, default=512, help='Isaac Gymå¹¶è¡Œç¯å¢ƒæ•°')
    p.add_argument('--eval-replicas-per-program', type=int, default=5, help='evaluate_single æ—¶å¹¶è¡Œå‰¯æœ¬æ•°ï¼Œå–å¹³å‡ä»¥æé«˜åˆ©ç”¨ç‡/ç¨³å®šæ€§')
    p.add_argument('--min-steps-frac', type=float, default=0.0, help='æ¯æ¬¡è¯„ä¼°è‡³å°‘æ‰§è¡Œçš„æ­¥æ•°æ¯”ä¾‹ [0,1]ï¼Œé¿å…è¿‡æ—© done é€€å‡º')
    p.add_argument('--reward-reduction', type=str, default='sum', choices=['sum','mean'], help="å¥–åŠ±å½’çº¦æ–¹å¼ï¼š'sum'ï¼ˆæ­¥æ¬¡æ±‚å’Œï¼‰æˆ– 'mean'ï¼ˆæ­¥æ¬¡å¹³å‡ï¼‰")
    # ğŸ”¥ å¥–åŠ±æƒé‡é…ç½®ï¼šåªä¿ç•™ SCG å¯¹é½ç‰ˆæœ¬ï¼Œé¿å…æ··ä¹±
    p.add_argument('--reward-profile', type=str, default='safe_control_tracking',
                   choices=['safe_control_tracking'],
                   help='å¥–åŠ±æƒé‡é…ç½®æ–‡ä»¶ï¼ˆå”¯ä¸€ï¼‰ï¼šsafe_control_trackingï¼Œå¯¹é½ safe-control-gym quadrotor_3D_track')
    p.add_argument('--prior-profile', type=str, default='none', choices=list(PRIOR_PROFILES.keys()),
                   help='ç»“æ„/ç¨³å®šå…ˆéªŒå®éªŒåˆ†ç»„ï¼šnone(Aç»„)ã€structure(Bç»„)ã€structure_stability(Cç»„)')
    p.add_argument('--structure-prior-weight', type=float, default=None,
                   help='è¦†ç›–ç»“æ„å…ˆéªŒæƒé‡ï¼ˆé»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨ profile å†…ç½®å€¼ï¼‰')
    p.add_argument('--stability-prior-weight', type=float, default=None,
                   help='è¦†ç›–ç¨³å®šæ€§å…ˆéªŒæƒé‡ï¼ˆé»˜è®¤Noneè¡¨ç¤ºä½¿ç”¨ profile å†…ç½®å€¼ï¼‰')
    # AST-first pipeline switch
    p.add_argument('--ast-pipeline', action='store_true', help='å¯ç”¨ASTä¼˜å…ˆç®¡çº¿ï¼šå†…éƒ¨ç»Ÿä¸€ASTè¡¨ç¤ºï¼Œå¯¹å¤–åºåˆ—åŒ–ä¸ºdict')
    # Debug programs explored during MCTS
    p.add_argument('--debug-programs', action='store_true', help='è°ƒè¯•ï¼šæ‰“å°æœç´¢è¿‡ç¨‹ä¸­æ‰©å±•çš„ç¨‹åºæ‘˜è¦ï¼ˆä»…æ ¹ä¸å…¶ä¸‹ä¸€å±‚ï¼Œé™æ•°é‡ï¼‰')
    p.add_argument('--debug-programs-limit', type=int, default=20, help='è°ƒè¯•ç¨‹åºæ‰“å°æ¡æ•°ä¸Šé™ï¼ˆå…¨ç¨‹ç´¯ç§¯ï¼‰')
    p.add_argument('--use-fast-path', action='store_true', help='å¯ç”¨è¶…é«˜æ€§èƒ½ä¼˜åŒ–è·¯å¾„ï¼ˆç¯å¢ƒæ± å¤ç”¨+Numba JITç¼–è¯‘ï¼Œ7Ã—åŠ é€Ÿï¼‰')
    p.add_argument('--disable-gpu-expression', action='store_true', help='å…³é—­GPUè¡¨è¾¾å¼æ‰§è¡Œå™¨ï¼Œå›é€€åˆ°CPUæ±‚å€¼')
    p.add_argument('--prior-level', type=int, default=2, choices=[1, 2, 3], 
                   help='å…ˆéªŒçº§åˆ«: 1=æœ€é«˜çº¦æŸ(å•è§„åˆ™4é€šé“), 2=ä¸­åº¦(ä¿ç•™ä¸‰è½´+å§¿æ€), 3=ä¸¥æ ¼(ä»…ä½ç½®è¯¯å·®/é€Ÿåº¦/è§’é€Ÿåº¦)')
    
    # ğŸ”¥ è´å¶æ–¯ä¼˜åŒ–è°ƒå‚ï¼ˆå†…å±‚å‚æ•°ä¼˜åŒ–ï¼‰
    p.add_argument('--enable-bayesian-tuning', action='store_true', help='å¯ç”¨è´å¶æ–¯ä¼˜åŒ–å¯¹ç¨‹åºå¸¸æ•°å‚æ•°è¿›è¡Œè‡ªåŠ¨è°ƒä¼˜ï¼ˆAAAI 2024 Ï€-Lightç­–ç•¥ï¼‰')
    p.add_argument('--bo-batch-size', type=int, default=50, help='BOæ¯æ¬¡å¹¶è¡Œè¯„ä¼°çš„å‚æ•°ç»„æ•°ï¼ˆåˆ©ç”¨Isaacå¹¶è¡Œç¯å¢ƒï¼Œé»˜è®¤50ï¼‰')
    p.add_argument('--bo-iterations', type=int, default=3, help='BOè¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼Œæ€»è¯„ä¼° batch_size Ã— iterations ç»„å‚æ•°ï¼‰')
    
    # ä¿å­˜å‚æ•°
    p.add_argument('--save-path', type=str, default='01_soar/results/online_best_program.json')
    p.add_argument('--checkpoint-freq', type=int, default=50, help='æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆé»˜è®¤50ï¼‰')
    p.add_argument('--warm-start', type=str, default=None, help='ä»å·²æœ‰ç¨‹åºæ–‡ä»¶å¼€å§‹è®­ç»ƒï¼ˆJSON è·¯å¾„ï¼‰')
    p.add_argument('--elite-archive-size', type=int, default=50, help='ç²¾è‹±ç¨‹åºæ± å¤§å°,ä¿ç•™Top-Kæœ€ä¼˜ç¨‹åºï¼ˆé»˜è®¤50ï¼‰')
    # è¯¾ç¨‹å­¦ä¹  & ç¨‹åºæ¼”åŒ–æ—¥å¿—
    p.add_argument('--curriculum-mode', type=str, default='none', choices=['none','basic'], help='è¯¾ç¨‹å­¦ä¹ æ¨¡å¼: none=å…³é—­, basic=ä¸‰é˜¶æ®µå˜é‡/ç®—å­é€æ­¥è§£é”')
    p.add_argument('--program-history-path', type=str, default='01_soar/results/program_history.jsonl', help='ä¿å­˜ç¨‹åºæ¼”åŒ–å†å²(JSON Lines)ï¼Œä»…åœ¨å‡ºç°æ–°bestæ—¶è¿½åŠ ')
    # è°ƒè¯•/è¯Šæ–­
    p.add_argument('--debug-rewards', action='store_true', help='å¼€å¯é€æ­¥å¥–åŠ±ä¸é›¶åŠ¨ä½œç»Ÿè®¡çš„è°ƒè¯•æ—¥å¿—(å½±å“æ€§èƒ½)')
    
    return p.parse_args(args=argv)


if __name__ == '__main__':
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # å¼€å§‹è®­ç»ƒ
    trainer = OnlineTrainer(args)
    trainer.train()
