"""Soar æ ‡å‡†è¯„ä¼°æ¡†æ¶ã€‚

æä¾›ä¸ SB3 Zoo / OpenAI Spinning Up ä¸€è‡´çš„è¯„ä¼°æ–¹æ³•ï¼Œ
ç¡®ä¿è®ºæ–‡ä¸­çš„ baseline å¯¹æ¯”å…¬å¹³ä¸”å¯å¤ç°ã€‚

è¯„ä¼°æ ‡å‡†ï¼ˆå¯¹é½ OpenAI / SB3ï¼‰ï¼š
- 10 random seeds
- ç¡®å®šæ€§è¯„ä¼°ï¼ˆno exploration noiseï¼‰
- æŠ¥å‘Š mean Â± std

ç‰©ç†æŒ‡æ ‡ï¼ˆQuadrotor ä¸“ç”¨ï¼‰ï¼š
- Position RMSE (m)
- Max Deviation (m)
- Velocity RMSE (m/s)
- Control Effort (âˆ‘|u|Â²)
- Success Rate (%)
- Settling Time (s)

ä½¿ç”¨æ–¹å¼:
    from scripts.evaluation import StandardEvaluator
    
    evaluator = StandardEvaluator(trajectory='figure8', n_seeds=10)
    
    # è¯„ä¼° SB3 æ¨¡å‹
    sb3_results = evaluator.evaluate_sb3_model('results/sb3/ppo_figure8.zip')
    
    # è¯„ä¼° Soar ç¨‹åº
    soar_results = evaluator.evaluate_soar_program('results/best_program.json')
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    evaluator.generate_comparison_table([sb3_results, soar_results])
"""
from __future__ import annotations

import json
import os
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
_project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(_project_root))
sys.path.insert(0, str(_project_root / "01_soar"))


@dataclass
class EvalMetrics:
    """è¯„ä¼°æŒ‡æ ‡æ•°æ®ç±»"""
    # ç‰©ç†æŒ‡æ ‡
    position_rmse: float = 0.0       # ä½ç½® RMSE (m)
    position_rmse_std: float = 0.0
    max_deviation: float = 0.0       # æœ€å¤§åå·® (m)
    max_deviation_std: float = 0.0
    velocity_rmse: float = 0.0       # é€Ÿåº¦ RMSE (m/s)
    velocity_rmse_std: float = 0.0
    
    # æ§åˆ¶æŒ‡æ ‡
    control_effort: float = 0.0      # æ§åˆ¶ä»£ä»· (âˆ‘|u|Â²)
    control_effort_std: float = 0.0
    smoothness: float = 0.0          # å¹³æ»‘åº¦ (jerk)
    smoothness_std: float = 0.0
    
    # ä»»åŠ¡æŒ‡æ ‡
    success_rate: float = 0.0        # æˆåŠŸç‡ (%)
    settling_time: float = 0.0       # ç¨³å®šæ—¶é—´ (s)
    settling_time_std: float = 0.0
    episode_return: float = 0.0      # Episode Return (ç”¨äº SB3 å¯¹æ¯”)
    episode_return_std: float = 0.0
    
    # å…ƒä¿¡æ¯
    n_episodes: int = 0
    n_seeds: int = 0
    method_name: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'position_rmse': f"{self.position_rmse:.4f} Â± {self.position_rmse_std:.4f}",
            'max_deviation': f"{self.max_deviation:.4f} Â± {self.max_deviation_std:.4f}",
            'velocity_rmse': f"{self.velocity_rmse:.4f} Â± {self.velocity_rmse_std:.4f}",
            'control_effort': f"{self.control_effort:.2f} Â± {self.control_effort_std:.2f}",
            'smoothness': f"{self.smoothness:.4f} Â± {self.smoothness_std:.4f}",
            'success_rate': f"{self.success_rate:.1f}%",
            'settling_time': f"{self.settling_time:.3f} Â± {self.settling_time_std:.3f}",
            'episode_return': f"{self.episode_return:.1f} Â± {self.episode_return_std:.1f}",
            'n_episodes': self.n_episodes,
            'n_seeds': self.n_seeds,
            'method': self.method_name,
        }


# =============================================================================
# OpenAI / SB3 æ ‡å‡†è¯„ä¼°é…ç½®
# =============================================================================

OPENAI_SPINUP_CONFIG = {
    'n_seeds': 10,
    'total_timesteps': 3_000_000,
    'eval_freq': 10_000,
    'n_eval_episodes': 10,
    'deterministic': True,  # è¯„ä¼°æ—¶ä¸åŠ å™ªå£°
    'network_on_policy': [64, 32],   # PPO/A2C
    'network_off_policy': [256, 256],  # SAC/TD3
    'activation_on_policy': 'tanh',
    'activation_off_policy': 'relu',
}

SB3_ZOO_CONFIG = {
    'n_seeds': 5,
    'total_timesteps': 1_000_000,
    'eval_freq': 10_000,
    'n_eval_episodes': 50,
    'deterministic': True,
}

# è®ºæ–‡æ¨èé…ç½®ï¼ˆå¹³è¡¡ç²¾åº¦å’Œè®¡ç®—æˆæœ¬ï¼‰
PAPER_CONFIG = {
    'n_seeds': 10,
    'n_eval_episodes': 20,
    'deterministic': True,
    'report_mean_std': True,
}


# =============================================================================
# SB3 Zoo Benchmark å‚è€ƒæ•°æ®
# =============================================================================

SB3_ZOO_BENCHMARKS = {
    # MuJoCo ç¯å¢ƒ (è®­ç»ƒ 1M steps)
    'mujoco': {
        'HalfCheetah-v3': {
            'PPO': (5819, 664),
            'SAC': (9535, 100),
            'TD3': (9656, 970),
            'TQC': (12090, 127),
        },
        'Hopper-v3': {
            'PPO': (2410, 10),
            'SAC': (2326, 1130),
            'TD3': (3606, 4),
            'TQC': (3754, 8),
        },
        'Walker2d-v3': {
            'PPO': (3479, 822),
            'SAC': (3863, 254),
            'TD3': (4718, 46),
            'TQC': (4381, 500),
        },
        'Ant-v3': {
            'PPO': (1327, 452),
            'SAC': (4616, 1354),
            'TD3': (5813, 590),
        },
    },
    # PyBullet ç¯å¢ƒï¼ˆå…è´¹ MuJoCo æ›¿ä»£ï¼‰
    'pybullet': {
        'HalfCheetahBulletEnv-v0': {
            'PPO': (2925, 64),
            'SAC': (2792, 12),
            'TD3': (2822, 20),
        },
        'HopperBulletEnv-v0': {
            'PPO': (2575, 223),
            'SAC': (2603, 164),
            'TD3': (2682, 28),
        },
        'Walker2DBulletEnv-v0': {
            'PPO': (2110, 14),
            'SAC': (2292, 14),
            'TD3': (2214, 231),
        },
        'AntBulletEnv-v0': {
            'PPO': (2866, 56),
            'SAC': (3073, 175),
            'TD3': (3300, 55),
        },
    },
}


class StandardEvaluator:
    """æ ‡å‡†è¯„ä¼°å™¨ï¼šå¯¹é½ OpenAI / SB3 è¯„ä¼°æ–¹æ³•ã€‚
    
    æä¾›å…¬å¹³ã€å¯å¤ç°çš„ baseline å¯¹æ¯”ã€‚
    """
    
    def __init__(
        self,
        trajectory: str = 'figure8',
        duration: float = 5.0,
        n_seeds: int = 10,
        n_eval_episodes: int = 20,
        deterministic: bool = True,
        reward_type: str = 'pybullet_drones',
        device: str = 'cuda:0',
    ):
        """
        Args:
            trajectory: è½¨è¿¹ç±»å‹
            duration: Episode æ—¶é•¿
            n_seeds: éšæœºç§å­æ•°é‡ï¼ˆå¯¹é½ OpenAI æ ‡å‡†ï¼‰
            n_eval_episodes: æ¯ä¸ª seed çš„è¯„ä¼° episode æ•°
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥ï¼ˆæ— æ¢ç´¢å™ªå£°ï¼‰
            reward_type: Reward ç±»å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.trajectory = trajectory
        self.duration = duration
        self.n_seeds = n_seeds
        self.n_eval_episodes = n_eval_episodes
        self.deterministic = deterministic
        self.reward_type = reward_type
        self.device = device
        
        # æ§åˆ¶é¢‘ç‡
        self.control_freq = 50
        self.dt = 1.0 / self.control_freq
        
        # æˆåŠŸåˆ¤å®šé˜ˆå€¼
        self.success_threshold = 0.1  # ä½ç½®è¯¯å·® < 0.1m è§†ä¸ºæˆåŠŸ
        self.settling_threshold = 0.05  # ç¨³å®šåˆ¤å®šé˜ˆå€¼
    
    def evaluate_sb3_model(
        self,
        model_path: str,
        algo: str = 'ppo',
    ) -> EvalMetrics:
        """è¯„ä¼° SB3 è®­ç»ƒçš„æ¨¡å‹ã€‚
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.zip)
            algo: ç®—æ³•ç±»å‹
        
        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        try:
            from stable_baselines3 import PPO, SAC, TD3
            from scripts.sb3.quadrotor_env import QuadrotorTrackingEnv
        except ImportError as e:
            print(f"âŒ SB3 æœªå®‰è£…: {e}")
            return EvalMetrics(method_name=f"SB3-{algo.upper()}")
        
        algo_map = {'ppo': PPO, 'sac': SAC, 'td3': TD3}
        if algo.lower() not in algo_map:
            print(f"âŒ ä¸æ”¯æŒçš„ç®—æ³•: {algo}")
            return EvalMetrics(method_name=f"SB3-{algo.upper()}")
        
        AlgoClass = algo_map[algo.lower()]
        
        # æ”¶é›†æ‰€æœ‰ seed çš„ç»“æœ
        all_results = []
        
        for seed in range(self.n_seeds):
            # åˆ›å»ºç¯å¢ƒ
            env = QuadrotorTrackingEnv(
                trajectory=self.trajectory,
                duration=self.duration,
                control_freq=self.control_freq,
            )
            
            # åŠ è½½æ¨¡å‹
            model = AlgoClass.load(model_path, env=env)
            
            # è¯„ä¼°
            seed_results = self._evaluate_policy(
                env=env,
                predict_fn=lambda obs: model.predict(obs, deterministic=self.deterministic)[0],
                seed=seed,
            )
            all_results.append(seed_results)
            env.close()
        
        # èšåˆç»“æœ
        metrics = self._aggregate_results(all_results, f"SB3-{algo.upper()}")
        return metrics
    
    def evaluate_soar_program(
        self,
        program_path: str,
    ) -> EvalMetrics:
        """è¯„ä¼° Soar ç”Ÿæˆçš„æ§åˆ¶ç¨‹åºã€‚
        
        Args:
            program_path: ç¨‹åº JSON æ–‡ä»¶è·¯å¾„
        
        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        try:
            from scripts.sb3.soar_policy import SoarPolicy
            from scripts.sb3.quadrotor_env import QuadrotorTrackingEnv
        except ImportError as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return EvalMetrics(method_name="Soar")
        
        # åŠ è½½ç¨‹åº
        policy = SoarPolicy(program_path)
        
        # æ”¶é›†æ‰€æœ‰ seed çš„ç»“æœ
        all_results = []
        
        for seed in range(self.n_seeds):
            env = QuadrotorTrackingEnv(
                trajectory=self.trajectory,
                duration=self.duration,
                control_freq=self.control_freq,
            )
            
            seed_results = self._evaluate_policy(
                env=env,
                predict_fn=policy.predict,
                seed=seed,
                reset_fn=policy.reset,
            )
            all_results.append(seed_results)
            env.close()
        
        metrics = self._aggregate_results(all_results, "Soar")
        return metrics
    
    def evaluate_pid_baseline(self) -> EvalMetrics:
        """è¯„ä¼° PID baselineã€‚"""
        try:
            from scripts.sb3.quadrotor_env import QuadrotorTrackingEnv
        except ImportError as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return EvalMetrics(method_name="PID")
        
        # ç®€å• PD æ§åˆ¶å™¨
        kp_pos = 2.0
        kd_pos = 0.5
        
        def pid_predict(obs):
            pos_err = obs[:3]
            vel = obs[3:6]
            
            # PD æ§åˆ¶
            fz = 0.265 + kp_pos * pos_err[2] - kd_pos * vel[2]  # hover + z control
            tx = kp_pos * pos_err[1] - kd_pos * vel[1]  # roll -> y
            ty = -kp_pos * pos_err[0] + kd_pos * vel[0]  # pitch -> x
            tz = 0.0  # yaw
            
            # å½’ä¸€åŒ–
            action = np.array([
                (fz - 3.5) / 3.5,
                tx / 0.12,
                ty / 0.12,
                tz / 0.06,
            ], dtype=np.float32)
            return np.clip(action, -1, 1)
        
        all_results = []
        for seed in range(self.n_seeds):
            env = QuadrotorTrackingEnv(
                trajectory=self.trajectory,
                duration=self.duration,
                control_freq=self.control_freq,
            )
            seed_results = self._evaluate_policy(env, pid_predict, seed)
            all_results.append(seed_results)
            env.close()
        
        return self._aggregate_results(all_results, "PID")
    
    def _evaluate_policy(
        self,
        env,
        predict_fn,
        seed: int,
        reset_fn=None,
    ) -> Dict[str, List[float]]:
        """è¯„ä¼°å•ä¸ª policy åœ¨ä¸€ä¸ª seed ä¸‹çš„è¡¨ç°ã€‚
        
        Returns:
            å„æŒ‡æ ‡çš„åˆ—è¡¨ï¼ˆæ¯ä¸ª episode ä¸€ä¸ªå€¼ï¼‰
        """
        np.random.seed(seed)
        
        results = {
            'position_rmse': [],
            'max_deviation': [],
            'velocity_rmse': [],
            'control_effort': [],
            'smoothness': [],
            'episode_return': [],
            'success': [],
            'settling_time': [],
        }
        
        for ep in range(self.n_eval_episodes):
            obs, info = env.reset(seed=seed * 1000 + ep)
            if reset_fn:
                reset_fn()
            
            pos_errors = []
            vel_errors = []
            actions = []
            rewards = []
            done = False
            truncated = False
            
            while not (done or truncated):
                action = predict_fn(obs)
                obs, reward, done, truncated, info = env.step(action)
                
                pos_errors.append(np.linalg.norm(obs[:3]))
                vel_errors.append(np.linalg.norm(obs[3:6]))
                actions.append(action)
                rewards.append(reward)
            
            # è®¡ç®—æŒ‡æ ‡
            pos_errors = np.array(pos_errors)
            vel_errors = np.array(vel_errors)
            actions = np.array(actions)
            
            results['position_rmse'].append(np.sqrt(np.mean(pos_errors ** 2)))
            results['max_deviation'].append(np.max(pos_errors))
            results['velocity_rmse'].append(np.sqrt(np.mean(vel_errors ** 2)))
            results['control_effort'].append(np.sum(actions ** 2))
            results['episode_return'].append(np.sum(rewards))
            results['success'].append(float(pos_errors[-1] < self.success_threshold))
            
            # è®¡ç®— settling time
            settling_idx = len(pos_errors)
            for i in range(len(pos_errors) - 1, -1, -1):
                if pos_errors[i] > self.settling_threshold:
                    settling_idx = i + 1
                    break
            results['settling_time'].append(settling_idx * self.dt)
            
            # è®¡ç®— smoothness (jerk)
            if len(actions) > 2:
                jerk = np.diff(actions, axis=0, n=2)
                results['smoothness'].append(np.mean(np.abs(jerk)))
            else:
                results['smoothness'].append(0.0)
        
        return results
    
    def _aggregate_results(
        self,
        all_results: List[Dict[str, List[float]]],
        method_name: str,
    ) -> EvalMetrics:
        """èšåˆå¤šä¸ª seed çš„ç»“æœã€‚"""
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        merged = {}
        for key in all_results[0].keys():
            merged[key] = []
            for seed_results in all_results:
                merged[key].extend(seed_results[key])
        
        # è®¡ç®— mean Â± std
        metrics = EvalMetrics(
            position_rmse=np.mean(merged['position_rmse']),
            position_rmse_std=np.std(merged['position_rmse']),
            max_deviation=np.mean(merged['max_deviation']),
            max_deviation_std=np.std(merged['max_deviation']),
            velocity_rmse=np.mean(merged['velocity_rmse']),
            velocity_rmse_std=np.std(merged['velocity_rmse']),
            control_effort=np.mean(merged['control_effort']),
            control_effort_std=np.std(merged['control_effort']),
            smoothness=np.mean(merged['smoothness']),
            smoothness_std=np.std(merged['smoothness']),
            success_rate=100.0 * np.mean(merged['success']),
            settling_time=np.mean(merged['settling_time']),
            settling_time_std=np.std(merged['settling_time']),
            episode_return=np.mean(merged['episode_return']),
            episode_return_std=np.std(merged['episode_return']),
            n_episodes=len(merged['position_rmse']),
            n_seeds=len(all_results),
            method_name=method_name,
        )
        
        return metrics
    
    def generate_comparison_table(
        self,
        results: List[EvalMetrics],
        output_format: str = 'markdown',
    ) -> str:
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ã€‚
        
        Args:
            results: å„æ–¹æ³•çš„è¯„ä¼°ç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ('markdown', 'latex', 'csv')
        
        Returns:
            æ ¼å¼åŒ–çš„è¡¨æ ¼å­—ç¬¦ä¸²
        """
        if output_format == 'markdown':
            return self._generate_markdown_table(results)
        elif output_format == 'latex':
            return self._generate_latex_table(results)
        else:
            return self._generate_csv_table(results)
    
    def _generate_markdown_table(self, results: List[EvalMetrics]) -> str:
        """ç”Ÿæˆ Markdown è¡¨æ ¼"""
        lines = [
            f"## è¯„ä¼°ç»“æœå¯¹æ¯” ({self.trajectory})",
            "",
            f"è¯„ä¼°è®¾ç½®: {self.n_seeds} seeds Ã— {self.n_eval_episodes} episodes, deterministic={self.deterministic}",
            "",
            "| Method | Pos RMSE (m) | Max Dev (m) | Ctrl Effort | Success (%) | Return |",
            "|--------|--------------|-------------|-------------|-------------|--------|",
        ]
        
        for r in results:
            lines.append(
                f"| {r.method_name} | "
                f"{r.position_rmse:.4f}Â±{r.position_rmse_std:.4f} | "
                f"{r.max_deviation:.4f}Â±{r.max_deviation_std:.4f} | "
                f"{r.control_effort:.1f}Â±{r.control_effort_std:.1f} | "
                f"{r.success_rate:.1f} | "
                f"{r.episode_return:.1f}Â±{r.episode_return_std:.1f} |"
            )
        
        return "\n".join(lines)
    
    def _generate_latex_table(self, results: List[EvalMetrics]) -> str:
        """ç”Ÿæˆ LaTeX è¡¨æ ¼"""
        lines = [
            r"\begin{table}[h]",
            r"\centering",
            f"\\caption{{Evaluation Results on {self.trajectory} Trajectory}}",
            r"\begin{tabular}{lccccc}",
            r"\toprule",
            r"Method & Pos RMSE (m) & Max Dev (m) & Ctrl Effort & Success (\%) & Return \\",
            r"\midrule",
        ]
        
        for r in results:
            lines.append(
                f"{r.method_name} & "
                f"${r.position_rmse:.4f} \\pm {r.position_rmse_std:.4f}$ & "
                f"${r.max_deviation:.4f} \\pm {r.max_deviation_std:.4f}$ & "
                f"${r.control_effort:.1f} \\pm {r.control_effort_std:.1f}$ & "
                f"{r.success_rate:.1f} & "
                f"${r.episode_return:.1f} \\pm {r.episode_return_std:.1f}$ \\\\"
            )
        
        lines.extend([
            r"\bottomrule",
            r"\end{tabular}",
            r"\end{table}",
        ])
        
        return "\n".join(lines)
    
    def _generate_csv_table(self, results: List[EvalMetrics]) -> str:
        """ç”Ÿæˆ CSV è¡¨æ ¼"""
        lines = ["method,pos_rmse,pos_rmse_std,max_dev,max_dev_std,ctrl_effort,ctrl_effort_std,success_rate,return,return_std"]
        
        for r in results:
            lines.append(
                f"{r.method_name},{r.position_rmse:.6f},{r.position_rmse_std:.6f},"
                f"{r.max_deviation:.6f},{r.max_deviation_std:.6f},"
                f"{r.control_effort:.2f},{r.control_effort_std:.2f},"
                f"{r.success_rate:.2f},{r.episode_return:.2f},{r.episode_return_std:.2f}"
            )
        
        return "\n".join(lines)


def run_full_evaluation(
    trajectory: str = 'figure8',
    sb3_model_path: Optional[str] = None,
    soar_program_path: Optional[str] = None,
    include_pid: bool = True,
    output_dir: str = 'results/evaluation',
):
    """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”è¯„ä¼°ã€‚
    
    Args:
        trajectory: è½¨è¿¹ç±»å‹
        sb3_model_path: SB3 æ¨¡å‹è·¯å¾„
        soar_program_path: Soar ç¨‹åºè·¯å¾„
        include_pid: æ˜¯å¦åŒ…å« PID baseline
        output_dir: è¾“å‡ºç›®å½•
    """
    print("=" * 60)
    print(f"æ ‡å‡†è¯„ä¼°æ¡†æ¶ (OpenAI / SB3 å¯¹é½)")
    print(f"è½¨è¿¹: {trajectory}")
    print("=" * 60)
    
    evaluator = StandardEvaluator(
        trajectory=trajectory,
        n_seeds=10,
        n_eval_episodes=20,
    )
    
    results = []
    
    # è¯„ä¼° PID baseline
    if include_pid:
        print("\nğŸ“Š è¯„ä¼° PID baseline...")
        pid_results = evaluator.evaluate_pid_baseline()
        results.append(pid_results)
        print(f"   Position RMSE: {pid_results.position_rmse:.4f} Â± {pid_results.position_rmse_std:.4f}")
    
    # è¯„ä¼° SB3 æ¨¡å‹
    if sb3_model_path and os.path.exists(sb3_model_path):
        print(f"\nğŸ“Š è¯„ä¼° SB3 æ¨¡å‹: {sb3_model_path}")
        sb3_results = evaluator.evaluate_sb3_model(sb3_model_path)
        results.append(sb3_results)
        print(f"   Position RMSE: {sb3_results.position_rmse:.4f} Â± {sb3_results.position_rmse_std:.4f}")
    
    # è¯„ä¼° Soar ç¨‹åº
    if soar_program_path and os.path.exists(soar_program_path):
        print(f"\nğŸ“Š è¯„ä¼° Soar ç¨‹åº: {soar_program_path}")
        soar_results = evaluator.evaluate_soar_program(soar_program_path)
        results.append(soar_results)
        print(f"   Position RMSE: {soar_results.position_rmse:.4f} Â± {soar_results.position_rmse_std:.4f}")
    
    # ç”ŸæˆæŠ¥å‘Š
    if results:
        print("\n" + "=" * 60)
        print(evaluator.generate_comparison_table(results, 'markdown'))
        
        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Markdown
        with open(output_path / f"eval_{trajectory}.md", 'w') as f:
            f.write(evaluator.generate_comparison_table(results, 'markdown'))
        
        # LaTeX
        with open(output_path / f"eval_{trajectory}.tex", 'w') as f:
            f.write(evaluator.generate_comparison_table(results, 'latex'))
        
        # CSV
        with open(output_path / f"eval_{trajectory}.csv", 'w') as f:
            f.write(evaluator.generate_comparison_table(results, 'csv'))
        
        # JSON (å®Œæ•´æ•°æ®)
        with open(output_path / f"eval_{trajectory}.json", 'w') as f:
            json.dump([r.to_dict() for r in results], f, indent=2)
        
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return results


# =============================================================================
# å­¦æœ¯å¼•ç”¨æ ¼å¼
# =============================================================================

CITATIONS = {
    'sb3_zoo': r"""
@misc{rl-zoo3,
  author = {Raffin, Antonin},
  title = {RL Baselines3 Zoo},
  year = {2020},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/DLR-RM/rl-baselines3-zoo}},
}
""",
    'stable_baselines3': r"""
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
""",
    'ppo': r"""
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
""",
    'sac': r"""
@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
""",
    'gym_pybullet_drones': r"""
@misc{panerati2021learning,
  title={Learning to Fly -- a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control}, 
  author={Jacopo Panerati and Hehui Zheng and SiQi Zhou and James Xu and Amanda Prorok and Angela P. Schoellig},
  year={2021},
  eprint={2103.02142},
  archivePrefix={arXiv},
  primaryClass={cs.RO}
}
""",
}


def print_citations():
    """æ‰“å°è®ºæ–‡å¼•ç”¨æ ¼å¼"""
    print("=" * 60)
    print("å­¦æœ¯å¼•ç”¨æ ¼å¼")
    print("=" * 60)
    for name, citation in CITATIONS.items():
        print(f"\n### {name}")
        print(citation)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Soar æ ‡å‡†è¯„ä¼°æ¡†æ¶")
    parser.add_argument("--trajectory", type=str, default="figure8",
                        choices=["hover", "figure8", "circle"])
    parser.add_argument("--sb3-model", type=str, default=None,
                        help="SB3 æ¨¡å‹è·¯å¾„")
    parser.add_argument("--soar-program", type=str, default=None,
                        help="Soar ç¨‹åºè·¯å¾„")
    parser.add_argument("--no-pid", action="store_true",
                        help="ä¸è¯„ä¼° PID baseline")
    parser.add_argument("--output-dir", type=str, default="results/evaluation")
    parser.add_argument("--citations", action="store_true",
                        help="æ‰“å°å¼•ç”¨æ ¼å¼")
    
    args = parser.parse_args()
    
    if args.citations:
        print_citations()
    else:
        run_full_evaluation(
            trajectory=args.trajectory,
            sb3_model_path=args.sb3_model,
            soar_program_path=args.soar_program,
            include_pid=not args.no_pid,
            output_dir=args.output_dir,
        )
