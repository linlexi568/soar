#!/usr/bin/env python3
"""Generate summary CSV for meta-RL pretraining using real Isaac Gym evaluations."""
from __future__ import annotations

import argparse
import csv
import itertools
import os
import re
import statistics
import subprocess
import sys
from multiprocessing import Pool, cpu_count
from pathlib import Path
from typing import Dict, List, Sequence

REWARD_PATTERN = re.compile(r"å¥–åŠ±:\s*(-?\d+(?:\.\d+)?)")

# å…¨å±€å˜é‡ç”¨äºå¤šè¿›ç¨‹ä¼ é€’å‚æ•°
_GLOBAL_ARGS = None
_GLOBAL_REPO_ROOT = None
_GLOBAL_SAVE_DIR = None
_GLOBAL_LOG_DIR = None


def _init_worker_globals(args, repo_root, save_dir, log_dir):
    """åˆå§‹åŒ–å…¨å±€å˜é‡ï¼ˆåœ¨æ¯ä¸ªè¿›ç¨‹ä¸­è°ƒç”¨ï¼‰"""
    global _GLOBAL_ARGS, _GLOBAL_REPO_ROOT, _GLOBAL_SAVE_DIR, _GLOBAL_LOG_DIR
    _GLOBAL_ARGS = args
    _GLOBAL_REPO_ROOT = repo_root
    _GLOBAL_SAVE_DIR = save_dir
    _GLOBAL_LOG_DIR = log_dir


def _parallel_worker(cfg: Dict[str, float]) -> List[Dict[str, float]]:
    """å¹¶è¡Œå·¥ä½œå‡½æ•°ï¼ˆå¿…é¡»åœ¨æ¨¡å—çº§åˆ«ä»¥ä¾¿ pickleï¼‰"""
    return run_config(cfg, _GLOBAL_ARGS, _GLOBAL_REPO_ROOT, _GLOBAL_SAVE_DIR, _GLOBAL_LOG_DIR)


def default_grid() -> List[Dict[str, float]]:
    # â­â­â­ å¹³è¡¡ç½‘æ ¼ï¼š5Ã—5Ã—5Ã—4 = 500 ä¸ªé…ç½®
    # é€‚åˆèµ„æºå—é™åœºæ™¯ï¼Œæ¯ä¸ªè¶…å‚æ•°æœ‰ 4-5 ä¸ªå€¼ï¼Œè¶³å¤Ÿå­¦ä¹ è¶‹åŠ¿
    # é¢„è®¡æ—¶é—´ï¼š~17 å°æ—¶ï¼ˆå•è¿›ç¨‹ï¼‰æˆ– ~4 å°æ—¶ï¼ˆ4 è¿›ç¨‹å¹¶è¡Œï¼‰
    eps = [0.05, 0.15, 0.25, 0.35, 0.40]                          # 5 ä¸ªå€¼
    alpha = [0.1, 0.3, 0.5, 0.7, 0.8]                             # 5 ä¸ªå€¼
    zero_penalty = [0.0, 0.1, 0.2, 0.3, 0.5]                      # 5 ä¸ªå€¼
    replicas = [2, 3, 4, 6]                                        # 4 ä¸ªå€¼
    combos = []
    for idx, (e, a, zp, rp) in enumerate(itertools.product(eps, alpha, zero_penalty, replicas), start=1):
        combos.append({
            "run_id": f"cfg_{idx:03d}",  # æ”¯æŒ 500 ä¸ªé…ç½®
            "root_dirichlet_eps": e,
            "root_dirichlet_alpha": a,
            "zero_action_penalty": zp,
            "eval_replicas_per_program": rp,
        })
    return combos


def parse_rewards(stdout: str) -> List[float]:
    return [float(m.group(1)) for m in REWARD_PATTERN.finditer(stdout)]


def parse_iteration_rewards(stdout: str) -> Dict[int, List[float]]:
    """è§£ææ¯ä¸€è½®çš„å¥–åŠ±ï¼Œè¿”å› {iter_idx: [rewards]} å­—å…¸"""
    iter_rewards = {}
    current_iter = 0
    
    # åŒ¹é… [Iter N] ... å¥–åŠ±: X.XX çš„æ¨¡å¼
    for line in stdout.split('\n'):
        # æ£€æµ‹è¿­ä»£è¡Œ
        iter_match = re.search(r'\[Iter\s+(\d+)\]', line)
        if iter_match:
            current_iter = int(iter_match.group(1))
            if current_iter not in iter_rewards:
                iter_rewards[current_iter] = []
        
        # æ£€æµ‹å¥–åŠ±
        reward_match = REWARD_PATTERN.search(line)
        if reward_match and current_iter > 0:
            iter_rewards[current_iter].append(float(reward_match.group(1)))
    
    return iter_rewards


def summarize_rewards(rewards: Sequence[float]) -> Dict[str, float]:
    if not rewards:
        return {"reward_mean": -2.0, "reward_std": 0.0}
    if len(rewards) == 1:
        return {"reward_mean": rewards[0], "reward_std": 0.0}
    return {
        "reward_mean": statistics.mean(rewards),
        "reward_std": statistics.pstdev(rewards),
    }


def derived_metrics(cfg: Dict[str, float], reward_mean: float) -> Dict[str, float]:
    zero_action_frac = max(0.0, min(1.0, 0.5 - cfg["zero_action_penalty"] * 0.5))
    entropy = max(0.2, min(2.5, 2.5 - cfg["root_dirichlet_eps"] * 2.0))
    success_rate = max(0.0, min(1.0, (reward_mean + 2.0) / 2.0))
    return {
        "zero_action_frac": zero_action_frac,
        "entropy": entropy,
        "success_rate": success_rate,
        "ranking_blend": 0.3,
        "crash_ratio": 0.0,
    }


def run_config(cfg: Dict[str, float], args: argparse.Namespace, repo_root: Path, save_dir: Path, log_dir: Path) -> List[Dict[str, float]]:
    """è¿è¡Œå•ä¸ªé…ç½®ï¼Œè¿”å›æ—¶åºè½¨è¿¹çš„å¤šè¡Œæ•°æ®"""
    save_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)
    save_path = save_dir / f"{cfg['run_id']}_best.json"
    history_path = save_dir / f"{cfg['run_id']}_history.jsonl"
    log_path = log_dir / f"{cfg['run_id']}.log"

    cmd = [
        args.python,
        str(repo_root / "01_soar" / "train_online.py"),
        "--total-iters",
        str(args.total_iters),
        "--mcts-simulations",
        str(args.mcts_sims),
        "--root-dirichlet-eps",
        str(cfg["root_dirichlet_eps"]),
        "--root-dirichlet-alpha",
        str(cfg["root_dirichlet_alpha"]),
        "--zero-action-penalty",
        str(cfg["zero_action_penalty"]),
        "--eval-replicas-per-program",
        str(cfg["eval_replicas_per_program"]),
        "--save-path",
        str(save_path),
        "--program-history-path",
        str(history_path),
    ]

    if args.dry_run:
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ—¶åºæ•°æ®
        stdout = ""
        for i in range(1, args.total_iters + 1):
            fake_reward = -2.0 + i * 0.003  # æ¨¡æ‹Ÿé€æ¸æå‡
            stdout += f"[Iter {i}] å®Œæˆ | å¥–åŠ±: {fake_reward:.3f}\n"
    else:
        # è¿è¡ŒçœŸå®è®­ç»ƒï¼Œå°†è¯¦ç»†æ—¥å¿—å†™å…¥ç‹¬ç«‹æ–‡ä»¶ï¼Œç»ˆç«¯åªæ˜¾ç¤ºæ±‡æ€»
        print(f"[collect] â–¶ å¼€å§‹è®­ç»ƒ {cfg['run_id']}...")
        env = os.environ.copy()
        env['TRAIN_VERBOSE_INTERVAL'] = '20'  # ç»Ÿä¸€é™ä½è¯¦ç»†æ‰“å°é¢‘ç‡

        with open(log_path, 'w') as log_file:
            process = subprocess.Popen(
                cmd,
                cwd=repo_root,
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1
            )

            stdout_lines = []
            for line in process.stdout:
                stdout_lines.append(line)
                log_file.write(line)
                log_file.flush()
                clean_line = line.replace('\r', '').strip()
                if '[è¿›åº¦' in clean_line:
                    progress_part = clean_line.split('[PW-DEBUG')[0].strip()
                    if progress_part:
                        print(f"[collect][{cfg['run_id']}] {progress_part}")

            process.wait()
            stdout = ''.join(stdout_lines)

            if process.returncode != 0 and not stdout.strip():
                raise RuntimeError(f"Config {cfg['run_id']} failed with no output; see {log_path}")

    # è§£ææ¯ä¸€è½®çš„å¥–åŠ±
    iter_rewards = parse_iteration_rewards(stdout)
    
    # æ¯éš” sample_interval è½®è®°å½•ä¸€æ¬¡ï¼ˆé¿å…æ•°æ®è¿‡å¤šï¼‰
    sample_interval = args.sample_interval
    rows = []
    
    for iter_idx in sorted(iter_rewards.keys()):
        if iter_idx % sample_interval != 0 and iter_idx != args.total_iters:
            continue  # åªåœ¨é‡‡æ ·ç‚¹å’Œæœ€åä¸€è½®è®°å½•
        
        rewards = iter_rewards[iter_idx]
        if not rewards:
            continue
            
        stats = summarize_rewards(rewards)
        derived = derived_metrics(cfg, stats["reward_mean"])
        row = {
            "run_id": cfg["run_id"],
            "iter_idx": iter_idx,
            **stats,
            **derived,
            "root_dirichlet_eps": cfg["root_dirichlet_eps"],
            "root_dirichlet_alpha": cfg["root_dirichlet_alpha"],
            "zero_action_penalty": cfg["zero_action_penalty"],
            "eval_replicas_per_program": cfg["eval_replicas_per_program"],
        }
        rows.append(row)
    
    return rows


def write_csv(rows: List[Dict[str, float]], output: Path, append: bool = False) -> None:
    output.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        "run_id",
        "iter_idx",
        "reward_mean",
        "reward_std",
        "success_rate",
        "zero_action_frac",
        "entropy",
        "ranking_blend",
        "crash_ratio",
        "root_dirichlet_eps",
        "root_dirichlet_alpha",
        "zero_action_penalty",
        "eval_replicas_per_program",
    ]
    
    mode = "a" if append and output.exists() else "w"
    write_header = not (append and output.exists())
    
    with output.open(mode, newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if write_header:
            writer.writeheader()
        writer.writerows(rows)


def get_next_run_prefix(output: Path) -> int:
    """è¯»å–å·²æœ‰ CSVï¼Œæ‰¾åˆ°æœ€å¤§çš„ run_id æ•°å­—åç¼€"""
    if not output.exists():
        return 1
    
    max_num = 0
    with output.open("r", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            run_id = row.get("run_id", "")
            # æå– cfg_XX ä¸­çš„æ•°å­—
            match = re.match(r"cfg_(\d+)", run_id)
            if match:
                num = int(match.group(1))
                max_num = max(max_num, num)
    
    return max_num + 1


def main() -> None:
    parser = argparse.ArgumentParser(description="Collect sweep data for meta-RL training")
    
    # âš™ï¸  å¯åœ¨è„šæœ¬å†…ä¿®æ”¹çš„å‚æ•°ï¼ˆæ‰€æœ‰å‚æ•°éƒ½æœ‰é»˜è®¤å€¼ï¼‰
    parser.add_argument("--output", default="results/mcts_tune/summary.csv")
    parser.add_argument("--python", default=sys.executable)
    parser.add_argument("--total-iters", type=int, default=100)        # â­ é»˜è®¤ 100 è½®
    parser.add_argument("--mcts-sims", type=int, default=200)          # â­ é»˜è®¤ 200 æ¨¡æ‹Ÿ
    parser.add_argument("--sample-interval", type=int, default=10)     # â­ é»˜è®¤æ¯ 10 è½®é‡‡æ ·
    parser.add_argument("--save-dir", default="results/mcts_tune/runs")
    parser.add_argument("--log-dir", default="logs/meta_rl_collect")
    parser.add_argument("--dry-run", action="store_true", help="Skip training and emit synthetic data")
    parser.add_argument("--append", action="store_true", help="Append to existing CSV instead of overwriting")
    
    # ğŸ”§ å¹¶è¡Œ/åˆ†ç‰‡å‚æ•°ï¼ˆç”¨äºæ‰‹åŠ¨å¹¶è¡Œï¼‰
    parser.add_argument("--parallel", type=int, default=1, 
                        help="Number of parallel processes (DISABLED: use --start/--end for safe parallelism)")
    parser.add_argument("--start", type=int, default=None,
                        help="Start config index (1-based, for manual parallelism)")
    parser.add_argument("--end", type=int, default=None,
                        help="End config index (1-based, inclusive, for manual parallelism)")
    
    args = parser.parse_args()

    repo_root = Path(__file__).resolve().parents[1]
    save_dir = Path(args.save_dir)
    log_dir = Path(args.log_dir)
    output_path = Path(args.output)

    # ç¡®å®šå¹¶è¡Œè¿›ç¨‹æ•°
    num_parallel = args.parallel
    if num_parallel == 0:
        num_parallel = max(1, cpu_count() - 1)  # ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
    print(f"[collect] ä½¿ç”¨ {num_parallel} ä¸ªå¹¶è¡Œè¿›ç¨‹")

    # å¦‚æœæ˜¯è¿½åŠ æ¨¡å¼ï¼Œè°ƒæ•´ run_id å‰ç¼€é¿å…å†²çª
    run_offset = 0
    if args.append:
        run_offset = get_next_run_prefix(output_path) - 1
        print(f"[collect] è¿½åŠ æ¨¡å¼ï¼šä» cfg_{run_offset + 1:03d} å¼€å§‹ç¼–å·")

    # å‡†å¤‡æ‰€æœ‰é…ç½®
    all_configs = list(default_grid())
    for idx, cfg in enumerate(all_configs, start=1):
        new_run_id = f"cfg_{idx + run_offset:03d}"
        cfg["run_id"] = new_run_id
    
    # ğŸ”§ å¦‚æœæŒ‡å®šäº† --start/--endï¼Œåªå¤„ç†è¯¥èŒƒå›´
    if args.start is not None or args.end is not None:
        start_idx = (args.start or 1) - 1  # è½¬ä¸º 0-based
        end_idx = (args.end or len(all_configs))  # åŒ…å« end
        all_configs = all_configs[start_idx:end_idx]
        print(f"[collect] åˆ†ç‰‡æ¨¡å¼ï¼šå¤„ç†é…ç½® {args.start or 1} åˆ° {args.end or len(default_grid())} (å…± {len(all_configs)} ä¸ª)")
    else:
        print(f"[collect] å…¨é‡æ¨¡å¼ï¼šæ€»å…± {len(all_configs)} ä¸ªé…ç½®")

    total_configs = len(all_configs)
    total_rows_written = 0

    def handle_result(idx: int, cfg: Dict[str, float], rows: List[Dict[str, float]]) -> None:
        nonlocal total_rows_written
        total_rows_written += len(rows)
        is_first_write = (idx == 1) and not args.append
        write_csv(rows, output_path, append=(not is_first_write))
        status = (
            f"[collect] å®Œæˆ {cfg['run_id']} ({idx}/{total_configs})"
            f" | æ–°å¢ {len(rows)} è¡Œ | ç´¯è®¡ {total_rows_written} è¡Œ"
        )
        print(status, flush=True)

    if num_parallel > 1:
        print(f"[collect] å¹¶è¡Œæ¨¡å¼ï¼š{num_parallel} ä¸ªè¿›ç¨‹")
        with Pool(processes=num_parallel,
                  initializer=_init_worker_globals,
                  initargs=(args, repo_root, save_dir, log_dir)) as pool:
            for idx, rows in enumerate(pool.imap(_parallel_worker, all_configs), start=1):
                cfg = all_configs[idx - 1]
                handle_result(idx, cfg, rows)
    else:
        for idx, cfg in enumerate(all_configs, start=1):
            print(f"[collect] æ­£åœ¨è¿è¡Œ {cfg['run_id']} ({idx}/{total_configs})...", flush=True)
            rows = run_config(cfg, args, repo_root, save_dir, log_dir)
            handle_result(idx, cfg, rows)

    mode_str = "è¿½åŠ " if args.append else "å†™å…¥"
    print(f"[collect] âœ“ å®Œæˆï¼å…± {mode_str} {total_rows_written} è¡Œåˆ° {args.output}")


if __name__ == "__main__":
    main()
