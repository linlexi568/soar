"""å¯¹æ¯”è¯„ä¼°è„šæœ¬ï¼šSB3 æ¨¡å‹ vs Soar ç¨‹åºã€‚

åœ¨ç›¸åŒçš„ SB3 ç¯å¢ƒä¸­å¯¹æ¯”è¯„ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    # å¯¹æ¯” PPO å’Œ Soar
    python scripts/sb3/compare_eval.py \
        --sb3-model results/sb3/ppo_figure8_final.zip \
        --soar-program results/figure8-safe_control_tracking.json \
        --trajectory figure8 \
        --episodes 20
"""
from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
_project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(_project_root))
sys.path.insert(0, str(_project_root / "01_soar"))

from scripts.sb3.quadrotor_env import QuadrotorTrackingEnv
from scripts.sb3.soar_policy import SoarPolicy, load_soar_program


def evaluate_policy(
    policy,
    env: QuadrotorTrackingEnv,
    n_episodes: int = 10,
    policy_name: str = "Policy",
    is_sb3: bool = False,
) -> Dict[str, Any]:
    """é€šç”¨ç­–ç•¥è¯„ä¼°å‡½æ•°ã€‚
    
    Args:
        policy: ç­–ç•¥å¯¹è±¡ï¼ˆSB3 æ¨¡å‹æˆ– SoarPolicyï¼‰
        env: è¯„ä¼°ç¯å¢ƒ
        n_episodes: è¯„ä¼° episode æ•°
        policy_name: ç­–ç•¥åç§°ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
        is_sb3: æ˜¯å¦æ˜¯ SB3 æ¨¡å‹
    
    Returns:
        è¯„ä¼°ç»“æœ
    """
    episode_rewards = []
    episode_lengths = []
    episode_pos_errors = []
    episode_final_pos_errors = []
    episode_trajectories = []
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        
        if hasattr(policy, 'reset'):
            policy.reset()
        
        total_reward = 0.0
        steps = 0
        pos_errors = []
        trajectory = []
        
        done = False
        truncated = False
        
        while not (done or truncated):
            # è·å–åŠ¨ä½œ
            if is_sb3:
                action, _ = policy.predict(obs, deterministic=True)
            else:
                action = policy.predict(obs)
            
            obs, reward, done, truncated, info = env.step(action)
            total_reward += reward
            steps += 1
            
            if 'pos_error' in info:
                pos_errors.append(info['pos_error'])
            
            if 'pos' in info:
                trajectory.append(info['pos'].copy())
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        if pos_errors:
            episode_pos_errors.append(np.mean(pos_errors))
            episode_final_pos_errors.append(pos_errors[-1])
        episode_trajectories.append(trajectory)
    
    results = {
        'name': policy_name,
        'mean_reward': float(np.mean(episode_rewards)),
        'std_reward': float(np.std(episode_rewards)),
        'min_reward': float(np.min(episode_rewards)),
        'max_reward': float(np.max(episode_rewards)),
        'mean_length': float(np.mean(episode_lengths)),
        'mean_pos_error': float(np.mean(episode_pos_errors)) if episode_pos_errors else None,
        'mean_final_pos_error': float(np.mean(episode_final_pos_errors)) if episode_final_pos_errors else None,
        'n_episodes': n_episodes,
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
    }
    
    return results


def compare_methods(
    sb3_model_path: Optional[str] = None,
    soar_program_path: Optional[str] = None,
    trajectory: str = 'figure8',
    n_episodes: int = 10,
    save_results: bool = True,
    output_dir: str = 'results/compare',
) -> Dict[str, Any]:
    """å¯¹æ¯”è¯„ä¼° SB3 æ¨¡å‹å’Œ Soar ç¨‹åºã€‚
    
    Args:
        sb3_model_path: SB3 æ¨¡å‹è·¯å¾„
        soar_program_path: Soar ç¨‹åºè·¯å¾„
        trajectory: è½¨è¿¹ç±»å‹
        n_episodes: è¯„ä¼° episode æ•°
        save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    # åˆ›å»ºç¯å¢ƒ
    env = QuadrotorTrackingEnv(
        trajectory=trajectory,
        duration=5.0,
        control_freq=50,
    )
    
    results = {
        'trajectory': trajectory,
        'n_episodes': n_episodes,
        'timestamp': datetime.now().isoformat(),
        'methods': {},
    }
    
    # è¯„ä¼° SB3 æ¨¡å‹
    if sb3_model_path:
        try:
            from stable_baselines3 import PPO, SAC, TD3, A2C
            
            # æ ¹æ®æ–‡ä»¶åçŒœæµ‹ç®—æ³•
            model_name = Path(sb3_model_path).stem.lower()
            if 'ppo' in model_name:
                AlgoClass = PPO
            elif 'sac' in model_name:
                AlgoClass = SAC
            elif 'td3' in model_name:
                AlgoClass = TD3
            elif 'a2c' in model_name:
                AlgoClass = A2C
            else:
                AlgoClass = PPO
            
            print(f"\nğŸ“¦ åŠ è½½ SB3 æ¨¡å‹: {sb3_model_path}")
            sb3_model = AlgoClass.load(sb3_model_path)
            
            print(f"ğŸ”„ è¯„ä¼° SB3 æ¨¡å‹...")
            sb3_results = evaluate_policy(
                sb3_model, env, n_episodes,
                policy_name=f"SB3-{AlgoClass.__name__}",
                is_sb3=True,
            )
            results['methods']['sb3'] = sb3_results
            
        except ImportError:
            print("âš ï¸ stable-baselines3 æœªå®‰è£…ï¼Œè·³è¿‡ SB3 è¯„ä¼°")
        except Exception as e:
            print(f"âš ï¸ SB3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    # è¯„ä¼° Soar ç¨‹åº
    if soar_program_path:
        try:
            print(f"\nğŸ“¦ åŠ è½½ Soar ç¨‹åº: {soar_program_path}")
            program = load_soar_program(soar_program_path)
            policy = SoarPolicy(program, dt=1.0/50.0)
            
            print(f"ğŸ”„ è¯„ä¼° Soar ç¨‹åº...")
            soar_results = evaluate_policy(
                policy, env, n_episodes,
                policy_name="Soar",
                is_sb3=False,
            )
            results['methods']['soar'] = soar_results
            
        except Exception as e:
            print(f"âš ï¸ Soar ç¨‹åºåŠ è½½å¤±è´¥: {e}")
    
    # è¯„ä¼° PID åŸºçº¿
    print(f"\nğŸ“¦ è¯„ä¼° PID åŸºçº¿...")
    pid_policy = SimplePIDPolicy()
    pid_results = evaluate_policy(
        pid_policy, env, n_episodes,
        policy_name="PID-Baseline",
        is_sb3=False,
    )
    results['methods']['pid'] = pid_results
    
    env.close()
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print_comparison(results)
    
    # ä¿å­˜ç»“æœ
    if save_results:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = output_path / f"compare_{trajectory}_{timestamp}.json"
        
        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„æ•°æ®
        save_results_data = {
            k: {kk: vv for kk, vv in v.items() if kk != 'episode_rewards' and kk != 'episode_lengths'}
            if isinstance(v, dict) else v
            for k, v in results.items()
        }
        
        with open(result_file, 'w') as f:
            json.dump(save_results_data, f, indent=2, default=str)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜: {result_file}")
    
    return results


def print_comparison(results: Dict[str, Any]):
    """æ‰“å°å¯¹æ¯”ç»“æœè¡¨æ ¼"""
    print("\n" + "=" * 70)
    print(f"ğŸ“Š å¯¹æ¯”è¯„ä¼°ç»“æœ - {results['trajectory']} è½¨è¿¹")
    print("=" * 70)
    
    methods = results.get('methods', {})
    if not methods:
        print("æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°ç»“æœ")
        return
    
    # è¡¨å¤´
    print(f"\n{'æ–¹æ³•':<20} {'Mean Reward':>12} {'Std':>8} {'Pos Error':>12} {'Final Err':>12}")
    print("-" * 70)
    
    # æŒ‰ reward æ’åº
    sorted_methods = sorted(
        methods.items(),
        key=lambda x: x[1].get('mean_reward', float('-inf')),
        reverse=True,
    )
    
    for name, data in sorted_methods:
        mean_r = data.get('mean_reward', 0)
        std_r = data.get('std_reward', 0)
        pos_err = data.get('mean_pos_error')
        final_err = data.get('mean_final_pos_error')
        
        pos_err_str = f"{pos_err:.4f}" if pos_err is not None else "N/A"
        final_err_str = f"{final_err:.4f}" if final_err is not None else "N/A"
        
        print(f"{data['name']:<20} {mean_r:>12.2f} {std_r:>8.2f} {pos_err_str:>12} {final_err_str:>12}")
    
    print("-" * 70)
    
    # è®¡ç®—ç›¸å¯¹æ€§èƒ½
    if len(sorted_methods) > 1:
        best_name, best_data = sorted_methods[0]
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_data['name']}")
        
        for name, data in sorted_methods[1:]:
            diff = best_data['mean_reward'] - data['mean_reward']
            pct = (diff / abs(data['mean_reward'])) * 100 if data['mean_reward'] != 0 else 0
            print(f"   vs {data['name']}: +{diff:.2f} ({pct:+.1f}%)")


class SimplePIDPolicy:
    """ç®€å• PID æ§åˆ¶å™¨ä½œä¸ºåŸºçº¿ã€‚"""
    
    def __init__(
        self,
        kp_pos: float = 2.0,
        kd_pos: float = 1.0,
        kp_att: float = 5.0,
        kd_att: float = 1.0,
    ):
        self.kp_pos = kp_pos
        self.kd_pos = kd_pos
        self.kp_att = kp_att
        self.kd_att = kd_att
        
        # åŠ¨ä½œç¼©æ”¾
        self.ACTION_SCALE = np.array([3.5, 0.12, 0.12, 0.06], dtype=np.float32)
        self.ACTION_OFFSET = np.array([3.5, 0.0, 0.0, 0.0], dtype=np.float32)
    
    def reset(self):
        pass
    
    def predict(self, obs: np.ndarray) -> np.ndarray:
        obs = np.asarray(obs, dtype=np.float32).flatten()
        
        # è§£æè§‚æµ‹
        pos_err = obs[0:3]    # ä½ç½®è¯¯å·®
        vel = obs[3:6]        # é€Ÿåº¦
        euler = obs[6:9]      # æ¬§æ‹‰è§’
        omega = obs[9:12]     # è§’é€Ÿåº¦
        
        # é«˜åº¦æ§åˆ¶ (fz)
        fz = self.kp_pos * pos_err[2] - self.kd_pos * vel[2]
        fz += 0.265  # æ‚¬åœè¡¥å¿
        
        # å§¿æ€æ§åˆ¶
        # æœŸæœ›å§¿æ€æ¥è‡ªä½ç½®è¯¯å·®
        desired_roll = -self.kp_pos * pos_err[1]
        desired_pitch = self.kp_pos * pos_err[0]
        
        roll_err = desired_roll - euler[0]
        pitch_err = desired_pitch - euler[1]
        yaw_err = -euler[2]  # ä¿æŒé›¶åèˆª
        
        tx = self.kp_att * roll_err - self.kd_att * omega[0]
        ty = self.kp_att * pitch_err - self.kd_att * omega[1]
        tz = 0.5 * yaw_err - 0.1 * omega[2]
        
        # ç‰©ç†è¾“å‡º
        raw_action = np.array([fz, tx, ty, tz], dtype=np.float32)
        
        # å½’ä¸€åŒ–
        normalized = (raw_action - self.ACTION_OFFSET) / self.ACTION_SCALE
        return np.clip(normalized, -1.0, 1.0)


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯”è¯„ä¼° SB3 å’Œ Soar')
    
    parser.add_argument('--sb3-model', type=str, help='SB3 æ¨¡å‹è·¯å¾„')
    parser.add_argument('--soar-program', type=str, help='Soar ç¨‹åºè·¯å¾„')
    parser.add_argument('--trajectory', type=str, default='figure8',
                       choices=['hover', 'figure8', 'circle'],
                       help='è½¨è¿¹ç±»å‹')
    parser.add_argument('--episodes', type=int, default=10, help='è¯„ä¼° episode æ•°')
    parser.add_argument('--output-dir', type=str, default='results/compare',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜ç»“æœ')
    
    args = parser.parse_args()
    
    compare_methods(
        sb3_model_path=args.sb3_model,
        soar_program_path=args.soar_program,
        trajectory=args.trajectory,
        n_episodes=args.episodes,
        save_results=not args.no_save,
        output_dir=args.output_dir,
    )


if __name__ == '__main__':
    main()
