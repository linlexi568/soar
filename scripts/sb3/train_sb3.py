"""SB3 è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬ã€‚

ä½¿ç”¨ Stable-Baselines3 è®­ç»ƒ PPO/SACï¼Œå¹¶ä¸ Soar ç”Ÿæˆçš„ç¨‹åºå¯¹æ¯”ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    # è®­ç»ƒ PPO
    python scripts/sb3/train_sb3.py --algo ppo --trajectory figure8 --timesteps 100000
    
    # è®­ç»ƒ SAC
    python scripts/sb3/train_sb3.py --algo sac --trajectory hover --timesteps 50000
    
    # è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹
    python scripts/sb3/train_sb3.py --eval --model results/sb3/ppo_figure8.zip
"""
from __future__ import annotations

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
_project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(_project_root))
sys.path.insert(0, str(_project_root / "01_soar"))

# å¯¼å…¥ç¯å¢ƒ
from scripts.sb3.quadrotor_env import QuadrotorTrackingEnv


def train_sb3(
    algo: str = 'ppo',
    trajectory: str = 'figure8',
    total_timesteps: int = 100_000,
    save_dir: str = 'results/sb3',
    reward_weights: Optional[Dict[str, float]] = None,
    seed: int = 42,
    verbose: int = 1,
    **kwargs,
) -> Dict[str, Any]:
    """ä½¿ç”¨ SB3 è®­ç»ƒ RL æ™ºèƒ½ä½“ã€‚
    
    Args:
        algo: ç®—æ³• ('ppo', 'sac', 'td3', 'a2c')
        trajectory: è½¨è¿¹ç±»å‹
        total_timesteps: è®­ç»ƒæ­¥æ•°
        save_dir: ä¿å­˜ç›®å½•
        reward_weights: Reward æƒé‡è¦†ç›–
        seed: éšæœºç§å­
        verbose: æ—¥å¿—çº§åˆ«
    
    Returns:
        è®­ç»ƒç»“æœç»Ÿè®¡
    """
    try:
        from stable_baselines3 import PPO, SAC, TD3, A2C
        from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
        from stable_baselines3.common.env_util import make_vec_env
        from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
        from stable_baselines3.common.monitor import Monitor
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… stable-baselines3:")
        print("   pip install stable-baselines3[extra]")
        return {}
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    # æ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"{algo}_{trajectory}_{timestamp}"
    
    print(f"=" * 60)
    print(f"SB3 Training: {algo.upper()} on {trajectory}")
    print(f"=" * 60)
    
    # åˆ›å»ºç¯å¢ƒ
    env_kwargs = {
        'trajectory': trajectory,
        'duration': 5.0,
        'control_freq': 50,
    }
    if reward_weights:
        env_kwargs['reward_weights'] = reward_weights
    
    def make_env():
        env = QuadrotorTrackingEnv(**env_kwargs)
        return Monitor(env)
    
    # å‘é‡åŒ–ç¯å¢ƒ
    n_envs = 4
    vec_env = make_vec_env(make_env, n_envs=n_envs, seed=seed)
    
    # è¯„ä¼°ç¯å¢ƒ
    eval_env = make_vec_env(make_env, n_envs=1, seed=seed + 100)
    
    # é€‰æ‹©ç®—æ³•
    algo_map = {
        'ppo': PPO,
        'sac': SAC,
        'td3': TD3,
        'a2c': A2C,
    }
    
    if algo.lower() not in algo_map:
        print(f"âŒ ä¸æ”¯æŒçš„ç®—æ³•: {algo}")
        print(f"   æ”¯æŒ: {list(algo_map.keys())}")
        return {}
    
    AlgoClass = algo_map[algo.lower()]
    
    # ç®—æ³•è¶…å‚æ•°
    if algo.lower() == 'ppo':
        model = AlgoClass(
            'MlpPolicy',
            vec_env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            verbose=verbose,
            seed=seed,
            tensorboard_log=str(save_path / "tb_logs"),
        )
    elif algo.lower() == 'sac':
        model = AlgoClass(
            'MlpPolicy',
            vec_env,
            learning_rate=3e-4,
            buffer_size=100_000,
            learning_starts=1000,
            batch_size=256,
            tau=0.005,
            gamma=0.99,
            train_freq=1,
            gradient_steps=1,
            ent_coef='auto',
            verbose=verbose,
            seed=seed,
            tensorboard_log=str(save_path / "tb_logs"),
        )
    elif algo.lower() == 'td3':
        model = AlgoClass(
            'MlpPolicy',
            vec_env,
            learning_rate=3e-4,
            buffer_size=100_000,
            learning_starts=1000,
            batch_size=256,
            tau=0.005,
            gamma=0.99,
            verbose=verbose,
            seed=seed,
            tensorboard_log=str(save_path / "tb_logs"),
        )
    else:  # a2c
        model = AlgoClass(
            'MlpPolicy',
            vec_env,
            learning_rate=7e-4,
            n_steps=5,
            gamma=0.99,
            gae_lambda=0.95,
            ent_coef=0.01,
            verbose=verbose,
            seed=seed,
            tensorboard_log=str(save_path / "tb_logs"),
        )
    
    # Callbacks
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(save_path / "best"),
        log_path=str(save_path / "eval_logs"),
        eval_freq=max(total_timesteps // 20, 1000),
        n_eval_episodes=5,
        deterministic=True,
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=max(total_timesteps // 10, 5000),
        save_path=str(save_path / "checkpoints"),
        name_prefix=run_name,
    )
    
    # è®­ç»ƒ
    print(f"\nå¼€å§‹è®­ç»ƒ: {total_timesteps:,} æ­¥")
    print(f"å¹¶è¡Œç¯å¢ƒ: {n_envs}")
    print(f"ä¿å­˜è·¯å¾„: {save_path}")
    print()
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True,
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = save_path / f"{run_name}_final.zip"
    model.save(str(final_model_path))
    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # è¯„ä¼°æœ€ç»ˆæ€§èƒ½
    print("\nè¯„ä¼°æœ€ç»ˆæ€§èƒ½...")
    eval_results = evaluate_model(model, eval_env, n_eval_episodes=10)
    
    # ä¿å­˜ç»“æœ
    results = {
        'algo': algo,
        'trajectory': trajectory,
        'total_timesteps': total_timesteps,
        'seed': seed,
        'reward_weights': reward_weights,
        'eval_results': eval_results,
        'model_path': str(final_model_path),
        'timestamp': timestamp,
    }
    
    results_path = save_path / f"{run_name}_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {results_path}")
    
    # æ¸…ç†
    vec_env.close()
    eval_env.close()
    
    return results


def evaluate_model(
    model,
    env,
    n_eval_episodes: int = 10,
    deterministic: bool = True,
) -> Dict[str, Any]:
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
    
    Returns:
        è¯„ä¼°ç»“æœç»Ÿè®¡
    """
    episode_rewards = []
    episode_lengths = []
    episode_pos_errors = []
    
    for ep in range(n_eval_episodes):
        obs = env.reset()
        done = False
        total_reward = 0.0
        steps = 0
        pos_errors = []
        
        while not done:
            action, _ = model.predict(obs, deterministic=deterministic)
            obs, reward, done, info = env.step(action)
            total_reward += reward[0]
            steps += 1
            
            if 'pos_error' in info[0]:
                pos_errors.append(info[0]['pos_error'])
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        if pos_errors:
            episode_pos_errors.append(np.mean(pos_errors))
    
    results = {
        'mean_reward': float(np.mean(episode_rewards)),
        'std_reward': float(np.std(episode_rewards)),
        'mean_length': float(np.mean(episode_lengths)),
        'mean_pos_error': float(np.mean(episode_pos_errors)) if episode_pos_errors else None,
        'n_episodes': n_eval_episodes,
    }
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ ({n_eval_episodes} episodes):")
    print(f"   Mean Reward: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"   Mean Length: {results['mean_length']:.1f}")
    if results['mean_pos_error'] is not None:
        print(f"   Mean Pos Error: {results['mean_pos_error']:.4f} m")
    
    return results


def load_and_evaluate(
    model_path: str,
    trajectory: str = 'figure8',
    n_eval_episodes: int = 10,
) -> Dict[str, Any]:
    """åŠ è½½å¹¶è¯„ä¼°å·²ä¿å­˜çš„æ¨¡å‹ã€‚"""
    try:
        from stable_baselines3 import PPO, SAC, TD3, A2C
        from stable_baselines3.common.vec_env import DummyVecEnv
        from stable_baselines3.common.monitor import Monitor
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… stable-baselines3")
        return {}
    
    # æ ¹æ®æ–‡ä»¶åçŒœæµ‹ç®—æ³•
    model_name = Path(model_path).stem.lower()
    if 'ppo' in model_name:
        AlgoClass = PPO
    elif 'sac' in model_name:
        AlgoClass = SAC
    elif 'td3' in model_name:
        AlgoClass = TD3
    elif 'a2c' in model_name:
        AlgoClass = A2C
    else:
        # é»˜è®¤å°è¯• PPO
        AlgoClass = PPO
    
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    model = AlgoClass.load(model_path)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    env = DummyVecEnv([lambda: Monitor(QuadrotorTrackingEnv(trajectory=trajectory))])
    
    results = evaluate_model(model, env, n_eval_episodes)
    
    env.close()
    return results


def main():
    parser = argparse.ArgumentParser(description='SB3 Quadrotor Training')
    
    # æ¨¡å¼
    parser.add_argument('--eval', action='store_true', help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--model', type=str, help='è¦è¯„ä¼°çš„æ¨¡å‹è·¯å¾„')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--algo', type=str, default='ppo',
                       choices=['ppo', 'sac', 'td3', 'a2c'],
                       help='RL ç®—æ³•')
    parser.add_argument('--trajectory', type=str, default='figure8',
                       choices=['hover', 'figure8', 'circle'],
                       help='è½¨è¿¹ç±»å‹')
    parser.add_argument('--timesteps', type=int, default=100_000,
                       help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--save-dir', type=str, default='results/sb3',
                       help='ä¿å­˜ç›®å½•')
    
    # Reward æƒé‡
    parser.add_argument('--pos-weight', type=float, default=1.0,
                       help='ä½ç½®è¯¯å·®æƒé‡')
    parser.add_argument('--ctrl-weight', type=float, default=0.001,
                       help='æ§åˆ¶ä»£ä»·æƒé‡')
    
    args = parser.parse_args()
    
    if args.eval:
        if not args.model:
            print("âŒ è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --model å‚æ•°")
            return
        load_and_evaluate(args.model, args.trajectory)
    else:
        reward_weights = {
            'pos_cost_weight': args.pos_weight,
            'ctrl_cost_weight': args.ctrl_weight,
        }
        
        train_sb3(
            algo=args.algo,
            trajectory=args.trajectory,
            total_timesteps=args.timesteps,
            save_dir=args.save_dir,
            reward_weights=reward_weights,
            seed=args.seed,
        )


if __name__ == '__main__':
    main()
