#!/usr/bin/env python3
"""
ç¨‹åºéªŒè¯å·¥å…· - ç›´æ¥åŸºäº u_* åŠ›/åŠ›çŸ©è¾“å‡º
ä¸ä¾èµ–ä»»ä½• PID å°è£…ï¼Œçº¯ç²¹éªŒè¯ DSL ç¨‹åºæ€§èƒ½
"""
import os
import sys
import json
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
REPO_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(REPO_ROOT))
sys.path.insert(0, str(REPO_ROOT / "01_soar"))

# Isaac Gym å¿…é¡»åœ¨ torch ä¹‹å‰å¯¼å…¥
ISAAC_GYM_PATH = REPO_ROOT / "isaacgym" / "python"
if ISAAC_GYM_PATH.exists():
    sys.path.insert(0, str(ISAAC_GYM_PATH))

def load_program(program_path: str):
    """åŠ è½½ DSL ç¨‹åº JSON"""
    with open(program_path, 'r') as f:
        data = json.load(f)
    
    if isinstance(data, dict) and 'rules' in data:
        return data['rules'], data.get('meta', {})
    elif isinstance(data, list):
        return data, {}
    else:
        raise ValueError(f"Invalid program format in {program_path}")

def evaluate_program(program, traj='square', duration=5.0, num_envs=1024, replicas=1):
    """
    ä½¿ç”¨ BatchEvaluator è¯„ä¼°ç¨‹åºæ€§èƒ½
    
    Args:
        program: DSL ç¨‹åºè§„åˆ™åˆ—è¡¨
        traj: è½¨è¿¹ç±»å‹
        duration: ä»¿çœŸæ—¶é•¿
        num_envs: å¹¶è¡Œç¯å¢ƒæ•°
        replicas: é‡å¤æ¬¡æ•°
    
    Returns:
        dict: è¯„ä¼°ç»“æœ {reward, state_cost, action_cost, ...}
    """
    from utils.batch_evaluation import BatchEvaluator
    from utilities.trajectory_presets import get_scg_trajectory_config
    
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„è½¨è¿¹é…ç½®
    traj_cfg = get_scg_trajectory_config(traj)
    trajectory_config = {
        'type': traj_cfg.task,
        'params': dict(traj_cfg.params),
        'initial_xyz': list(traj_cfg.center)
    }
    
    evaluator = BatchEvaluator(
        isaac_num_envs=num_envs,
        reward_profile='safe_control_tracking',
        trajectory_config=trajectory_config,
        duration=duration,
        device='cuda:0',
        use_fast_path=True,
        strict_no_prior=True,  # è®­ç»ƒæ—¶ç”¨çš„æ˜¯ Trueï¼šå®Œå…¨ç›´æ¥ u_* æ§åˆ¶
        reward_reduction='sum',  # ä¸è®­ç»ƒæ—¶å¯¹é½ï¼šä½¿ç”¨ sum è€Œä¸æ˜¯ mean
        zero_action_penalty=0.0,  # æµ‹è¯•æ—¶ä¸ä½¿ç”¨é›¶åŠ¨ä½œæƒ©ç½šï¼Œåªçœ‹çœŸå®æ€§èƒ½
        replicas_per_program=1,  # å…³é”®ï¼šä¸è®­ç»ƒæ—¶å¯¹é½ï¼Œè®¾ç½®ä¸º 1ï¼ˆè®­ç»ƒæ—¶ç”¨çš„å°±æ˜¯ 1ï¼‰
        enable_output_mad=False,  # ğŸ”§ è®­ç»ƒæ—¶ ENABLE_OUTPUT_MAD=falseï¼Œå¿…é¡»å…³é—­
    )
    
    # ä½¿ç”¨ evaluate_batch_with_metrics è·å–è¯¦ç»†çš„å¥–åŠ±åˆ†è§£
    rewards_train, rewards_true, metrics_list = evaluator.evaluate_batch_with_metrics(
        programs=[program] * replicas
    )
    
    # è®¡ç®—å¹³å‡ç»“æœ
    avg_reward_train = sum(rewards_train) / len(rewards_train) if rewards_train else 0.0
    avg_reward_true = sum(rewards_true) / len(rewards_true) if rewards_true else 0.0
    
    # èšåˆ metrics
    avg_metrics = {}
    if metrics_list:
        for key in metrics_list[0].keys():
            values = [m.get(key, 0.0) for m in metrics_list]
            avg_metrics[key] = sum(values) / len(values)
    
    return {
        'reward_train': float(avg_reward_train),  # åŒ…å«æƒ©ç½šé¡¹
        'reward_true': float(avg_reward_true),    # ä¸å«æƒ©ç½šé¡¹
        'metrics': avg_metrics,
        'num_envs': num_envs,
        'replicas': replicas
    }

def main():
    parser = argparse.ArgumentParser(description='éªŒè¯ DSL ç¨‹åºæ€§èƒ½')
    parser.add_argument('--program', type=str, required=True, help='ç¨‹åº JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--traj', type=str, default='square', choices=['square', 'circle', 'figure8', 'helix'])
    parser.add_argument('--duration', type=float, default=5.0, help='ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰')
    parser.add_argument('--num-envs', type=int, default=1024, help='å¹¶è¡Œç¯å¢ƒæ•°')
    parser.add_argument('--replicas', type=int, default=1, help='é‡å¤è¯„ä¼°æ¬¡æ•°')
    parser.add_argument('--match-training', action='store_true', help='ä½¿ç”¨è®­ç»ƒæ—¶é…ç½®ï¼ˆä» meta è¯»å–ï¼‰')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("Soar ç¨‹åºéªŒè¯å·¥å…·")
    print("=" * 80)
    print(f"ç¨‹åºæ–‡ä»¶: {args.program}")
    print(f"è½¨è¿¹: {args.traj} | æ—¶é•¿: {args.duration}s")
    print(f"å¹¶è¡Œç¯å¢ƒ: {args.num_envs} | é‡å¤: {args.replicas}")
    print()
    
    # åŠ è½½ç¨‹åº
    program, meta = load_program(args.program)
    print(f"âœ“ ç¨‹åºåŠ è½½æˆåŠŸ: {len(program)} æ¡è§„åˆ™")
    
    # ååºåˆ—åŒ–ç¨‹åºä»¥ä¾¿æ£€æŸ¥æ—¶é—´ç®—å­
    try:
        from core.serialization import deserialize_program
        program_ast = deserialize_program({'rules': program})
    except Exception:
        program_ast = None
    
    # å¦‚æœå¯ç”¨ --match-trainingï¼Œä» meta ä¸­è¯»å–é…ç½®
    num_envs = args.num_envs
    if args.match_training and meta:
        print(f"  ä½¿ç”¨è®­ç»ƒæ—¶é…ç½®:")
        if 'isaac_num_envs' in meta:
            num_envs = meta['isaac_num_envs']
            print(f"    ç¯å¢ƒæ•°: {num_envs}")
    
    if meta:
        print(f"  è®­ç»ƒå…ƒä¿¡æ¯:")
        for key in ['iteration', 'reward', 'isaac_num_envs', 'mcts_simulations']:
            if key in meta:
                print(f"    {key}: {meta[key]}")
    print()
    
    # è¯„ä¼°ç¨‹åº
    print("å¼€å§‹è¯„ä¼°...")
    results = evaluate_program(
        program=program,
        traj=args.traj,
        duration=args.duration,
        num_envs=num_envs,
        replicas=args.replicas
    )
    
    print()
    print("=" * 80)
    print("è¯„ä¼°ç»“æœ")
    print("=" * 80)
    print(f"è®­ç»ƒå¥–åŠ± (å«æƒ©ç½š):   {results['reward_train']:.4f}")
    print(f"çœŸå®å¥–åŠ± (ä¸å«æƒ©ç½š): {results['reward_true']:.4f}")
    print()
    
    # æ˜¾ç¤ºè¯¦ç»† metricsï¼ˆå®é™…è¿”å›çš„ state_cost å’Œ action_costï¼‰
    if results['metrics']:
        print("ä»£ä»·åˆ†è§£:")
        metrics = results['metrics']
        if 'state_cost' in metrics:
            print(f"  çŠ¶æ€ä»£ä»· (state_cost):   {metrics['state_cost']:.6f}")
        if 'action_cost' in metrics:
            print(f"  åŠ¨ä½œä»£ä»· (action_cost):  {metrics['action_cost']:.6f}")
    print()
    
    if meta and 'reward' in meta:
        train_reward = meta['reward']
        test_reward = results['reward_train']  # ä½¿ç”¨è®­ç»ƒå¥–åŠ±å¯¹æ¯”
        diff = abs(train_reward - test_reward)
        print(f"è®­ç»ƒæ—¶å¥–åŠ± (meta):   {train_reward:.4f}")
        print(f"é‡æ–°è¯„ä¼°å¥–åŠ±:        {test_reward:.4f}")
        print(f"å·®å¼‚:                {diff:.4f} ({diff/abs(train_reward)*100:.1f}%)")
        print()
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç®—å­ï¼ˆdelay/ema/diff/rateï¼‰- ä½¿ç”¨ååºåˆ—åŒ–çš„ AST
        has_temporal = False
        def check_temporal(node):
            nonlocal has_temporal
            if node is None:
                return
            if hasattr(node, 'op') and node.op in ('delay', 'ema', 'diff', 'rate', 'rate_limit'):
                has_temporal = True
            for attr in ['child', 'left', 'right', 'condition', 'then_branch', 'else_branch']:
                if hasattr(node, attr):
                    check_temporal(getattr(node, attr))
        if program_ast is not None:
            for rule in program_ast:
                check_temporal(rule.get('condition'))
                for a in rule.get('action', []) or []:
                    check_temporal(a)
        
        if has_temporal:
            print("âš ï¸  æ³¨æ„: ç¨‹åºåŒ…å«æ—¶é—´ç®—å­ (delay/ema/diff/rate)ã€‚")
            print("   è®­ç»ƒæ—¶è®°å½•çš„å¥–åŠ±å¯èƒ½å—åˆ°çŠ¶æ€ç´¯ç§¯å½±å“ï¼ˆéç¡®å®šæ€§ï¼‰ã€‚")
            print("   å½“å‰è¯„ä¼°ä½¿ç”¨ç¡®å®šæ€§é‡ç½®ï¼Œæ¯æ¬¡ä»é›¶çŠ¶æ€å¼€å§‹ã€‚")
            print("   å·®å¼‚å±äºé¢„æœŸè¡Œä¸ºï¼Œä¸å½±å“å®é™…æ§åˆ¶æ€§èƒ½è¯„ä¼°ã€‚")
        else:
            print("æ³¨æ„: å·®å¼‚æ¥è‡ªä¸åŒçš„ç¯å¢ƒåˆå§‹åŒ–æˆ–é…ç½®ï¼Œå±äºæ­£å¸¸ç°è±¡ã€‚")
        print("      çœŸå®å¥–åŠ± = -(state_cost + action_cost)")
    
    print("=" * 80)

if __name__ == '__main__':
    main()
